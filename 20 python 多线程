
 3.10.2 Documentation » Python Frequently Asked Questions » Library and Extension FAQ
 
怎样修改全局变量是线程安全的？

Python VM 内部会使用 global interpreter lock （GIL）来确保同一时间只有一个线程运行。通常 Python 只会在字节码指令之间切换线程；切换的频率可以通过设置 sys.setswitchinterval() 指定。从 Python 程序的角度来看，每一条字节码指令以及每一条指令对应的 C 代码实现都是原子的。
理论上说，具体的结果要看具体的 PVM 字节码实现对指令的解释。而实际上，对内建类型（int，list，dict 等）的共享变量的“类原子”操作都是原子的。

举例来说，下面的操作是原子的（L、L1、L2 是列表，D、D1、D2 是字典，x、y 是对象，i，j 是 int 变量）：

L.append(x)
L1.extend(L2)
x = L[i]
x = L.pop()
L1[i:j] = L2
L.sort()
x = y
x.field = y
D[x] = y
D1.update(D2)
D.keys()
这些不是原子的：

i = i+1
L.append(L[-1])
L[i] = L[j]
D[x] = D[x] + 1




'''--------------------------------------------------------------------------------------------------------------------------'''


# import threading
# import multiprocessing.dummy as dummy

# # threading: 'current_thread',  'Lock', 'RLock', 'Condition', 'Event', 'Barrier', 'Semaphore', 'BoundedSemaphore'
# # mtpsdummy: 'current_process', 'Lock', 'RLock', 'Condition', 'Event', 'Barrier', 'Semaphore', 'BoundedSemaphore', 'Pipe', 'Queue',, 'JoinableQueue', 'Manager', 'Pool'


'''--------------------------------------------------------------------------------------------------------------------------'''
'''------------------------------------------------------ threading ---------------------------------------------------------'''
'''--------------------------------------------------------------------------------------------------------------------------'''

## 多线程:

''' threading.current_thread(),threading.enumerate(),threading.current_thread().getName(),threading.current_thread().name
    name 是当前线程的属性， getName 是当前线程的方法。
    尽管 threading.current_thread().name 和 threading.current_thread().getName() 的结果一样，但是完全不是同一种东西呀， 例如通过 threading.current_thread().name ＝ ‘thread_python’ 来改变它。    

    import threading, time
    def run(arg):
        print("running sub thread...{}".format(threading.current_thread()))
        threading.current_thread().name="xurui_python"
        print("sub1 Thread...{}".format(threading.current_thread().getName()))
        print("sub2 Thread...{}".format(threading.current_thread().name))
        time.sleep(3)
    if __name__ == "__main__":
        t1 = threading.Thread(target=run,args=("t1",))
        t1.start()
        print("mian1 Thread...{}".format(threading.current_thread().getName()))
        print("mian2 Thread...{}".format(threading.current_thread().name))
''' 


# import threading
# def func(n):
#     while n > 0:
#         print("当前线程数:", threading.activeCount())
#         n -= 1
# for x in range(5):
#     t = threading.Thread(target=func, args=(5,))
#     t.start()

# print("主线程：", threading.current_thread().name)


# import threading
# def test():
#     x = 0
#     for i in range(5):
#         x = i + x
#         print(x)
#     print(str(threading.enumerate())) # 打印出当前进程中的所有线程

# if __name__ == '__main__':
#     thread = threading.Thread(target=test)
#     thread.start()
#     print('nihao')


# import threading
# def test():
#     x = 0
#     for i in range(5):
#         x = i + x
#         print(x)
#     print(str(threading.enumerate()))  # 打印出当前进程中的所有线程
# if __name__ == '__main__':
#     thread = threading.Thread(target=test)
#     thread.setDaemon(True)
#     thread.start()
#     print('nihao')


# import threading
# def test1():
#     while True:
#         print('nihao')
# def test2():
#     while True:
#         print(str(threading.enumerate()))
# if __name__ == '__main__':
#     thread1 = threading.Thread(target=test1)
#     thread2 = threading.Thread(target=test2)
#     thread1.setDaemon(True)
#     thread1.start()
#     thread2.start()


# (1)setDaemon(True) 将子线程设置为守护进程（默认False）， 主线程结束后，守护子线程随之中止。
# (2)join() 用于阻塞主线程， 可以想象成将某个子线程的执行过程插入(join)到主线程的时间线上，主线程的后续代码延后执行。 注意和 t.start() 分开写在两个for循环中。
# (3)第一个for循环同时启动了所有子线程，随后在第二个for循环中执行t.join() ， 主线程实际被阻塞的总时长==其中执行时间最长的一个子线程。


# import threading
# import time

# def run():
#     time.sleep(2)
#     print(f'当前线程的名字是： {threading.current_thread().name}\n')
#     time.sleep(2)

# if __name__ == '__main__':
#     start_time = time.time()
#     print(f'这是主线程：{threading.current_thread().name}\n')
#     thread_list = []
#     for i in range(5):
#         t = threading.Thread(target=run)
#         thread_list.append(t)

#     for t in thread_list:
#         t.setDaemon(True)
#         print(f'{t.getName()}:开始\n')  # 获取子线程名字
#         t.start()
#     print('-----------------------------')
#     for t in thread_list:
#         print(f'{t.getName()}:阻塞\n')  # 获取子线程名字
#         t.join()
#     print('=============================')        
#     for t in thread_list:
#         print(t.is_alive())  # 判断线程是否在运行

#     print(f'主线程结束！{threading.current_thread().name}\n')
#     print(f'一共用时：{time.time()-start_time}\n')

    # print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~') 
    # #### thread_list[2].run()  # 线程被cpu调度后自动执行线程对象的run方法
    # thread_list[2].setName('第一个线程')  # 设置线程名字
    # print(thread_list[2].getName())
    # thread_list[2].start()
    # print(thread_list[2].is_alive())  # 判断线程是否在运行
    # print(thread_list[2].getName())

# setDaemon(True)：将线程声明为守护线程，必须在start()方法调用之前设置。这个方法基本和join是相反的。
# 当我们在程序运行中，执行一个主线程，如果主线程又创建一个子线程，主线程和子线程就分兵两路，分别运行，那么当主线程完成
# 想退出时，会检验子线程是否完成。如果子线程未完成，则主线程会等待子线程完成后再退出。但是有时候我们需要的是只要主线程
# 完成了，不管子线程是否完成，都要和主线程一起退出，这时就可以用setDaemon()方法



'''-------------------------------------------------join---------------------------------------------------'''

# import threading, time
 
# def action(arg):
#     time.sleep(1)
#     print('sub thread start!the thread name is:%s    ' % threading.currentThread().getName())
#     print('the arg is:%s   ' %arg)
#     time.sleep(1)
 
# # #不正确写法，会导致多线程顺序执行，失去了多线程的意义
# # for i in range(4):
# #     t =threading.Thread(target=action,args=(i,))
# #     t.setDaemon(True)
# #     t.start()
# #     t.join()
 
# #正确写法
# thread_list = []    #线程存放列表
# for i in range(4):
#     t =threading.Thread(target=action,args=(i,))
#     t.setDaemon(True)
#     thread_list.append(t)
 
# for t in thread_list:
#     t.start()
 
# for t in thread_list:
#     t.join()
# print('main_thread end!')



'''------------------------------------------------- Lock ---------------------------------------------------'''

# 这个锁是不被某个特定线程所拥有的。一个线程获取了锁，之后任何线程尝试着获取锁，都会失败。
# 只有在这个范围内的代码才受到lock锁的约束，意思是只有请求到lock这个对象的锁时，才能执行。所有线程抢锁，但只有一个能抢上,等到这个lock锁释放时，别的lock锁才能申请锁的权限，否则别的lock.acquire()这里只能等待。
# import threading,time,random

# count = 0
# class MyThread_lock(threading.Thread):
#     def __init__(self,lock,threadName):
#         super(MyThread_lock,self).__init__(name=threadName)
#         self.lock = lock
#     def run(self):
#         global count
#         self.lock.acquire()  #acquire()方法提供了确定对象被锁定的标志
#         for i in range(5):
#             count +=1
#             time.sleep(2)
#             print(self.getName(),count)
#         self.lock.release()  #release()在对象被当前线程使用完毕后将当前对象释放
# lock = threading.Lock()
# for i in range(2):
#     MyThread_lock(lock,"MyThreadName:"+str(i)).start()


# # 多线程锁使用方法
# import threading,time
# sume, loop = 0, 100

# #生成锁实例
# lock = threading.Lock()
# def myadd():
#     global sume,loop #声明全局变量只能在代码块中有效
#     for i in range(1,loop):
#         lock.acquire()  #申请锁，就是加锁
#         sume += 1
#         print(threading.current_thread().name,sume)
#         lock.release()  #释放锁

# def mysub():
#     global sume,loop
#     for i in range(1,loop):
#         lock.acquire()  #申请锁，就是加锁
#         sume -= 1
#         print(threading.current_thread().name,sume)
#         lock.release() #释放锁

# #本模块作为主进程运行时执行下面的操作
# if __name__ == '__main__':
#     print("开始:{}={}".format(threading.current_thread().name,sume))
#     t1 = threading.Thread(target=myadd,args=())
#     t2 = threading.Thread(target=mysub,args=())
#     t1.start()
#     t2.start()
#     t1.join()
#     t2.join()
#     print("结束:{}={}".format(threading.current_thread().name,sume))


# import time, threading
 
# # 假定这是你的银行存款:
# balance = 0
# lock = threading.Lock() 
# def change_it(n):
#     # 先存后取，结果应该为0:
#     global balance    
#     balance = balance + n
#     print(f'{threading.current_thread().name} = +{balance}')
#     balance = balance - n
#     print(f'{threading.current_thread().name} = -{balance}')

#         # (1)==>
#         # t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2：
#         # 初始值 balance = 0
#         # t1: x1 = balance + 5  # x1 = 0 + 5 = 5
#         # t2: x2 = balance + 8  # x2 = 0 + 8 = 8
#         # t2: balance = x2      # balance = 8
         
#         # t1: balance = x1      # balance = 5
#         # t1: x1 = balance - 5  # x1 = 5 - 5 = 0
#         # t1: balance = x1      # balance = 0
         
#         # t2: x2 = balance - 8  # x2 = 0 - 8 = -8
#         # t2: balance = x2   # balance = -8
         
#         # 结果 balance = -8
#         # 究其原因，是因为修改balance需要多条语句，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。
#         # 两个线程同时一存一取，就可能导致余额不对，你肯定不希望你的银行存款莫名其妙地变成了负数，所以，我们必须确保一个线程在修改balance的时候，别的线程一定不能改。 

# def run_thread(n):
#     for i in range(100000):

#         lock.acquire()    #### 要确保balance计算正确，就要给change_it()上一把锁，当某个线程开始执行change_it()时，我们说，该线程因为获得了锁，因此其他线程不能同时执行change_it()，只能等待，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁就是通过threading.Lock()来实现：
#         change_it(n)
#         lock.release()

# for i in range(2):
#     threading.Thread(target=run_thread, args=(i*5+3,)).start()

# # t1 = threading.Thread(target=run_thread, args=(5,))
# # t2 = threading.Thread(target=run_thread, args=(8,))
# t1.start()
# t2.start()
# t1.join()
# t2.join()
# print(balance)      


# from threading import Lock,Thread
# from time import sleep,time
# lock = Lock()
# list1 = [0]*10

# def putR(s):
#     lock.acquire()
#     for i in range(len(list1)):
#         list1[i] = i
#         print("put ",list1[i])
#         sleep(s)
#     lock.release()
# def getR(s):
#     lock.acquire()
#     for i in range(len(list1)):
#         print("get ",list1[i])
#         sleep(s)
#     lock.release()

# if __name__ == "__main__":

#     t1 = Thread(target=putR, name="aa", args=(0.1,))
#     t2 = Thread(target=getR, name="aa", args=(0.5,))

#     t1.start()
#     t2.start()
#     t1.join()
#     t2.join()


# # 使用一个锁
# import threading, time
 
# def num():
#     for i in range(1, 53): # ,2
#         lock.acquire()
#         print(f'{threading.current_thread().name},{i}')
# def alpha():
#     for i in range(65, 91):
#         time.sleep(2)
#         # time.sleep(0.000001)
#         print(f'{threading.current_thread().name},{i}')
#         lock.release()
# if __name__ == '__main__':
#     lock = threading.Lock()
#     sn = threading.Thread(target=num)
#     sa = threading.Thread(target=alpha)
#     sn.start()
#     sa.start()


# # # 使用多个锁
# import threading,time
# lock1=threading.Lock()
# lock2=threading.Lock()
# # lock1 和lock2 是两把不同的锁，二者可以同时存在，不冲突，但是lock1本身只能有一把锁存在
# def do_t1(name):
#        lock1.acquire()
#        print(name)
#        time.sleep(10)
#        print('I am function t1')
#        lock1.release()

# def do_t2(name):
#        lock2.acquire()
#        print(name)
#        print('I am function t2')
#        lock2.release()

# t1=threading.Thread(target=do_t1,args=('th_t1',))
# t2=threading.Thread(target=do_t2,args=('th_t2',))

# t1.start()
# time.sleep(1)
# t2.start()
# t1.join()
# t2.join()

# print("ok")


# import threading, time

# alpha_lock = threading.Lock()
# num_lock = threading.Lock()
# def print_num():
#     num_lock.acquire() #先加锁，当输出2自己在获取锁时必须等待另一进程释放
#     for i in range(1, 50):
#         print(f'{threading.current_thread().name},{i}\n')
#         num_lock.acquire()
#         alpha_lock.release()
# def print_alpla():
#     for w in range(1,20):
#         print(f'{threading.current_thread().name},{w}\n')
#         num_lock.release()
#         alpha_lock.acquire()
# if __name__ == '__main__':
#     num_thread = threading.Thread(target=print_num,)
#     alpha_thread = threading.Thread(target=print_alpla)
#     alpha_lock.acquire()
#     num_thread.start()
#     alpha_thread.start()


#      ||                 ||             ||      #

# import time
# import multiprocessing.dummy as dummy

# alpha_lock = dummy.Lock()
# num_lock = dummy.Lock()
# def print_num():
#     num_lock.acquire() #先加锁，当输出2自己在获取锁时必须等待另一进程释放
#     for i in range(1, 50):
#         print(f'{dummy.threading.current_thread().name},{i}\n')
#         num_lock.acquire()
#         alpha_lock.release()
# def print_alpla():
#     for w in range(1,50):
#         print(f'{dummy.threading.current_thread().name},{chr(w)}\n')
#         num_lock.release()
#         alpha_lock.acquire()
#     alpha_lock.acquire()        # (1)

# if __name__ == '__main__':
#     num_thread = dummy.threading.Thread(target=print_num,)
#     alpha_thread = dummy.threading.Thread(target=print_alpla)
#     # alpha_lock.acquire()      # (2)
#     num_thread.start()
#     alpha_thread.start()


'''---------------------------------------------------- condition --------------------------------------------------'''


# threading.Condition 是一个继承自threading.Lock的一个类,Condition的处理流程如下：
# 首先acquire一个条件变量，然后判断一些条件。
# 如果条件不满足则wait；
# 如果条件满足，进行一些处理改变条件后，通过notify方法通知其他线程，其他处于wait状态的线程接到通知后会重新判断条件。
# 不断的重复这一过程，从而解决复杂的同步问题。


# import threading
# import time

# class Producer(threading.Thread):
#     # 生产者函数
#     def run(self):
#         global count
#         while True:
#             if con.acquire():
#                 # 当count 小于等于1000 的时候进行生产
#                 if count > 1000:
#                     con.wait()
#                 else:
#                     count = count+100
#                     msg = self.name+' produce 100, count=' + str(count)
#                     print(msg)
#                     # 完成生成后唤醒waiting状态的线程，
#                     # 从waiting池中挑选一个线程，通知其调用acquire方法尝试取到锁
#                     con.notify()
#                 con.release()
#                 time.sleep(1)

# class Consumer(threading.Thread):
#     # 消费者函数
#     def run(self):
#         global count
#         while True:
#             # 当count 大于等于100的时候进行消费
#             if con.acquire():
#                 if count < 100:
#                     con.wait()
                
#                 else:
#                     count = count-5
#                     msg = self.name+' consume 5, count='+str(count)
#                     print(msg)
#                     con.notify()
#                     # 完成生成后唤醒waiting状态的线程，
#                     # 从waiting池中挑选一个线程，通知其调用acquire方法尝试取到锁
#                 con.release()
#                 time.sleep(1)

# count = 500
# con = threading.Condition()

# # def test():
# #     for i in range(2):
# #         p = Producer()
# #         p.start()
# #     for i in range(5):
# #         c = Consumer()
# #         c.start()
# # if __name__ == '__main__':
# #     test()


# print(con.acquire())
# print(con.wait().list())
# print(con.release())



# import threading,time
 
#引入线程条件变量
# cond = threading.Condition()
 
# def run():
 
#     #使用with来使用线程条件变量
#     with cond:
 
#        #遍历数据，步长为2 ，既:0,2,4,6,8,10
#         for i in range(0,10,2):
 
#             #打印线程名称和步数   0
#             print (threading.current_thread().name,i)
 
#             #执行完成后等待信号（这里等待的是run1函数执行完打印步数和线程名称后的释放信号）
#             cond.wait()
 
#             #释放信号（这里是释放run1函数执行完打印步数和线程名称之后释放run1的信号）
#             cond.notify()
 
# def run1():
 
#     #使用with来使用线程条件变量
#     with cond:
 
#         #遍历数据，步长为2 ，既:1,3,5,7,9
#         for i in range(1,10,2):
 
#             #打印线程名称和步数   1
#             print (threading.current_thread().name,i)
 
#             #释放信号（这里释放的是run打印完步数 0 的时候等待的信息）
#             cond.notify()
 
#             #等待信号（这里是run1执行完打印线程名称和步数后进入等待状态）
#             cond.wait()

# def run2():
 
#     #使用with来使用线程条件变量
#     with cond:
 
#         #遍历数据，步长为2 ，既:1,3,5,7,9
#         for i in ['a','b','c','d','e','f','g','h','i','j']:
 
#             #打印线程名称和步数   1
#             print (threading.current_thread().name,i)
 
#             #释放信号（这里释放的是run打印完步数 0 的时候等待的信息）
#             cond.notify()
 
#             #等待信号（这里是run1执行完打印线程名称和步数后进入等待状态）
#             cond.wait()
 
# if __name__=="__main__":
    # threading.Thread(target=run).start()
    # threading.Thread(target=run1).start()
    # threading.Thread(target=run2).start()



# import time
# from threading import Condition, RLock

# def test_v1():
#     t = Condition()
#     a = 1

#     def test_th(n):
#         print('thread start ', n)
#         with t:
#             print('thread start wait ', n)
#             if t.wait(5):
#                 print('thread ', n)
#             else:
#                 print("************************* ", n)
#                 print("sleep ", n)
#                 time.sleep(n)
#                 print("sleep over ", n)

#             if n == 2:
#                 t.notify()

#     threads = [threading.Thread(target=test_th, args=(i, )) for i in range(1, 10)]
#     [t.start() for t in threads]

#     with t:
#         print("start notify ")
#         t.notify(2)


# if __name__ == '__main__':
#     test_v1()


# import threading
# import time
 
# con = threading.Condition()
# products = 0
 
# class Product(threading.Thread):
#     def run(self):
#         global products
#         while True:
#             print(self.getName(),": 等待生产环境")
#             con.acquire()
#             print(self.getName(),": 准备生产")
#             if products >= 20:
#                 print(self.getName(),": 放不下了，我先休息一下")
#                 con.notifyAll()
#                 con.wait()
#             else:
#                 print(self.getName(),": 生产中")
#                 time.sleep(1)
#                 products += 1
#                 print(self.getName(),": 完成了一个")
#                 print(self.getName(),": 一共有:",products,"个")
#                 con.notifyAll()
#             con.release()
 
#     def mycall(cls):
#         try:
#             con.acquire()
#         except RuntimeError as e:
#             print(e)
#         con.notify()
#         con.release()
 
# class Consumer(threading.Thread):
#     def run(self):
#         global products
#         while True:
#             if products > 0:
#                 print(self.getName(),": 消耗产品")
#                 products -= 1
#             else:
#                 print(self.getName(),": 没了")
#                 print(self.getName(),": 等一会,打电话通知下")
#                 Product().mycall()
#                 # time.sleep(3)
 
# if __name__ == "__main__":
#     p1 = Product(name="thread_A")
#     p2 = Product(name="thread_B")
#     c1 = Consumer(name="thread_C")
#     p1.start()
#     p2.start()
#     c1.start()
#     p1.join()
#     p2.join()
#     c1.join()
#     print("main")


# import threading
# import time
# L=[]
# class boy(threading.Thread):
#     def __init__(self,cond,name = 'A boy'):
           
#         threading.Thread.__init__(self)
#         self.cond = cond
#         self.name = name
#     def run(self):
#         time.sleep(1)
#         '''boy start conversation, make sure
#            the girl thread stared before send notify'''
#         self.cond.acquire()
#         print (self.name + ':Hello pretty~,I miss you\n')
#         self.cond.notify()
#         self.cond.wait()
#         print (self.name + ':like moth missing fire\n')
#         self.cond.notify()
#         self.cond.wait()
#         print (self.name + ':and I bought a gift for you in the list L\n')
#         L.append('channel5')
#         self.cond.notify()
#         self.cond.release()
           
# class girl(threading.Thread):
#     def __init__(self,cond,name = 'A girl'):
           
#         threading.Thread.__init__(self)
#         self.cond = cond
#         self.name = name
#     def run(self):
#         self.cond.acquire()
#         self.cond.wait()
#         print (self.name + ':Really, show me how much~\n')
#         self.cond.notify()
#         self.cond.wait()
#         print (self.name +':you\'re so sweet~')
#         self.cond.notify()
#         self.cond.wait()
#         print (self.name +':wow~~, that\'s '+L.pop()+'---the one I dreamed for so long, I love you')
#         self.cond.release()
# if __name__ == '__main__':
#     cond = threading.Condition()
#     husband = boy(cond, 'Aidan')
#     wife = girl(cond,'PPP')
#     husband.start()
#     wife.start()
#     #husband.start()
#     husband.join() #wait untill these two threads end
#     wife.join()
#     print ('end converdarion\n')


'''----------------------------------------------- ThreadLocal -------------------------------------------------'''

# # ThreadLocal是一个全局的字典，用每个线程的名称做为key去存储和访问变量，这样每个线程之间的数据就变得独立，互相不受到干扰。
# # 1.示例
# import time
# import threading
 
# v = threading.local()
 
# def func(arg):
#     # 内部会为当前线程创建一个空间用于存储：phone=自己的值
#     v.phone = arg
#     time.sleep(2)
#     print(f'{threading.current_thread().name} = v.phone:{v.phone},arg:{arg}') # 去当前线程自己空间取值
 
# for i in range(10):
#     t =threading.Thread(target=func,args=(i,))
#     t.start()


# # 2.原理
# import time
# import threading
 
# DATA_DICT = {}
 
# def func(arg):
#     ident = threading.get_ident()
#     DATA_DICT[ident] = arg
#     time.sleep(1)
#     print(f'ident:{DATA_DICT[ident]},arg:{arg}')
 
# for i in range(10):
#     t =threading.Thread(target=func,args=(i,))
#     t.start()


# # 3.拓展
# import time
# import threading
 
# INFO = {}
# class Local(object):
#     def __getattr__(self, item):
#         ident = threading.get_ident()
#         return INFO[ident][item]
#     def __setattr__(self, key, value):
#         ident = threading.get_ident()
#         if ident in INFO:
#             INFO[ident][key] = value
#         else:
#             INFO[ident] = {key: value}
# # 实例化
# obj = Local()
# def func(arg):
#     # 调用对象的 __setattr__方法（“phone”,1）
#     obj.phone = arg
#     time.sleep(2)
#     print(obj.phone, arg)
# if __name__ == '__main__':
#     for i in range(10):
#         t = threading.Thread(target=func, args=(i,))
#         t.start()


'''-------------------------------------------------- Event ----------------------------------------------------'''

# 通过threading.Event()可以创建一个事件管理标志，该标志( event )默认为False，event对象主要有四种方法可以调用：
#     event.wait(timeout=None)：调用该方法的线程会被阻塞，如果设置了timeout参数，超时后，线程会停止阻塞继续执行；
#     event.set()：将event的标志设置为True，调用wait方法的所有线程将被唤醒；
#     event.clear()：将event的标志设置为False，调用wait方法的所有线程将被阻塞；
#     event.isSet()：判断event的标志是否为True。


# import threading
 
# def test1(n, event):
#     print ('Thread %s is ready' % n)
#     event.wait(timeout=1)
#     print ('Thread %s is running' % n)
 
# def main():
#     event = threading.Event()
#     for i in range(4):
#         th = threading.Thread(target=test1, args=(i, event))
#         th.start()
 
# if __name__ == '__main__':
#     main()


# import threading, time
 
# def test(n, event):
#     while not event.isSet():
#         print ('Thread %s is ready' % n)
#         time.sleep(1)
#     event.wait()
#     while event.isSet():
#         print ('Thread %s is running' % n)
#         time.sleep(1)
 
# def main():
#     event = threading.Event()
#     for i in range(4):
#         threading.Thread(target=test, args=(i, event)).start()
#     #     th = threading.Thread(target=test, args=(i, event))
#     #     th.start()
#     # print(f'thread::{th}')
#     # time.sleep(10)
#     print ('----- event is set -----')
#     event.set()
#     # time.sleep(3)
#     print ('----- event is clear -----')
#     event.clear()
#     print ('----- event is over -----')
 
# if __name__ == '__main__':
#     main()


'''-------------------------------------------------- Semaphore ----------------------------------------------------'''

# import time, threading
# semaphore = threading.Semaphore(3)
 
# def func():
#     if semaphore.acquire():
#         print (threading.currentThread().getName() + '获取锁')
#         time.sleep(5) 
#         semaphore.release()
#         print (threading.currentThread().getName() + '释放锁')
  
# for i in range(10):
#     print('================================================')
#     t1 = threading.Thread(target=func)
#     t1.start()
#     print('------------------------------------------------')


'''--------------------------------------------------threading + queue------------------------------------------------------'''


import queue
import threading
import time

fifo_queue = queue.Queue()

def hd():
    print(f'{threading.current_thread().name}:start')
    time.sleep(1)
    print(f'{threading.current_thread().name}:finish')

for i in range(3):
    cc = threading.Thread(target=hd)
    fifo_queue.put(cc)

for th in range(fifo_queue.qsize()):
	fifo_queue.get().start()


# import threading
# import time
# from queue import Queue

# music_album = ['One hundred miles', 'Two hundred miles', 'Three hundred miles', 'Four hundred miles',
#                'Five hundred miles', 'Six hundred miles', 'Seven hundred miles', 'Eight hundred miles',
#                'Nine hundred miles', 'Ten hundred miles', ]

# q = Queue(3)

# class Music_Cols(threading.Thread):
#     def __init__(self, name):
#         super().__init__(name=name)

#     def run(self):
#         global music_album
#         global q
#         num = 0
#         while True:
#             try:
#                 music = music_album[num]
#                 q.put(music)
#             except IndexError:
#                 break
#             num += 1

# class Music_Play(threading.Thread):
#     def __init__(self, name):
#         super().__init__(name=name)

#     def run(self):
#         global q
#         while True:
#             if q.not_empty:
#                 music = q.get()
#                 print('{}正在播放{}'.format(threading.current_thread(), music))
#                 time.sleep(20)
#                 q.task_done()
#                 print('{}播放结束'.format(music))
#             else:
#                 break

# if __name__ == '__main__':
#     mc_thread = Music_Cols('music_cols')
#     mc_thread.setDaemon(True) # 设置为守护进程，主线程退出时，子进程也kill掉
#     mc_thread.start() # 启动进程
#     for i in range(10):
#         mp_thread = Music_Play('music_play')
#         mp_thread.setDaemon(True)
#         mp_thread.start()
# 	# 每当消息添加到队列时(put)，队列消息计数器就会增加，可以利用get取出消息，只有子线程调用task_done函数后，队列消息计数器减少，当队列消息为零也就是计数器为0时，join()取消阻塞。
#     q.join() # 阻塞，等待所有任务完成后取消阻塞




'''-----------------------------------------------------------------------------------------------------------------'''
'''------------------------------------------ ---- multiprocessing -------------------------------------------------'''
'''-----------------------------------------------------------------------------------------------------------------'''


'''----------------------------------------------------- Lock ------------------------------------------------------'''

# Process对象与Thread对象的用法相同，也有start(), run(), join()的方法。
# Process.PID中保存有PID，如果进程还没有start()，则PID为None
# # 所有的任务在打印的时候都会向同一个标准输出(stdout)输出。这样输出的字符会混合在一起，无法阅读。使用Lock同步，在一个任务输出完成之后，再允许另一个任务输出，可以避免多个任务同时向终端输出
# import os
# import threading
# import multiprocessing

# # Main
# print('Main:', os.getpid())

# # worker function
# def worker(sign, lock):
#     lock.acquire()
#     print(sign, os.getpid())
#     lock.release()

# # Multi-thread
# record = []
# lock = threading.Lock()

# # Multi-process
# record = []
# lock = multiprocessing.Lock()

# if __name__ == '__main__':
#     for i in range(5):
#         thread = threading.Thread(target=worker, args=('thread', lock))
#         thread.start()
#         record.append(thread)
#     for thread in record:
#         thread.join()
#     print('----------------')
#     for i in range(5):
#         process = multiprocessing.Process(target=worker, args=('process', lock))
#         process.start()
#         record.append(process)
#     for process in record:
#         process.join()


'''----------------------------------------------------- pipe ------------------------------------------------------'''

# # 由于进程之间不共享内存，所以进程之间的通信不能像线程之间直接引用，因而需要采取一些策略来完成进程之间的数据通信
# multiprocessing提供了threading包中没有的IPC(比如Pipe和Queue)，效率上更高。应优先考虑Pipe和Queue，避免使用Lock/Event/Semaphore/Condition等同步方式 (因为它们占据的不是用户进程的资源)。
# Pipe可以是单向(half-duplex)，也可以是双向(duplex)。我们通过mutiprocessing.Pipe(duplex=False)创建单向管道 (默认为双向)。一个进程从PIPE一端输入对象，然后被PIPE另一端的进程接收，单向管道只允许管道一端的进程输入，而双向管道则允许从两端输入。

# import multiprocessing as mtp

# def proc1(pipe):
#     pipe.send('hello')
#     print('proc1 rec:', pipe.recv())

# # def proc2(pipe):
# #     print('proc2 rec:', pipe.recv())
# #     pipe.send('hello, too')

# # Build a pipe
# pipe = mtp.Pipe()
# if __name__ == '__main__':
#     # Pass an end of the pipe to process 1
#     p1 = mtp.Process(target=proc1, args=(pipe[0],))
#     # Pass the other end of the pipe to process 2
#     # p2 = mtp.Process(target=proc2, args=(pipe[1],))
#     p1.start()
#     # p2.start()
#     p1.join()
#     # p2.join()

# 这里的Pipe是双向的。Pipe对象建立的时候，返回一个含有两个元素的表，每个元素代表Pipe的一端(Connection对象)。我们对Pipe的某一端调用send()方法来传送对象，在另一端使用recv()来接收。


import os
from multiprocessing import Process, Pipe

def talk(pipe):
    pipe.send(dict(name = 'Bob', spam = 42))
    reply = pipe.recv()
    print('talker got:', reply)

if __name__ == '__main__':

    (parentEnd, childEnd) = Pipe()

    child = Process(target = talk, name = 'talk', args = (childEnd,))
    child.start()
    print('parent got:', parentEnd.recv())
    parentEnd.send({x * 2 for x in 'spam'})
    child.join()

    print('parent exit')
    




# from multiprocessing import Process, current_process, Pipe
# import os, time
 
# def send(a,msgs):
#     print(f"这是send子进程的id: {os.getpid()}")

#     for i in range(msgs):
#         print ("发送 %s"%i)
#         a.send(i)
#     print(f'subs_a: {a.closed}')

# def recv(b):
#     print(f"这是recv子进程的id: {os.getpid()}")

#     while True:
#         try:
#             print ('proc2 接收:',b.recv())
#         except EOFError:
#             break
#     print(f'subs_b: {b.closed}')

# if __name__ == "__main__":

#     print(f"父进程的pid: {os.getpid()}")
#     a, b = Pipe(True)
 
#     p_send = Process(target=send, args=(a,100))
#     p_recv = Process(target=recv, args=(b,))

#     p_send.start()    
#     p_recv.start()
#     a.close()         ## 内部的close了，外部的也要close,只有当所有进程中的A端都关闭了才能算真正的close
#     p_send.join()
#     p_recv.join()
#     print("结束了")
#     print(f'mian_a: {a.closed}')
#     print(f'mian_b: {b.closed}')


# from multiprocessing import Process, current_process, Pipe
# import os, time
 
# def send(a,msgs):
#     print(f"这是send子进程的id: {os.getpid()}")

#     for i in range(msgs):
#         print ("发送 %s"%i)
#         a.send(i)
#     # a.close()   
 
# def recv(b):
#     print(f"这是recv子进程的id: {os.getpid()}")

#     while True: 
#         msg = b.recv() 
#         if msg == 40:
#             break
#         print("Received the message: {}".format(msg))   

# if __name__ == "__main__":

#     print(f"父进程的pid: {os.getpid()}")
#     a, b = Pipe(True)
 
#     p_send = Process(target=send, args=(a,100))
#     p_recv = Process(target=recv, args=(b,))

#     p_send.start()    
#     p_recv.start()
#     p_send.join()
#     p_recv.join()
#     print("结束了")
#     print(f'a: {a.closed}')
#     print(f'b: {b.closed}')



import multiprocessing
import time

n = 0
def proc1(pipe):
    while True:
    # for a in range(6):
        global n
        n += 1
        print('-------------------')
        for i in range(100):
            print ("%s 发送 %s"%(n,i))
            pipe.send((n,i))
            time.sleep(0.01)
        print('===================')

def proc2(pipe):
    while True:
        print ('proc2 接收:',pipe.recv())
        time.sleep(0.01)
 
def proc3(pipe):
    while True:
        print ('proc3 接收:',pipe.recv())
        time.sleep(0.01)

if __name__ == '__main__':
    # Build a pipe
    pipe = multiprocessing.Pipe()
    print (pipe)

    # Pass an end of the pipe to process 1
    p1   = multiprocessing.Process(target=proc1, args=(pipe[0],))
    # Pass the other end of the pipe to process 2
    p2   = multiprocessing.Process(target=proc2, args=(pipe[1],))
    p3   = multiprocessing.Process(target=proc3, args=(pipe[1],))


    p1.start()
    p2.start()
    p3.start()
    p1.join()
    p2.join()
    p3.join()




# from multiprocessing import Process,Pipe
# import os
 
# def proc1(pipe):
#     for i in range(1200):
#         print ("发送 %s"%(i))
#         pipe.send(i)
#     print('proc1 rec:',pipe.recv())
 
# def proc2(pipe):
#     while True:
#         try:
#             print('proc2 rec:',pipe.recv())
#         except:print('over')
            

#     pipe.send('hello too')

# if __name__=='__main__':
#     pipe = Pipe()
     
#     p1 = Process(target=proc1,args=(pipe[0],))
#     p2 = Process(target=proc2,args=(pipe[1],))
     
#     p1.start()
#     p2.start()
#     # p1.join()
#     # p2.join()


# from multiprocessing import current_process
# import multiprocessing
# import time

# def proc1(pipe,num):
#     # while True:
#     for a in range(2):
#         for i in range(num):
#             print ("%s 发送 %s"%(a,i))
#             pipe.send((pipe.fileno(),a,i))
#             # time.sleep(0.1)
#     pipe.close()
#     print('proc1: ',current_process().name,pipe.closed)

# def proc2(pipe):
#     while True:
#         try:
#             print ('proc2 接收:',pipe.recv())
#             # time.sleep(1)
#         except:
#             break
#     print('proc2: ',current_process().name,pipe.closed)

# def main(num):
#     pipe = multiprocessing.Pipe()
#     # print pipe

#     # Pass an end of the pipe to process 1
#     p1   = multiprocessing.Process(target=proc1, args=(pipe[0],num))
#     # Pass the other end of the pipe to process 2
#     p2   = multiprocessing.Process(target=proc2, args=(pipe[1],))
#     # p3   = multiprocessing.Process(target=proc3, args=(pipe[1],))

#     p1.start()
#     p2.start()

#     # p1.join()
#     # ####  p2.join()

#     # pipe[0].close()
#     # p1.join()
#     # p2.join()

#     print('main: ',current_process().name,pipe[0].fileno(),pipe[0].closed)

# if __name__=='__main__':
#     main(10)
#     print('-------------------')
#     main(20)

#     # pipe = multiprocessing.Pipe()
#     # # print pipe

#     # # Pass an end of the pipe to process 1
#     # p1   = multiprocessing.Process(target=proc1, args=(pipe[0],))
#     # # Pass the other end of the pipe to process 2
#     # p2   = multiprocessing.Process(target=proc2, args=(pipe[1],))
#     # # p3   = multiprocessing.Process(target=proc3, args=(pipe[1],))

#     # p1.start()
#     # p2.start()
#     # print(pipe[0].fileno())
#     # pipe[0].close()
#     # p1.join()

#     # print('main: ',current_process().name,pipe[0].closed)


# import time, random
# from multiprocessing import Process, Pipe, current_process
# from multiprocessing.connection import wait

# def foo(w):
#     for i in range(10):
#         w.send(f'foo_{i}:{current_process().name}:{w.fileno()}')
#         # print('-------------------------')
#         time.sleep(0.01)
#     # w.close()

# if __name__ == '__main__':
#     readers = []
#     writes = []
#     for i in range(4):
#         r, w = Pipe(duplex=False)
#         readers.append(r)
#         writes.append(w)
#         p = Process(target=foo, args=(w,))
#         p.start()
#         # We close the writable end of the pipe now to be sure that
#         # p is the only process which owns a handle for it.  This
#         # ensures that when p closes its handle for the writable end,
#         # wait() will promptly report the readable end as being ready.
#         w.close()

#     # print(readers,'--------', r)
#     # print(writes,'---------', w)
#     while readers:  #
#         for r in wait(readers):
#             try:
#                 print(r.recv(), r.fileno())
#             except EOFError:
#                 readers.remove(r)
#                 # break

#     # w.send('结束了')
#     # w.send('finished')
#     # wait([r])
#     # print(w.fileno(),'=====',r.fileno(),r.recv())
#     # print(w.fileno(),'=====',r.fileno(),r.recv())
#     w.close()
#     print(readers,'========', r)
#     print(writes,'========', w)

#     # for w in writes:
#     #     print(w.closed)

#     # print('all is over')


# from  multiprocessing import Pipe,Process
# def consumer(lp,rp):
#     lp.close()  #不写close将不会引发EOFError,消费者一直阻塞
#     while True:
#         try:
#             print(rp.recv())
#         except EOFError:break

# if __name__ == '__main__':
#     lp, rp =  Pipe()
#     Process(target=consumer,args=(lp,rp)).start()
#     Process(target=consumer,args=(lp,rp)).start()
#     Process(target=consumer,args=(lp,rp)).start()
#     Process(target=consumer,args=(lp,rp)).start()
#     rp.close()
#     for i in range(100):
#         lp.send('food%s'%i)
#     lp.close()


# from multiprocessing import Process, current_process, Pipe
# import time

# def consumer (left, right):
#     left.close()  # 因为右管道要发送信息，还要接收信息，这里用不到左管道
#     print(f'>>> {current_process().name}:右管道发送的信息：【你好】')
#     right.send("【你好】")  # 1.给左管道发送信息

#     while True:
#         try:
#             print(f"\t\t右管道接收的信息：{right.recv()}")
#         except EOFError:
#             break
#     print(f'{current_process().name}_left.closed:{left.closed}')
#     print(f'{current_process().name}_right.closed:{left.closed}')                
# # #-------------------------------------------------------------------------------
# # def main():
# #     left, right = Pipe()
# #     Process(target=consumer, args=(left, right)).start()
# #     print(f"\t\t{current_process().name}:左管道接收的信息：{left.recv()}")  # 2.接收右管道的信息

# #     for i in range(20):
# #         if i%5 == 0:
# #             time.sleep(1)
# #         print(f'\t>>> 左管道发送的信息：【{i}个包子】')
# #         left.send(f'【{i}个包子】')
# #     print(f'main_left.closed:{left.closed}')
# #     print(f'main_right.closed:{left.closed}')    
# # if __name__ == '__main__':    main()
# # #-------------------------------------------------------------------------------
# if __name__ == '__main__':
#     left, right = Pipe()
#     Process(target=consumer, args=(left, right)).start()
#     print(f"\t\t{current_process().name}:左管道接收的信息：{left.recv()}")  # 2.接收右管道的信息

#     for i in range(10):
#         print(f'\t>>> 左管道发送的信息：【{i}个包子】')
#         left.send(f'【{i}个包子】')

#     left.send('【拜拜】')
#     left.close()
#     print(f'{current_process().name}_left.closed:{left.closed}')
#     print(f'{current_process().name}_right.closed:{left.closed}')  



'''----------------------------------------------------- Queue ------------------------------------------------------'''

# Queue与Pipe相类似，都是先进先出的结构。但Queue允许多个进程放入，多个进程从队列取出对象。Queue使用mutiprocessing.Queue(maxsize)创建，maxsize表示队列中可以存放对象的最大数量。

# import multiprocessing
# import os, time
# #==================
# # input worker
# def inputQ(queue):
#     info = str(os.getpid()) + '(put):' + str(time.time())
#     queue.put(info)

# # output worker
# def outputQ(queue,lock):
#     info = queue.get()
#     lock.acquire()
#     print (str(os.getpid()) + ' get: ' + info)
#     lock.release()
# #===================
# # Main
# record1 = []   # store input processes
# record2 = []   # store output processes
# lock  = multiprocessing.Lock()    # To prevent messy print
# queue = multiprocessing.Queue(3)

# if __name__ == '__main__':
#     # input processes
#     for i in range(10):
#         process = multiprocessing.Process(target=inputQ,args=(queue,))
#         process.start()
#         record1.append(process)
#     # output processes
#     for i in range(10):
#         process = multiprocessing.Process(target=outputQ,args=(queue,lock))
#         process.start()
#         record2.append(process)
#     for p in record1:
#         p.join()
#     queue.close()  # No more object will come, close the queue
#     for p in record2:
#         p.join()


import multiprocessing,time

def put_worker(queue): #队列生产者
    for item in range(50): #生产50次数据
        time.sleep(0.01)
        print("【%s】生产者数据，item = %s" %(multiprocessing.current_process().name,item))
        queue.put("item = %s" % item)

def get_worker(queue):#队列消费者
    while True:
        try:
            print("【%s】消费数据：%s" % (multiprocessing.current_process().name,queue.get(block=True,timeout=2)))
        except:
            break

def main():#主函数
    queue = multiprocessing.Queue()#创建进程延迟队列
    pool = multiprocessing.Pool(4)
    producer_process = multiprocessing.Process(target=put_worker,name="生产者进程",args=(queue,))
    consumer_process = multiprocessing.Process(target=get_worker,name="消费者进程",args=(queue,))
    producer_process.start()
    consumer_process.start()
    producer_process.join()
    consumer_process.join()

if __name__ == '__main__':
    main()
    print('---------------')
    main()


# import os
# import multiprocessing
# import time
# # 写入 worker
# def inputQ(queue):
#     while True:
#         info = "进程号 %s : 时间: %s"%(os.getpid(),int(time.time()))
#         queue.put(info)
#         time.sleep(0.01)
# # 获取 worker
# def outputQ(queue,lock):
#     while True:
#         info = queue.get()
# #        lock.acquire()
#         print (str(os.getpid()) + '(get):' + info)
# #        lock.release()
#         time.sleep(0.01)
# #===================
# if __name__=='__main__':
#     # Main
#     record1 = []   # store input processes
#     record2 = []   # store output processes
#     lock  = multiprocessing.Lock()    # To prevent messy print
#     queue = multiprocessing.Queue(3)
     
#     # input processes
#     for i in range(10):
#         process = multiprocessing.Process(target=inputQ,args=(queue,))
#         process.start()
#         record1.append(process)
     
#     # output processes
#     for i in range(10):
#         process = multiprocessing.Process(target=outputQ,args=(queue,lock))
#         process.start()
#         record2.append(process)

'''---------------------------------------------------- manager -----------------------------------------------------'''

# 在多线程中，我们可以比较容易地共享资源，比如使用全局变量或者传递参数。在多进程情况下，由于每个进程有自己独立的内存空间，以上方法并不合适。此时我们可以通过共享内存和Manager的方法来共享资源。但这样做提高了程序的复杂度，并因为同步的需要而降低了程序的效率。

# from multiprocessing import Process, Manager
# from time import sleep
 
# def thread_a_main(sync_data_pool):  # A 进程主函数，存入100+的数
#     for ix in range(100, 105):
#         sleep(1)
#         sync_data_pool.append(ix)
 
# def thread_b_main(sync_data_pool):  # B 进程主函数，存入300+的数
#     for ix in range(300, 309):
#         sleep(0.6)
#         sync_data_pool.append(ix)
 
# def _test_case_000():  # 测试用例
#     manager = Manager()  # multiprocessing 中的 Manager 是一个工厂方法，直接获取一个 SyncManager 的实例
#     sync_data_pool = manager.list()  # 利用 SyncManager 的实例来创建同步数据池
#     Process(target=thread_a_main, args=(sync_data_pool, )).start()  # 创建并启动 A 进程
#     Process(target=thread_b_main, args=(sync_data_pool, )).start()  # 创建并启动 B 进程
#     for ix in range(6):  # C 进程（主进程）中实时的去查看数据池中的数据
#         sleep(1)
#         print(sync_data_pool)
 
# if '__main__' == __name__:  # 养成好习惯，将测试用例单独列出
#     _test_case_000()
 

# Manager是通过共享进程的方式共享数据。
# Manager管理的共享数据类型有：Value、Array、dict、list、Lock、Semaphore等等，同时Manager还可以共享类的实例对象。

# from multiprocessing import Process,Manager

# def func1(shareList,shareValue,shareDict,lock):
#     with lock:
#         shareValue.value+=1
#         shareDict[1]='1'
#         shareDict[2]='2'
#         for i in range(len(shareList)):
#             shareList[i]+=1

# if __name__ == '__main__':
#     manager=Manager()
#     list1=manager.list([1,2,3,4,5])
#     dict1=manager.dict()
#     array1=manager.Array('i',range(10))
#     value1=manager.Value('i',1)
#     lock=manager.Lock()
#     proc=[Process(target=func1,args=(list1,value1,dict1,lock)) for i in range(20)]
#     for p in proc:
#         p.start()
#     for p in proc:
#         p.join()
#     print (list1)
#     print (dict1)
#     print (array1)
#     print (value1)


'''---------------------------------------------------- array -----------------------------------------------------'''

# 共享内存：array
# 这里实际上只有主进程和Process对象代表的进程。我们在主进程的内存空间中创建共享的内存，也就是Value和Array两个对象。对象Value被设置成为双精度数(d), 并初始化为1.0。而Array则类似于C中的数组，有固定的类型(i, 也就是整数)。在Process进程中，我们修改了Value和Array对象。回到主程序，打印出结果，主程序也看到了两个对象的改变，说明资源确实在两个进程之间共享。

# import multiprocessing

# # Value/Array
# def func1(a, arr):
#     a.value = 3.14
#     for i in range(len(arr)):
#         arr[i] = 0
#     a.value = 6

# if __name__ == '__main__':
#     num = multiprocessing.Value('d', 1.0)  # num=0
#     arr = multiprocessing.Array('i', range(10))  # arr=range(10)
#     p = multiprocessing.Process(target=func1, args=(num, arr))
#     p.start()
#     p.join()
#     print (num.value)
#     print (arr[:])



'''---------------------------------------------------------------------------------------------------------------------'''
'''--------------------------------------------------------- pool ------------------------------------------------------'''
'''---------------------------------------------------------------------------------------------------------------------'''


'''---------------------------------------------------- multiprocessing ------------------------------------------------'''

''' # 进程池:
''' # 注  意： 在Windows上要想使用进程模块，就必须把有关进程的代码写在当前.py文件的if __name__ == ‘__main__’ :语句的下面，才能正常使用Windows下的进程模块。Unix/Linux下则不需要

# import multiprocessing as mp
# import  time ,os ,random

# def worker(msg):
#     t_start = time.time() #获取当前系统时间，长整型，常用来测试程序执行时间
#     print("%s 开始执行,进程名为%d" % (f'进程：{mp.current_process().name}',os.getpid()))
#     time.sleep(5)
#     t_stop = time.time()
#     print(f'参数：{msg}',"执行完毕，耗时%0.2f" % (t_stop-t_start))
#     return f'msg:{msg}'
# results = []
# if __name__ == '__main__':

#     p_start = time.time() #获取当前系统时间，长整型，常用来测试程序执行时间
#     pool= mp.Pool(3)# 定义一个进程池，最大进程数3，大小可以自己设置，也可写成processes=3
#     for i in range(0,10):
#         # Pool().apply_async(要调用的目标,(传递给目标的参数元祖,))
#         # 每次循环将会用空闲出来的子进程去调用目标
#         print('pool apply_async start')
#         result = pool.apply_async(worker,(i,))
#         print(f'{pool} apply_async taken')
#         print('foreach:',results.append(result))

#     print("----start----")
#     pool.close()  # 关闭进程池，关闭后po不再接收新的请求
#     print(f'{pool} has closed')
#     pool.join()  # 等待po中所有子进程执行完成，必须放在close语句之后
#     print(f'{pool} has joined')
#     print([res.get() for res in results])
#     print("-----end-----")
#     print(f'耗时:{p_start-time.time()}')

#-------------------------------------------------------------------

# import multiprocessing as mp
# import os, time
# def cube(x):
#     print(mp.current_process().name,f'参数:{x}')
#     print('等待开始......')
#     time.sleep(3)
#     print('等待结束。。。。。')
#     return x**3

# if __name__ == "__main__":
#     t1 = time.time()
#     pool    = mp.Pool(4)  # processes=4
#     results = [pool.apply_async(cube, args=(x,)) for x in range(1,10)]
#     print("----start----")
#     pool.close()  # 关闭进程池，关闭后po不再接收新的请求
#     print(f'{pool} has closed')
#     pool.join()  # 等待po中所有子进程执行完成，必须放在close语句之后
#     print(f'{pool} has joined')
#     print(res.get() for res in results)
#     print("-----end-----")
#     t2 = time.time()
#     print(f'用时：{t2-t1}')

#-------------------------------------------------------------------

# import os,time
# from multiprocessing import Pool

# def run(fn):
#     # fn: 函数参数是数据列表的一个元素
#     print(f'进程id: {os.getpid()}')
#     print('等待开始......')    
#     time.sleep(3)
#     print('等待结束。。。。。')
#     # print(fn * fn)
#     return f'返回值：{fn * fn * fn}'

# results = []
# if __name__ == "__main__":
#     testFL = [1, 2, 3, 4, 5, 6,7,8,9,10]
#     print('shunxu:')  # 顺序执行(也就是串行执行，单进程)
#     s = time.time()
#     for fn in testFL:
#         run(fn)
#     t0 = time.time()
#     print("顺序执行时间：", int(t0 - s))

#     print('concurrent:')  # 创建多个进程，并行执行
#     t1 = time.time()
#     pool = Pool(3)  # 创建拥有3个进程数量的进程池
#     # testFL:要处理的数据列表，run：处理testFL列表中数据的函数
#     for fn in testFL:
#         result = pool.apply_async(run, (fn,))
#         results.append(result)

#     print("----start----")
#     pool.close()  # 关闭进程池，不再接受新的进程
#     print(f'{pool} has closed')    
#     pool.join()  # 主进程阻塞等待子进程的退出
#     print(f'{pool} has joined')
#     print([res.get() for res in results])
#     print("-----end-----")
#     t2 = time.time()
#     print("并行执行时间：", int(t2 - t1))


## 线程池：
## 这里的多线程也是受到它受到全局解释器锁（GIL）的限制，并且一次只有一个线程可以执行附加到CPU的操作。

# from multiprocessing.dummy import Pool as mpdpool
# import os, time, random, threading

# def fun(msg):
#     print(f'参数: {msg}', threading.current_thread().name)
#     time.sleep(1)
#     print('********')
#     return 'fun_return %s' % msg
 
 
# # apply
# print('\n------apply-------')
# pool = mpdpool(processes=4)
# results =[]
# for i in range(5):
#     msg = 'msg: %d' % i
#     result = pool.apply(fun, (msg, ))
#     results.append(result)
 
# print('apply: 堵塞')
# print(results)


# # apply_async
# print('\n------apply_async-------')
# async_pool = mpdpool(processes=4)
# results =[]
# for i in range(10):
#     msg = 'msg: %d' % i
#     result = async_pool.apply_async(fun, (msg, ))
#     print(f'apply_async result: {result}')
#     results.append(result)

# print('apply_async: 不堵塞')
# async_pool.close()
# async_pool.join()
# for i in results:
#     i.wait()  # 等待线程函数执行完毕
# print('get the func return:') 
# for i in results:
#     if i.ready():  # 线程函数是否已经启动了
#         if i.successful():  # 线程函数是否执行成功
#             print(i.get())  # 线程函数返回值

 
# # map
# print('\n------map-------')
# arg = [3, 5, 11, 19, 12,7,6,8,12,21,34,13]
# t1 = time.time()
# pool = mpdpool(processes=3)
# return_list = pool.map(fun, arg)  ## 注意：虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程
# print('map: 堵塞')
# pool.close()
# print(f'{pool} has closed')
# pool.join()
# print(f'{pool} has joined')
# print(return_list)
# print("-----end-----")
# t2 = time.time()
# print(f'用时：{t2-t1}')


# # map_async
# print('\n------map_async-------')
# arg = [3, 5, 11, 19, 12,7,6,8,12,21,34,13]
# t1 = time.time()
# async_pool = mpdpool(processes=4)
# result = async_pool.map_async(fun, arg)
# print(f'函数启动{result.ready()}')  # 线程函数是否已经启动了
# print('map_async: 不堵塞')
# result.wait()  # 等待所有线程函数执行完毕
# print('after wait')
# if result.ready():  # 线程函数是否已经启动了
#     if result.successful():  # 线程函数是否执行成功
#         print(result.get())  # 线程函数返回值
# print("-----end-----")
# t2 = time.time()
# print(f'用时：{t2-t1}')


#-------------------------------------------------------------------
#-------------------------------------------------------------------


# 进程池的使用有四种方式：apply_async、apply、map_async、map。其中apply_async和map_async是异步的，也就是启动进程函数以后会继续执行后续的代码不用等待进程函数返回。
# apply_async和map_async方式提供了一些获取进程函数状态的函数： ready()、 successful()、 get()。
# 线程池的使用方式和进程池相似。


# from multiprocessing.dummy import Pool as ThreadPool
# import time


# def fun(msg):
#     print('msg: ', msg)
#     time.sleep(1)
#     print('********')
#     return 'fun_return %s' % msg


# # map_async
# print('\n------map_async-------')
# arg = [1, 2, 10, 11, 18]
# async_pool = ThreadPool(processes=4)
# result = async_pool.map_async(fun, arg)
# print(result.ready())  # 线程函数是否已经启动了
# print('map_async: 不堵塞')
# result.wait()  # 等待全部线程函数执行完毕
# print('after wait')
# if result.ready():  # 线程函数是否已经启动了
#     if result.successful():  # 线程函数是否执行成功
#         print(result.get())  # 线程函数返回值

# # map
# print('\n------map-------')
# arg = [3, 5, 11, 19, 12]
# pool = ThreadPool(processes=3)
# return_list = pool.map(fun, arg)
# print('map: 堵塞')
# pool.close()
# pool.join()
# print(return_list)

# # apply_async
# print('\n------apply_async-------')
# async_pool = ThreadPool(processes=4)
# results =[]
# for i in range(5):
#     msg = 'msg: %d' % i
#     result = async_pool.apply_async(fun, (msg, ))
#     results.append(result)

# print('apply_async: 不堵塞')
# # async_pool.close()
# # async_pool.join()
# for i in results:
#     i.wait()  # 等待线程函数执行完毕

# for i in results:
#     if i.ready():  # 线程函数是否已经启动了
#         if i.successful():  # 线程函数是否执行成功
#             print(i.get())  # 线程函数返回值

# # apply
# print('\n------apply-------')
# pool = ThreadPool(processes=4)
# results =[]
# for i in range(5):
#     msg = 'msg: %d' % i
#     result = pool.apply(fun, (msg, ))
#     results.append(result)

# print('apply: 堵塞')
# print(results)

# ---------------------------------------------------------------------


# from multiprocessing.dummy import Pool
# import os, time, threading

# def func(msg):
#     print(f'传递:{msg}\n')
#     time.sleep(2)
#     print(f'{threading.current_thread()} is running\n')
#     return f'返回：{msg}'
    
# pool = Pool(processes=3)
# result = []
# for i in range(1, 5):
#     msg = 'hello %d' % (i)
#     res = pool.apply_async(func,(msg,))
#     result.append(res)

# print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
# pool.close()
# print ('-------------------------------')
# pool.join()  # 调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束
# for res in result:
#     print (f"sub_process return:  {res.get()}\n")
# print ('sub-process done')


# def task(i):
#     print("{} begin".format(threading.current_thread().name))
#     print(i)#1-5(1-5输出的顺便随机) 6-10 11-15 16-20 (每次输出的随机) 每次5个线程进行处理 但是先后顺序没有关系，不影响输出的顺序
#     time.sleep(5)
#     return i
#     # print("{} end".format(threading.current_thread().name))

# if __name__ == "__main__":
#     a=range(1,21)
#     thread_count=5
#     P=Pool(thread_count)
#     #map  每次从iter a(1-20)调用5个元素分别给5个task 进行并行处理，所以会进行4(20/5)次 
#     #map  每次得到的结果都会等4次5个task都处理完，同时将最后的结果按照传入的顺序转为list输出
#     listres=P.map(task,a,chunksize=1)
#     print(listres)#得到结果list，最后一次输出
#     print('task has done')
#    # P.map(task,a,chunksize=2)
#    # 1-20 每次从iter a调用5个元素（隔chunksize个元素）分别给5个task 五个task 进行并行处理，
#    # 最后还是输出[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]


# def task(i):
#     print("{} begin".format(threading.current_thread().name))
#     print(i)#1-5(1-5输出的顺便随机) 6-10 11-15 16-20 (每次输出的随机) 每次5个线程进行处理 但是先后顺序没有关系，不影响输出的顺序
#     time.sleep(1)
#     return i
#     # print("{} end".format(threading.current_thread().name))

# if __name__ == "__main__":
#     a=range(1,21)
#     thread_count=5
#     P=Pool(thread_count)
#     # pool.imap_unordered,终端输出：可以不是按照a的顺序，每次处理完一个线程函数task，就会返回一个结果
#     for i,res in enumerate(P.imap(task,a,chunksize=2)):
#         print(f"res: {res}")

# imap作用同map 但imap是一个存放所有结果的迭代器 需要在主进程中主动使用next来驱动子进程的调用，
# 可以不用等5个线程都处理完才返回，这里每次处理完一个task函数将得到一个迭代器结果
# 输出：最终res还是按照1-20的顺序进行输出，chunksize=2时输出相同，此时res:2需要等第2次5个task处理时才会输出


