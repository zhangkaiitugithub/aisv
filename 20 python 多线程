

'''--------------------------------------------------------------------------------------------------------------------------'''


# import threading
# import multiprocessing.dummy as dummy

# # threading: 'current_thread',  'Lock', 'RLock', 'Condition', 'Event', 'Barrier', 'Semaphore', 'BoundedSemaphore'
# # mtpsdummy: 'current_process', 'Lock', 'RLock', 'Condition', 'Event', 'Barrier', 'Semaphore', 'BoundedSemaphore', 'Pipe', 'Queue',, 'JoinableQueue', 'Manager', 'Pool'


'''--------------------------------------------------------------------------------------------------------------------------'''
'''------------------------------------------------------ threading ---------------------------------------------------------'''
'''--------------------------------------------------------------------------------------------------------------------------'''

## 多线程:

''' threading.current_thread(),threading.enumerate(),threading.current_thread().getName(),threading.current_thread().name
    name 是当前线程的属性， getName 是当前线程的方法。
    尽管 threading.current_thread().name 和 threading.current_thread().getName() 的结果一样，但是完全不是同一种东西呀， 例如通过 threading.current_thread().name ＝ ‘thread_python’ 来改变它。    

    import threading, time
    def run(arg):
        print("running sub thread...{}".format(threading.current_thread()))
        threading.current_thread().name="xurui_python"
        print("sub1 Thread...{}".format(threading.current_thread().getName()))
        print("sub2 Thread...{}".format(threading.current_thread().name))
        time.sleep(3)
    if __name__ == "__main__":
        t1 = threading.Thread(target=run,args=("t1",))
        t1.start()
        print("mian1 Thread...{}".format(threading.current_thread().getName()))
        print("mian2 Thread...{}".format(threading.current_thread().name))
''' 



# import threading
# def func(n):
#     while n > 0:
#         print("当前线程数:", threading.activeCount())
#         n -= 1
# for x in range(5):
#     t = threading.Thread(target=func, args=(5,))
#     t.start()

# print("主线程：", threading.current_thread().name)


# import threading
# def test():
#     x = 0
#     for i in range(5):
#         x = i + x
#         print(x)
#     print(str(threading.enumerate())) # 打印出当前进程中的所有线程

# if __name__ == '__main__':
#     thread = threading.Thread(target=test)
#     thread.start()
#     print('nihao')


# import threading
# def test():
#     x = 0
#     for i in range(5):
#         x = i + x
#         print(x)
#     print(str(threading.enumerate()))  # 打印出当前进程中的所有线程
# if __name__ == '__main__':
#     thread = threading.Thread(target=test)
#     thread.setDaemon(True)
#     thread.start()
#     print('nihao')


# import threading
# def test1():
#     while True:
#         print('nihao')
# def test2():
#     while True:
#         print(str(threading.enumerate()))
# if __name__ == '__main__':
#     thread1 = threading.Thread(target=test1)
#     thread2 = threading.Thread(target=test2)
#     thread1.setDaemon(True)
#     thread1.start()
#     thread2.start()


# (1)setDaemon(True) 将子线程设置为守护进程（默认False）， 主线程结束后，守护子线程随之中止。
# (2)join() 用于阻塞主线程， 可以想象成将某个子线程的执行过程插入(join)到主线程的时间线上，主线程的后续代码延后执行。 注意和 t.start() 分开写在两个for循环中。
# (3)第一个for循环同时启动了所有子线程，随后在第二个for循环中执行t.join() ， 主线程实际被阻塞的总时长==其中执行时间最长的一个子线程。


# import threading
# import time

# def run():
#     time.sleep(2)
#     print(f'当前线程的名字是： {threading.current_thread().name}\n')
#     time.sleep(2)

# if __name__ == '__main__':
#     start_time = time.time()
#     print(f'这是主线程：{threading.current_thread().name}\n')
#     thread_list = []
#     for i in range(5):
#         t = threading.Thread(target=run)
#         thread_list.append(t)

#     for t in thread_list:
#         t.setDaemon(True)
#         print(f'{t.getName()}:开始\n')  # 获取子线程名字
#         t.start()
#     print('-----------------------------')
#     for t in thread_list:
#         print(f'{t.getName()}:阻塞\n')  # 获取子线程名字
#         t.join()
#     print('=============================')        
#     for t in thread_list:
#         print(t.is_alive())  # 判断线程是否在运行

#     print(f'主线程结束！{threading.current_thread().name}\n')
#     print(f'一共用时：{time.time()-start_time}\n')

    # print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~') 
    # #### thread_list[2].run()  # 线程被cpu调度后自动执行线程对象的run方法
    # thread_list[2].setName('第一个线程')  # 设置线程名字
    # print(thread_list[2].getName())
    # thread_list[2].start()
    # print(thread_list[2].is_alive())  # 判断线程是否在运行
    # print(thread_list[2].getName())

# setDaemon(True)：将线程声明为守护线程，必须在start()方法调用之前设置。这个方法基本和join是相反的。
# 当我们在程序运行中，执行一个主线程，如果主线程又创建一个子线程，主线程和子线程就分兵两路，分别运行，那么当主线程完成
# 想退出时，会检验子线程是否完成。如果子线程未完成，则主线程会等待子线程完成后再退出。但是有时候我们需要的是只要主线程
# 完成了，不管子线程是否完成，都要和主线程一起退出，这时就可以用setDaemon()方法



'''-------------------------------------------------join---------------------------------------------------'''


# import threading, time
 
# def action(arg):
#     time.sleep(1)
#     print('sub thread start!the thread name is:%s    ' % threading.currentThread().getName())
#     print('the arg is:%s   ' %arg)
#     time.sleep(1)
 
# # #不正确写法，会导致多线程顺序执行，失去了多线程的意义
# # for i in range(4):
# #     t =threading.Thread(target=action,args=(i,))
# #     t.setDaemon(True)
# #     t.start()
# #     t.join()
 
# #正确写法
# thread_list = []    #线程存放列表
# for i in range(4):
#     t =threading.Thread(target=action,args=(i,))
#     t.setDaemon(True)
#     thread_list.append(t)
 
# for t in thread_list:
#     t.start()
 
# for t in thread_list:
#     t.join()
# print('main_thread end!')



'''------------------------------------------------- Lock ---------------------------------------------------'''

# 这个锁是不被某个特定线程所拥有的。一个线程获取了锁，之后任何线程尝试着获取锁，都会失败。
# 只有在这个范围内的代码才受到lock锁的约束，意思是只有请求到lock这个对象的锁时，才能执行。所有线程抢锁，但只有一个能抢上,等到这个lock锁释放时，别的lock锁才能申请锁的权限，否则别的lock.acquire()这里只能等待。
# import threading,time,random

# count = 0
# class MyThread_lock(threading.Thread):
#     def __init__(self,lock,threadName):
#         super(MyThread_lock,self).__init__(name=threadName)
#         self.lock = lock
#     def run(self):
#         global count
#         self.lock.acquire()  #acquire()方法提供了确定对象被锁定的标志
#         for i in range(5):
#             count +=1
#             time.sleep(2)
#             print(self.getName(),count)
#         self.lock.release()  #release()在对象被当前线程使用完毕后将当前对象释放
# lock = threading.Lock()
# for i in range(2):
#     MyThread_lock(lock,"MyThreadName:"+str(i)).start()


# # 多线程锁使用方法
# import threading,time
# sume, loop = 0, 100

# #生成锁实例
# lock = threading.Lock()
# def myadd():
#     global sume,loop #声明全局变量只能在代码块中有效
#     for i in range(1,loop):
#         lock.acquire()  #申请锁，就是加锁
#         sume += 1
#         print(threading.current_thread().name,sume)
#         lock.release()  #释放锁

# def mysub():
#     global sume,loop
#     for i in range(1,loop):
#         lock.acquire()  #申请锁，就是加锁
#         sume -= 1
#         print(threading.current_thread().name,sume)
#         lock.release() #释放锁

# #本模块作为主进程运行时执行下面的操作
# if __name__ == '__main__':
#     print("开始:{}={}".format(threading.current_thread().name,sume))
#     t1 = threading.Thread(target=myadd,args=())
#     t2 = threading.Thread(target=mysub,args=())
#     t1.start()
#     t2.start()
#     t1.join()
#     t2.join()
#     print("结束:{}={}".format(threading.current_thread().name,sume))


# import time, threading
 
# # 假定这是你的银行存款:
# balance = 0
# lock = threading.Lock() 
# def change_it(n):
#     # 先存后取，结果应该为0:
#     global balance    
#     balance = balance + n
#     print(f'{threading.current_thread().name} = +{balance}')
#     balance = balance - n
#     print(f'{threading.current_thread().name} = -{balance}')

#         # (1)==>
#         # t1和t2是交替运行的，如果操作系统以下面的顺序执行t1、t2：
#         # 初始值 balance = 0
#         # t1: x1 = balance + 5  # x1 = 0 + 5 = 5
#         # t2: x2 = balance + 8  # x2 = 0 + 8 = 8
#         # t2: balance = x2      # balance = 8
         
#         # t1: balance = x1      # balance = 5
#         # t1: x1 = balance - 5  # x1 = 5 - 5 = 0
#         # t1: balance = x1      # balance = 0
         
#         # t2: x2 = balance - 8  # x2 = 0 - 8 = -8
#         # t2: balance = x2   # balance = -8
         
#         # 结果 balance = -8
#         # 究其原因，是因为修改balance需要多条语句，而执行这几条语句时，线程可能中断，从而导致多个线程把同一个对象的内容改乱了。
#         # 两个线程同时一存一取，就可能导致余额不对，你肯定不希望你的银行存款莫名其妙地变成了负数，所以，我们必须确保一个线程在修改balance的时候，别的线程一定不能改。 

# def run_thread(n):
#     for i in range(100000):

#         lock.acquire()    #### 要确保balance计算正确，就要给change_it()上一把锁，当某个线程开始执行change_it()时，我们说，该线程因为获得了锁，因此其他线程不能同时执行change_it()，只能等待，直到锁被释放后，获得该锁以后才能改。由于锁只有一个，无论多少线程，同一时刻最多只有一个线程持有该锁，所以，不会造成修改的冲突。创建一个锁就是通过threading.Lock()来实现：
#         change_it(n)
#         lock.release()

# for i in range(2):
#     threading.Thread(target=run_thread, args=(i*5+3,)).start()

# # t1 = threading.Thread(target=run_thread, args=(5,))
# # t2 = threading.Thread(target=run_thread, args=(8,))
# t1.start()
# t2.start()
# t1.join()
# t2.join()
# print(balance)      


# from threading import Lock,Thread
# from time import sleep,time
# lock = Lock()
# list1 = [0]*10

# def putR(s):
#     lock.acquire()
#     for i in range(len(list1)):
#         list1[i] = i
#         print("put ",list1[i])
#         sleep(s)
#     lock.release()
# def getR(s):
#     lock.acquire()
#     for i in range(len(list1)):
#         print("get ",list1[i])
#         sleep(s)
#     lock.release()

# if __name__ == "__main__":

#     t1 = Thread(target=putR, name="aa", args=(0.1,))
#     t2 = Thread(target=getR, name="aa", args=(0.5,))

#     t1.start()
#     t2.start()
#     t1.join()
#     t2.join()


# # 使用一个锁
# import threading, time
 
# def num():
#     for i in range(1, 53): # ,2
#         lock.acquire()
#         print(f'{threading.current_thread().name},{i}')
# def alpha():
#     for i in range(65, 91):
#         time.sleep(2)
#         # time.sleep(0.000001)
#         print(f'{threading.current_thread().name},{i}')
#         lock.release()
# if __name__ == '__main__':
#     lock = threading.Lock()
#     sn = threading.Thread(target=num)
#     sa = threading.Thread(target=alpha)
#     sn.start()
#     sa.start()


# # # 使用多个锁
# import threading,time
# lock1=threading.Lock()
# lock2=threading.Lock()
# # lock1 和lock2 是两把不同的锁，二者可以同时存在，不冲突，但是lock1本身只能有一把锁存在
# def do_t1(name):
#        lock1.acquire()
#        print(name)
#        time.sleep(10)
#        print('I am function t1')
#        lock1.release()

# def do_t2(name):
#        lock2.acquire()
#        print(name)
#        print('I am function t2')
#        lock2.release()

# t1=threading.Thread(target=do_t1,args=('th_t1',))
# t2=threading.Thread(target=do_t2,args=('th_t2',))

# t1.start()
# time.sleep(1)
# t2.start()
# t1.join()
# t2.join()

# print("ok")


# import threading, time

# alpha_lock = threading.Lock()
# num_lock = threading.Lock()
# def print_num():
#     num_lock.acquire() #先加锁，当输出2自己在获取锁时必须等待另一进程释放
#     for i in range(1, 50):
#         print(f'{threading.current_thread().name},{i}\n')
#         num_lock.acquire()
#         alpha_lock.release()
# def print_alpla():
#     for w in range(1,20):
#         print(f'{threading.current_thread().name},{w}\n')
#         num_lock.release()
#         alpha_lock.acquire()
# if __name__ == '__main__':
#     num_thread = threading.Thread(target=print_num,)
#     alpha_thread = threading.Thread(target=print_alpla)
#     alpha_lock.acquire()
#     num_thread.start()
#     alpha_thread.start()


#=========================================================================#

# import time
# import multiprocessing.dummy as dummy

# alpha_lock = dummy.Lock()
# num_lock = dummy.Lock()
# def print_num():
#     num_lock.acquire() #先加锁，当输出2自己在获取锁时必须等待另一进程释放
#     for i in range(1, 50):
#         print(f'{dummy.threading.current_thread().name},{i}\n')
#         num_lock.acquire()
#         alpha_lock.release()
# def print_alpla():
#     for w in range(1,20):
#         print(f'{dummy.threading.current_thread().name},{w}\n')
#         num_lock.release()
#         alpha_lock.acquire()
# if __name__ == '__main__':
#     num_thread = dummy.threading.Thread(target=print_num,)
#     alpha_thread = dummy.threading.Thread(target=print_alpla)
#     alpha_lock.acquire()
#     num_thread.start()
#     alpha_thread.start()


'''---------------------------------------------------- condition --------------------------------------------------'''


# threading.Condition 是一个继承自threading.Lock的一个类,Condition的处理流程如下：
# 首先acquire一个条件变量，然后判断一些条件。
# 如果条件不满足则wait；
# 如果条件满足，进行一些处理改变条件后，通过notify方法通知其他线程，其他处于wait状态的线程接到通知后会重新判断条件。
# 不断的重复这一过程，从而解决复杂的同步问题。


# import threading
# import time

# class Producer(threading.Thread):
#     # 生产者函数
#     def run(self):
#         global count
#         while True:
#             if con.acquire():
#                 # 当count 小于等于1000 的时候进行生产
#                 if count > 1000:
#                     con.wait()
#                 else:
#                     count = count+100
#                     msg = self.name+' produce 100, count=' + str(count)
#                     print(msg)
#                     # 完成生成后唤醒waiting状态的线程，
#                     # 从waiting池中挑选一个线程，通知其调用acquire方法尝试取到锁
#                     con.notify()
#                 con.release()
#                 time.sleep(1)

# class Consumer(threading.Thread):
#     # 消费者函数
#     def run(self):
#         global count
#         while True:
#             # 当count 大于等于100的时候进行消费
#             if con.acquire():
#                 if count < 100:
#                     con.wait()
                
#                 else:
#                     count = count-5
#                     msg = self.name+' consume 5, count='+str(count)
#                     print(msg)
#                     con.notify()
#                     # 完成生成后唤醒waiting状态的线程，
#                     # 从waiting池中挑选一个线程，通知其调用acquire方法尝试取到锁
#                 con.release()
#                 time.sleep(1)

# count = 500
# con = threading.Condition()

# # def test():
# #     for i in range(2):
# #         p = Producer()
# #         p.start()
# #     for i in range(5):
# #         c = Consumer()
# #         c.start()
# # if __name__ == '__main__':
# #     test()


# print(con.acquire())
# print(con.wait().list())
# print(con.release())



# import threading,time
 
#引入线程条件变量
# cond = threading.Condition()
 
# def run():
 
#     #使用with来使用线程条件变量
#     with cond:
 
#        #遍历数据，步长为2 ，既:0,2,4,6,8,10
#         for i in range(0,10,2):
 
#             #打印线程名称和步数   0
#             print (threading.current_thread().name,i)
 
#             #执行完成后等待信号（这里等待的是run1函数执行完打印步数和线程名称后的释放信号）
#             cond.wait()
 
#             #释放信号（这里是释放run1函数执行完打印步数和线程名称之后释放run1的信号）
#             cond.notify()
 
# def run1():
 
#     #使用with来使用线程条件变量
#     with cond:
 
#         #遍历数据，步长为2 ，既:1,3,5,7,9
#         for i in range(1,10,2):
 
#             #打印线程名称和步数   1
#             print (threading.current_thread().name,i)
 
#             #释放信号（这里释放的是run打印完步数 0 的时候等待的信息）
#             cond.notify()
 
#             #等待信号（这里是run1执行完打印线程名称和步数后进入等待状态）
#             cond.wait()

# def run2():
 
#     #使用with来使用线程条件变量
#     with cond:
 
#         #遍历数据，步长为2 ，既:1,3,5,7,9
#         for i in ['a','b','c','d','e','f','g','h','i','j']:
 
#             #打印线程名称和步数   1
#             print (threading.current_thread().name,i)
 
#             #释放信号（这里释放的是run打印完步数 0 的时候等待的信息）
#             cond.notify()
 
#             #等待信号（这里是run1执行完打印线程名称和步数后进入等待状态）
#             cond.wait()
 
# if __name__=="__main__":
    # threading.Thread(target=run).start()
    # threading.Thread(target=run1).start()
    # threading.Thread(target=run2).start()



# import time
# from threading import Condition, RLock

# def test_v1():
#     t = Condition()
#     a = 1

#     def test_th(n):
#         print('thread start ', n)
#         with t:
#             print('thread start wait ', n)
#             if t.wait(5):
#                 print('thread ', n)
#             else:
#                 print("************************* ", n)
#                 print("sleep ", n)
#                 time.sleep(n)
#                 print("sleep over ", n)

#             if n == 2:
#                 t.notify()

#     threads = [threading.Thread(target=test_th, args=(i, )) for i in range(1, 10)]
#     [t.start() for t in threads]

#     with t:
#         print("start notify ")
#         t.notify(2)


# if __name__ == '__main__':
#     test_v1()


# import threading
# import time
 
# con = threading.Condition()
# products = 0
 
# class Product(threading.Thread):
#     def run(self):
#         global products
#         while True:
#             print(self.getName(),": 等待生产环境")
#             con.acquire()
#             print(self.getName(),": 准备生产")
#             if products >= 20:
#                 print(self.getName(),": 放不下了，我先休息一下")
#                 con.notifyAll()
#                 con.wait()
#             else:
#                 print(self.getName(),": 生产中")
#                 time.sleep(1)
#                 products += 1
#                 print(self.getName(),": 完成了一个")
#                 print(self.getName(),": 一共有:",products,"个")
#                 con.notifyAll()
#             con.release()
 
#     def mycall(cls):
#         try:
#             con.acquire()
#         except RuntimeError as e:
#             print(e)
#         con.notify()
#         con.release()
 
# class Consumer(threading.Thread):
#     def run(self):
#         global products
#         while True:
#             if products > 0:
#                 print(self.getName(),": 消耗产品")
#                 products -= 1
#             else:
#                 print(self.getName(),": 没了")
#                 print(self.getName(),": 等一会,打电话通知下")
#                 Product().mycall()
#                 # time.sleep(3)
 
# if __name__ == "__main__":
#     p1 = Product(name="thread_A")
#     p2 = Product(name="thread_B")
#     c1 = Consumer(name="thread_C")
#     p1.start()
#     p2.start()
#     c1.start()
#     p1.join()
#     p2.join()
#     c1.join()
#     print("main")


# import threading
# import time
# L=[]
# class boy(threading.Thread):
#     def __init__(self,cond,name = 'A boy'):
           
#         threading.Thread.__init__(self)
#         self.cond = cond
#         self.name = name
#     def run(self):
#         time.sleep(1)
#         '''boy start conversation, make sure
#            the girl thread stared before send notify'''
#         self.cond.acquire()
#         print (self.name + ':Hello pretty~,I miss you\n')
#         self.cond.notify()
#         self.cond.wait()
#         print (self.name + ':like moth missing fire\n')
#         self.cond.notify()
#         self.cond.wait()
#         print (self.name + ':and I bought a gift for you in the list L\n')
#         L.append('channel5')
#         self.cond.notify()
#         self.cond.release()
           
# class girl(threading.Thread):
#     def __init__(self,cond,name = 'A girl'):
           
#         threading.Thread.__init__(self)
#         self.cond = cond
#         self.name = name
#     def run(self):
#         self.cond.acquire()
#         self.cond.wait()
#         print (self.name + ':Really, show me how much~\n')
#         self.cond.notify()
#         self.cond.wait()
#         print (self.name +':you\'re so sweet~')
#         self.cond.notify()
#         self.cond.wait()
#         print (self.name +':wow~~, that\'s '+L.pop()+'---the one I dreamed for so long, I love you')
#         self.cond.release()
# if __name__ == '__main__':
#     cond = threading.Condition()
#     husband = boy(cond, 'Aidan')
#     wife = girl(cond,'PPP')
#     husband.start()
#     wife.start()
#     #husband.start()
#     husband.join() #wait untill these two threads end
#     wife.join()
#     print ('end converdarion\n')


'''----------------------------------------------- ThreadLocal -------------------------------------------------'''

# # ThreadLocal是一个全局的字典，用每个线程的名称做为key去存储和访问变量，这样每个线程之间的数据就变得独立，互相不受到干扰。
# # 1.示例
# import time
# import threading
 
# v = threading.local()
 
# def func(arg):
#     # 内部会为当前线程创建一个空间用于存储：phone=自己的值
#     v.phone = arg
#     time.sleep(2)
#     print(f'{threading.current_thread().name} = v.phone:{v.phone},arg:{arg}') # 去当前线程自己空间取值
 
# for i in range(10):
#     t =threading.Thread(target=func,args=(i,))
#     t.start()


# # 2.原理
# import time
# import threading
 
# DATA_DICT = {}
 
# def func(arg):
#     ident = threading.get_ident()
#     DATA_DICT[ident] = arg
#     time.sleep(1)
#     print(f'ident:{DATA_DICT[ident]},arg:{arg}')
 
# for i in range(10):
#     t =threading.Thread(target=func,args=(i,))
#     t.start()


# # 3.拓展
# import time
# import threading
 
# INFO = {}
# class Local(object):
#     def __getattr__(self, item):
#         ident = threading.get_ident()
#         return INFO[ident][item]
#     def __setattr__(self, key, value):
#         ident = threading.get_ident()
#         if ident in INFO:
#             INFO[ident][key] = value
#         else:
#             INFO[ident] = {key: value}
# # 实例化
# obj = Local()
# def func(arg):
#     # 调用对象的 __setattr__方法（“phone”,1）
#     obj.phone = arg
#     time.sleep(2)
#     print(obj.phone, arg)
# if __name__ == '__main__':
#     for i in range(10):
#         t = threading.Thread(target=func, args=(i,))
#         t.start()


'''-------------------------------------------------- Event ----------------------------------------------------'''

# 通过threading.Event()可以创建一个事件管理标志，该标志( event )默认为False，event对象主要有四种方法可以调用：
#     event.wait(timeout=None)：调用该方法的线程会被阻塞，如果设置了timeout参数，超时后，线程会停止阻塞继续执行；
#     event.set()：将event的标志设置为True，调用wait方法的所有线程将被唤醒；
#     event.clear()：将event的标志设置为False，调用wait方法的所有线程将被阻塞；
#     event.isSet()：判断event的标志是否为True。


# import threading
 
# def test1(n, event):
#     print ('Thread %s is ready' % n)
#     event.wait(timeout=1)
#     print ('Thread %s is running' % n)
 
# def main():
#     event = threading.Event()
#     for i in range(4):
#         th = threading.Thread(target=test1, args=(i, event))
#         th.start()
 
# if __name__ == '__main__':
#     main()


# import threading, time
 
# def test(n, event):
#     while not event.isSet():
#         print ('Thread %s is ready' % n)
#         time.sleep(1)
#     event.wait()
#     while event.isSet():
#         print ('Thread %s is running' % n)
#         time.sleep(1)
 
# def main():
#     event = threading.Event()
#     for i in range(4):
#         threading.Thread(target=test, args=(i, event)).start()
#     #     th = threading.Thread(target=test, args=(i, event))
#     #     th.start()
#     # print(f'thread::{th}')
#     # time.sleep(10)
#     print ('----- event is set -----')
#     event.set()
#     # time.sleep(3)
#     print ('----- event is clear -----')
#     event.clear()
#     print ('----- event is over -----')
 
# if __name__ == '__main__':
#     main()


'''-------------------------------------------------- Semaphore ----------------------------------------------------'''

# import time, threading
# semaphore = threading.Semaphore(3)
 
# def func():
#     if semaphore.acquire():
#         print (threading.currentThread().getName() + '获取锁')
#         time.sleep(5) 
#         semaphore.release()
#         print (threading.currentThread().getName() + '释放锁')
  
# for i in range(10):
#     print('================================================')
#     t1 = threading.Thread(target=func)
#     t1.start()
#     print('------------------------------------------------')



'''-----------------------------------------------------------------------------------------------------------------'''
'''------------------------------------------ ---- multiprocessing -------------------------------------------------'''
'''-----------------------------------------------------------------------------------------------------------------'''


'''----------------------------------------------------- Lock ------------------------------------------------------'''

# Process对象与Thread对象的用法相同，也有start(), run(), join()的方法。
# Process.PID中保存有PID，如果进程还没有start()，则PID为None
# # 所有的任务在打印的时候都会向同一个标准输出(stdout)输出。这样输出的字符会混合在一起，无法阅读。使用Lock同步，在一个任务输出完成之后，再允许另一个任务输出，可以避免多个任务同时向终端输出
# import os
# import threading
# import multiprocessing

# # Main
# print('Main:', os.getpid())

# # worker function
# def worker(sign, lock):
#     lock.acquire()
#     print(sign, os.getpid())
#     lock.release()

# # Multi-thread
# record = []
# lock = threading.Lock()

# # Multi-process
# record = []
# lock = multiprocessing.Lock()

# if __name__ == '__main__':
#     for i in range(5):
#         thread = threading.Thread(target=worker, args=('thread', lock))
#         thread.start()
#         record.append(thread)
#     for thread in record:
#         thread.join()
#     print('----------------')
#     for i in range(5):
#         process = multiprocessing.Process(target=worker, args=('process', lock))
#         process.start()
#         record.append(process)
#     for process in record:
#         process.join()


'''----------------------------------------------------- pipe ------------------------------------------------------'''

# # 由于进程之间不共享内存，所以进程之间的通信不能像线程之间直接引用，因而需要采取一些策略来完成进程之间的数据通信
# multiprocessing提供了threading包中没有的IPC(比如Pipe和Queue)，效率上更高。应优先考虑Pipe和Queue，避免使用Lock/Event/Semaphore/Condition等同步方式 (因为它们占据的不是用户进程的资源)。
# Pipe可以是单向(half-duplex)，也可以是双向(duplex)。我们通过mutiprocessing.Pipe(duplex=False)创建单向管道 (默认为双向)。一个进程从PIPE一端输入对象，然后被PIPE另一端的进程接收，单向管道只允许管道一端的进程输入，而双向管道则允许从两端输入。

# import multiprocessing as mtp

# def proc1(pipe):
#     pipe.send('hello')
#     print('proc1 rec:', pipe.recv())

# # def proc2(pipe):
# #     print('proc2 rec:', pipe.recv())
# #     pipe.send('hello, too')

# # Build a pipe
# pipe = mtp.Pipe()
# if __name__ == '__main__':
#     # Pass an end of the pipe to process 1
#     p1 = mtp.Process(target=proc1, args=(pipe[0],))
#     # Pass the other end of the pipe to process 2
#     # p2 = mtp.Process(target=proc2, args=(pipe[1],))
#     p1.start()
#     # p2.start()
#     p1.join()
#     # p2.join()

# 这里的Pipe是双向的。Pipe对象建立的时候，返回一个含有两个元素的表，每个元素代表Pipe的一端(Connection对象)。我们对Pipe的某一端调用send()方法来传送对象，在另一端使用recv()来接收。


'''----------------------------------------------------- Queue ------------------------------------------------------'''

# Queue与Pipe相类似，都是先进先出的结构。但Queue允许多个进程放入，多个进程从队列取出对象。Queue使用mutiprocessing.Queue(maxsize)创建，maxsize表示队列中可以存放对象的最大数量。

# import multiprocessing
# import os, time
# #==================
# # input worker
# def inputQ(queue):
#     info = str(os.getpid()) + '(put):' + str(time.time())
#     queue.put(info)

# # output worker
# def outputQ(queue,lock):
#     info = queue.get()
#     lock.acquire()
#     print (str(os.getpid()) + ' get: ' + info)
#     lock.release()
# #===================
# # Main
# record1 = []   # store input processes
# record2 = []   # store output processes
# lock  = multiprocessing.Lock()    # To prevent messy print
# queue = multiprocessing.Queue(3)

# if __name__ == '__main__':
#     # input processes
#     for i in range(10):
#         process = multiprocessing.Process(target=inputQ,args=(queue,))
#         process.start()
#         record1.append(process)
#     # output processes
#     for i in range(10):
#         process = multiprocessing.Process(target=outputQ,args=(queue,lock))
#         process.start()
#         record2.append(process)
#     for p in record1:
#         p.join()
#     queue.close()  # No more object will come, close the queue
#     for p in record2:
#         p.join()


'''---------------------------------------------------- manager -----------------------------------------------------'''

# 在多线程中，我们可以比较容易地共享资源，比如使用全局变量或者传递参数。在多进程情况下，由于每个进程有自己独立的内存空间，以上方法并不合适。此时我们可以通过共享内存和Manager的方法来共享资源。但这样做提高了程序的复杂度，并因为同步的需要而降低了程序的效率。

# from multiprocessing import Process, Manager
# from time import sleep
 
# def thread_a_main(sync_data_pool):  # A 进程主函数，存入100+的数
#     for ix in range(100, 105):
#         sleep(1)
#         sync_data_pool.append(ix)
 
# def thread_b_main(sync_data_pool):  # B 进程主函数，存入300+的数
#     for ix in range(300, 309):
#         sleep(0.6)
#         sync_data_pool.append(ix)
 
# def _test_case_000():  # 测试用例
#     manager = Manager()  # multiprocessing 中的 Manager 是一个工厂方法，直接获取一个 SyncManager 的实例
#     sync_data_pool = manager.list()  # 利用 SyncManager 的实例来创建同步数据池
#     Process(target=thread_a_main, args=(sync_data_pool, )).start()  # 创建并启动 A 进程
#     Process(target=thread_b_main, args=(sync_data_pool, )).start()  # 创建并启动 B 进程
#     for ix in range(6):  # C 进程（主进程）中实时的去查看数据池中的数据
#         sleep(1)
#         print(sync_data_pool)
 
# if '__main__' == __name__:  # 养成好习惯，将测试用例单独列出
#     _test_case_000()
 

# Manager是通过共享进程的方式共享数据。
# Manager管理的共享数据类型有：Value、Array、dict、list、Lock、Semaphore等等，同时Manager还可以共享类的实例对象。

# from multiprocessing import Process,Manager

# def func1(shareList,shareValue,shareDict,lock):
#     with lock:
#         shareValue.value+=1
#         shareDict[1]='1'
#         shareDict[2]='2'
#         for i in range(len(shareList)):
#             shareList[i]+=1

# if __name__ == '__main__':
#     manager=Manager()
#     list1=manager.list([1,2,3,4,5])
#     dict1=manager.dict()
#     array1=manager.Array('i',range(10))
#     value1=manager.Value('i',1)
#     lock=manager.Lock()
#     proc=[Process(target=func1,args=(list1,value1,dict1,lock)) for i in range(20)]
#     for p in proc:
#         p.start()
#     for p in proc:
#         p.join()
#     print (list1)
#     print (dict1)
#     print (array1)
#     print (value1)


'''---------------------------------------------------- array -----------------------------------------------------'''

# 共享内存：array
# 这里实际上只有主进程和Process对象代表的进程。我们在主进程的内存空间中创建共享的内存，也就是Value和Array两个对象。对象Value被设置成为双精度数(d), 并初始化为1.0。而Array则类似于C中的数组，有固定的类型(i, 也就是整数)。在Process进程中，我们修改了Value和Array对象。回到主程序，打印出结果，主程序也看到了两个对象的改变，说明资源确实在两个进程之间共享。

# import multiprocessing

# # Value/Array
# def func1(a, arr):
#     a.value = 3.14
#     for i in range(len(arr)):
#         arr[i] = 0
#     a.value = 6

# if __name__ == '__main__':
#     num = multiprocessing.Value('d', 1.0)  # num=0
#     arr = multiprocessing.Array('i', range(10))  # arr=range(10)
#     p = multiprocessing.Process(target=func1, args=(num, arr))
#     p.start()
#     p.join()
#     print (num.value)
#     print (arr[:])



'''---------------------------------------------------------------------------------------------------------------------'''
'''--------------------------------------------------------- pool ------------------------------------------------------'''
'''---------------------------------------------------------------------------------------------------------------------'''


'''---------------------------------------------------- multiprocessing ------------------------------------------------'''

''' # 进程池:
''' # 注  意： 在Windows上要想使用进程模块，就必须把有关进程的代码写在当前.py文件的if __name__ == ‘__main__’ :语句的下面，才能正常使用Windows下的进程模块。Unix/Linux下则不需要

# import multiprocessing as mp
# import  time ,os ,random

# def worker(msg):
#     t_start = time.time() #获取当前系统时间，长整型，常用来测试程序执行时间
#     print("%s 开始执行,进程名为%d" % (f'进程：{mp.current_process().name}',os.getpid()))
#     time.sleep(5)
#     t_stop = time.time()
#     print(f'参数：{msg}',"执行完毕，耗时%0.2f" % (t_stop-t_start))
#     return f'msg:{msg}'
# results = []
# if __name__ == '__main__':

#     p_start = time.time() #获取当前系统时间，长整型，常用来测试程序执行时间
#     pool= mp.Pool(3)# 定义一个进程池，最大进程数3，大小可以自己设置，也可写成processes=3
#     for i in range(0,10):
#         # Pool().apply_async(要调用的目标,(传递给目标的参数元祖,))
#         # 每次循环将会用空闲出来的子进程去调用目标
#         print('pool apply_async start')
#         result = pool.apply_async(worker,(i,))
#         print(f'{pool} apply_async taken')
#         print('foreach:',results.append(result))

#     print("----start----")
#     pool.close()  # 关闭进程池，关闭后po不再接收新的请求
#     print(f'{pool} has closed')
#     pool.join()  # 等待po中所有子进程执行完成，必须放在close语句之后
#     print(f'{pool} has joined')
#     print([res.get() for res in results])
#     print("-----end-----")
#     print(f'耗时:{p_start-time.time()}')

#-------------------------------------------------------------------

# import multiprocessing as mp
# import os, time
# def cube(x):
#     print(mp.current_process().name,f'参数:{x}')
#     print('等待开始......')
#     time.sleep(3)
#     print('等待结束。。。。。')
#     return x**3

# if __name__ == "__main__":
#     t1 = time.time()
#     pool    = mp.Pool(4)  # processes=4
#     results = [pool.apply_async(cube, args=(x,)) for x in range(1,10)]
#     print("----start----")
#     pool.close()  # 关闭进程池，关闭后po不再接收新的请求
#     print(f'{pool} has closed')
#     pool.join()  # 等待po中所有子进程执行完成，必须放在close语句之后
#     print(f'{pool} has joined')
#     print(res.get() for res in results)
#     print("-----end-----")
#     t2 = time.time()
#     print(f'用时：{t2-t1}')

#-------------------------------------------------------------------

# import os,time
# from multiprocessing import Pool

# def run(fn):
#     # fn: 函数参数是数据列表的一个元素
#     print(f'进程id: {os.getpid()}')
#     print('等待开始......')    
#     time.sleep(3)
#     print('等待结束。。。。。')
#     # print(fn * fn)
#     return f'返回值：{fn * fn * fn}'

# results = []
# if __name__ == "__main__":
#     testFL = [1, 2, 3, 4, 5, 6,7,8,9,10]
#     print('shunxu:')  # 顺序执行(也就是串行执行，单进程)
#     s = time.time()
#     for fn in testFL:
#         run(fn)
#     t0 = time.time()
#     print("顺序执行时间：", int(t0 - s))

#     print('concurrent:')  # 创建多个进程，并行执行
#     t1 = time.time()
#     pool = Pool(3)  # 创建拥有3个进程数量的进程池
#     # testFL:要处理的数据列表，run：处理testFL列表中数据的函数
#     for fn in testFL:
#         result = pool.apply_async(run, (fn,))
#         results.append(result)

#     print("----start----")
#     pool.close()  # 关闭进程池，不再接受新的进程
#     print(f'{pool} has closed')    
#     pool.join()  # 主进程阻塞等待子进程的退出
#     print(f'{pool} has joined')
#     print([res.get() for res in results])
#     print("-----end-----")
#     t2 = time.time()
#     print("并行执行时间：", int(t2 - t1))


## 线程池：
## 这里的多线程也是受到它受到全局解释器锁（GIL）的限制，并且一次只有一个线程可以执行附加到CPU的操作。

# from multiprocessing.dummy import Pool as mpdpool
# import os, time, random, threading

# def fun(msg):
#     print(f'参数: {msg}', threading.current_thread().name)
#     time.sleep(1)
#     print('********')
#     return 'fun_return %s' % msg
 
 
# # apply
# print('\n------apply-------')
# pool = mpdpool(processes=4)
# results =[]
# for i in range(5):
#     msg = 'msg: %d' % i
#     result = pool.apply(fun, (msg, ))
#     results.append(result)
 
# print('apply: 堵塞')
# print(results)


# # apply_async
# print('\n------apply_async-------')
# async_pool = mpdpool(processes=4)
# results =[]
# for i in range(10):
#     msg = 'msg: %d' % i
#     result = async_pool.apply_async(fun, (msg, ))
#     print(f'apply_async result: {result}')
#     results.append(result)

# print('apply_async: 不堵塞')
# async_pool.close()
# async_pool.join()
# for i in results:
#     i.wait()  # 等待线程函数执行完毕
# print('get the func return:') 
# for i in results:
#     if i.ready():  # 线程函数是否已经启动了
#         if i.successful():  # 线程函数是否执行成功
#             print(i.get())  # 线程函数返回值

 
# # map
# print('\n------map-------')
# arg = [3, 5, 11, 19, 12,7,6,8,12,21,34,13]
# t1 = time.time()
# pool = mpdpool(processes=3)
# return_list = pool.map(fun, arg)  ## 注意：虽然第二个参数是一个迭代器，但在实际使用中，必须在整个队列都就绪后，程序才会运行子进程
# print('map: 堵塞')
# pool.close()
# print(f'{pool} has closed')
# pool.join()
# print(f'{pool} has joined')
# print(return_list)
# print("-----end-----")
# t2 = time.time()
# print(f'用时：{t2-t1}')


# # map_async
# print('\n------map_async-------')
# arg = [3, 5, 11, 19, 12,7,6,8,12,21,34,13]
# t1 = time.time()
# async_pool = mpdpool(processes=4)
# result = async_pool.map_async(fun, arg)
# print(f'函数启动{result.ready()}')  # 线程函数是否已经启动了
# print('map_async: 不堵塞')
# result.wait()  # 等待所有线程函数执行完毕
# print('after wait')
# if result.ready():  # 线程函数是否已经启动了
#     if result.successful():  # 线程函数是否执行成功
#         print(result.get())  # 线程函数返回值
# print("-----end-----")
# t2 = time.time()
# print(f'用时：{t2-t1}')


#-------------------------------------------------------------------


# 进程池的使用有四种方式：apply_async、apply、map_async、map。其中apply_async和map_async是异步的，也就是启动进程函数以后会继续执行后续的代码不用等待进程函数返回。
# apply_async和map_async方式提供了一些获取进程函数状态的函数： ready()、 successful()、 get()。
# 线程池的使用方式和进程池相似。


# from multiprocessing.dummy import Pool as ThreadPool
# import time


# def fun(msg):
#     print('msg: ', msg)
#     time.sleep(1)
#     print('********')
#     return 'fun_return %s' % msg


# # map_async
# print('\n------map_async-------')
# arg = [1, 2, 10, 11, 18]
# async_pool = ThreadPool(processes=4)
# result = async_pool.map_async(fun, arg)
# print(result.ready())  # 线程函数是否已经启动了
# print('map_async: 不堵塞')
# result.wait()  # 等待全部线程函数执行完毕
# print('after wait')
# if result.ready():  # 线程函数是否已经启动了
#     if result.successful():  # 线程函数是否执行成功
#         print(result.get())  # 线程函数返回值

# # map
# print('\n------map-------')
# arg = [3, 5, 11, 19, 12]
# pool = ThreadPool(processes=3)
# return_list = pool.map(fun, arg)
# print('map: 堵塞')
# pool.close()
# pool.join()
# print(return_list)

# # apply_async
# print('\n------apply_async-------')
# async_pool = ThreadPool(processes=4)
# results =[]
# for i in range(5):
#     msg = 'msg: %d' % i
#     result = async_pool.apply_async(fun, (msg, ))
#     results.append(result)

# print('apply_async: 不堵塞')
# # async_pool.close()
# # async_pool.join()
# for i in results:
#     i.wait()  # 等待线程函数执行完毕

# for i in results:
#     if i.ready():  # 线程函数是否已经启动了
#         if i.successful():  # 线程函数是否执行成功
#             print(i.get())  # 线程函数返回值

# # apply
# print('\n------apply-------')
# pool = ThreadPool(processes=4)
# results =[]
# for i in range(5):
#     msg = 'msg: %d' % i
#     result = pool.apply(fun, (msg, ))
#     results.append(result)

# print('apply: 堵塞')
# print(results)

# ---------------------------------------------------------------------


# from multiprocessing.dummy import Pool
# import os, time, threading

# def func(msg):
#     print(f'传递:{msg}\n')
#     time.sleep(2)
#     print(f'{threading.current_thread()} is running\n')
#     return f'返回：{msg}'
    
# pool = Pool(processes=3)
# result = []
# for i in range(1, 5):
#     msg = 'hello %d' % (i)
#     res = pool.apply_async(func,(msg,))
#     result.append(res)

# print ('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')
# pool.close()
# print ('-------------------------------')
# pool.join()  # 调用join之前，先调用close函数，否则会出错。执行完close后不会有新的进程加入到pool,join函数等待所有子进程结束
# for res in result:
#     print (f"sub_process return:  {res.get()}\n")
# print ('sub-process done')


# def task(i):
#     print("{} begin".format(threading.current_thread().name))
#     print(i)#1-5(1-5输出的顺便随机) 6-10 11-15 16-20 (每次输出的随机) 每次5个线程进行处理 但是先后顺序没有关系，不影响输出的顺序
#     time.sleep(5)
#     return i
#     # print("{} end".format(threading.current_thread().name))

# if __name__ == "__main__":
#     a=range(1,21)
#     thread_count=5
#     P=Pool(thread_count)
#     #map  每次从iter a(1-20)调用5个元素分别给5个task 进行并行处理，所以会进行4(20/5)次 
#     #map  每次得到的结果都会等4次5个task都处理完，同时将最后的结果按照传入的顺序转为list输出
#     listres=P.map(task,a,chunksize=1)
#     print(listres)#得到结果list，最后一次输出
#     print('task has done')
#    # P.map(task,a,chunksize=2)
#    # 1-20 每次从iter a调用5个元素（隔chunksize个元素）分别给5个task 五个task 进行并行处理，
#    # 最后还是输出[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]


# def task(i):
#     print("{} begin".format(threading.current_thread().name))
#     print(i)#1-5(1-5输出的顺便随机) 6-10 11-15 16-20 (每次输出的随机) 每次5个线程进行处理 但是先后顺序没有关系，不影响输出的顺序
#     time.sleep(1)
#     return i
#     # print("{} end".format(threading.current_thread().name))

# if __name__ == "__main__":
#     a=range(1,21)
#     thread_count=5
#     P=Pool(thread_count)
#     # pool.imap_unordered,终端输出：可以不是按照a的顺序，每次处理完一个线程函数task，就会返回一个结果
#     for i,res in enumerate(P.imap(task,a,chunksize=2)):
#         print(f"res: {res}")

# imap作用同map 但imap是一个存放所有结果的迭代器 需要在主进程中主动使用next来驱动子进程的调用，
# 可以不用等5个线程都处理完才返回，这里每次处理完一个task函数将得到一个迭代器结果
# 输出：最终res还是按照1-20的顺序进行输出，chunksize=2时输出相同，此时res:2需要等第2次5个task处理时才会输出




'''-------------------------------------------- concurrent -----------------------------------------'''


# Python 中有多线程threading 和多进程multiprocessing 实现并发，
# 但是这两个东西开销很大，一是开启线程/进程的开销，二是主程序和子程序之间的通信需要 序列化和反序列化，
# 所以有些时候需要使用更加高级的用法，然而这些高级用法十分复杂，而且 threading 和 multiprocessing 用法还不一样。

# 于是诞生了 concurrent.future
# 1. 它可以解决大部分的复杂问题　　　　　　【但并不是全部，如果尝试后效果不好，还需要使用他们的高级用法】
# 2. 而且统一了线程和进程的用法
# concurrent.future 提供了 ThreadPoolExecutor 和 ProcessPoolExecutor 两个类，其实是对 线程池和进程池 的进一步抽象，而且具有以下特点：
# 3. 主程序可以获取子程序的状态和返回值
# 4. 子程序完成时，主程序能立刻知道


# 线程池：

 
# from concurrent.futures import ThreadPoolExecutor
# import time

# def spider(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# with ThreadPoolExecutor(max_workers=5) as t:  # 创建一个最大容纳量为5的线程池
#     task1 = t.submit(spider, 1)
#     task2 = t.submit(spider, 2)  # 通过submit提交执行的函数到线程池中
#     task3 = t.submit(spider, 3)  # 注意submit()不是阻塞的，而是立即返回的

#     # 通过done来判断线程是否完成
#     print(f"task1: {task1.done()}")  # 通过使用done()方法判断该任务是否结束，提交任务后立即判断任务状态，显示都是未完成，在显示2.5秒后
#     print(f"task2: {task2.done()}")
#     print(f"task3: {task3.done()}")

#     time.sleep(2.5)

#     print(f"task1: {task1.done()}")
#     print(f"task2: {task2.done()}")
#     print(f"task3: {task3.done()}")
#     # 通过result()来获取返回值
#     print(task1.result())


# ------------

# # wait(fs, timeout=None, return_when=All_COMPLETED)
# # fs 表示需要执行的序列
# # timeout 等待的最大时间，如果超过这个时间
# # return_when 表示wait返回结果的结果，默认为ALLP_COMPLETED全部执行完成后再返回结果

# from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED, ALL_COMPLETED

# def spider2(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# with ThreadPoolExecutor(max_workers=5) as t:
#     all_task = [t.submit(spider2, page) for page in range(1, 5)]
#     # 返回条件 FIRST_COMPLETED 当第一个任务完成的时候就停止等待，继续主线程任务，所以紧接着打印了“结束”
#     wait(all_task, return_when=FIRST_COMPLETED)
#     print("结束")
#     # 设置延时（等待）时间2.5秒
#     print(wait(all_task, timeout=2.5))
#     # 所以最后只有task4还在运行


# ------------

# as_completed
# 虽然使用return_when=FIRST_COMPLETED判断任务是否结束，但是不能在主线程中一直判断
# 最好的办法是当某个任务结束来，就给主线程返回结果，而不是一直判断每个任务是否结束
# as_completed就是当子线程中的任务执行完成后，直接用result()获取返回结果

# from concurrent.futures import ThreadPoolExecutor,as_completed
# import time

# def spider3(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# def main():
#     with ThreadPoolExecutor(max_workers=5) as t:
#         obj_list = []
#         for page in range(1, 5):
#             obj = t.submit(spider3, page)
#             obj_list.append(obj)
#         for future in as_completed(obj_list):
#             data = future.result()
#             print(f"main:{data}")

# main()  ## 使用过后发现，这个多线程比上面的测试耗时都多一些

# as_completed()方法是一个生成器，在没有任务完成的时候就会一直阻塞，除非设置了timeout
# 当某个任务完成的时候，会yield这个任务，就能执行for循环下面的语句，然后继续阻塞程序，直到所有任务结束
# 同时，先完成的任务会先返回给主线程

# ------------
# wait()与as_completed():
# wait方法可以让主线程阻塞，直到满足设定的要求。有三种条件ALL_COMPLETED, FIRST_COMPLETED，FIRST_EXCEPTION
# as_completed同样也与主进程阻塞有关(即需要所有future对象都运行结束才结束阻塞）

# from concurrent.futures import ThreadPoolExecutor,wait,as_completed,FIRST_COMPLETED,ALL_COMPLETED
# import time

# def test(g):
#     time.sleep(2)
#     print(f'{g}_crawling has done\n')
#     return f'result: {g}'

# if __name__ == '__main__':
#     record = []                                      # 创建列表放置后续产生的future对象
#     t = time.time()
#     with ThreadPoolExecutor(max_workers=10) as executor:
#     # 线程池的使用方式，由于是with方式，因此不需要自己关闭。同时线程池会自动join()
#     # 当然也可以使用executor = ThreadPoolExecutor(max_workers=4),
#     # 但需要在使用结束后使用shutdown方法，才能阻塞主线程。
#         for l in list(range(10)):                    # 创建一个参数列表用于后续传入函数
#             future = executor.submit(test,l)
#             # 传入要执行的函数及参数，返回的是一个future对象，注意这里不要调用，此处调用会使线程阻塞。
#             record.append(future)                    # 将对象添加到record列表中
#         wait(record, return_when=FIRST_COMPLETED)    # 第一次完成后即不再阻塞
#         # wait(record, return_when=ALL_COMPLETED)      # 全部完成后即不再阻塞
#         print('第一次运行结束')                       # 测试第一次完成后是否不再阻塞
#         for future in as_completed(record):          # 其实就相当于shutdown操作
#             print(future.result())                   # 并不按顺序提取，谁先完成就先打印，此处看不出来
#     print('程序结束')                                 # 测试主线程是否阻塞
#     print(time.time()-t)

# ------------

# map
# map(fn, *iterables, timeout=None)
# fn 需要线程执行的函数
# iterables 接收一个可迭代对象
# 和wait()的timeout一样，用于延时，但是map是返回线程执行的结果，如果timeout小于线程执行时间会抛出TimeoutError

# from concurrent.futures import ThreadPoolExecutor,as_completed
# import time

# def spider4(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# def main2():
#     executor = ThreadPoolExecutor(max_workers=4)
#     i = 1
#     # 列表中的每一个元素都执行来spider4()函数，并分配各线程池   task1:2
#     for result in executor.map(spider4, [2, 3, 1, 4]):
#         print(f"task{i}:{result}")
#         i += 1

# main2()

# 使用map方法，无需提前使用submit方法，与Python高阶函数map含义相同，都是将序列中的每个元素都执行同一个行数
# 与as_completed()方法的结果不同，输出顺序和列表的顺序相同，
# 就算1秒的任务执行完成，也会先打印前面提交的任务返回的结果

# map可以保证输出的顺序, submit输出的顺序是乱的
# 如果你要提交的任务的函数是一样的，就可以简化成map。但是假如提交的任务函数是不一样的，
# 或者执行的过程之可能出现异常（使用map执行过程中发现问题会直接抛出错误）就要用到submit（）
# submit和map的参数是不同的，submit每次都需要提交一个目标函数和对应的参数，
# map只需要提交一次目标函数，目标函数的参数放在一个迭代器（列表，字典）里就可以。




#-----------------------

# from concurrent import futures
# import time, threading


# def func(n):                                 # 首先定义一个供线程调用的函数。  
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '{}开始时间{}，传入值<{}>，结束时间{}'.format(threading.current_thread(),start, n, end)  ## threading.enumerate(), threading.current_thread(), threading.current_thread().name, threading.current_thread().getName()


# executor = futures.ThreadPoolExecutor(max_workers=4)
#     # max_workers 为线程池中，最大工作线程的个数。

# results = executor.map(func,  [2,3,8,5,4])
#     # 利用 executor.map() 调用函数，第一个参数为函数名，后面为函数参数。
#     # executor.map 返回的是一个生成器，results内的值是按函数的“调用顺序“来排列的。
#     # 完成一个线程，就把结果放入 results 中。
#     # 如果要取里面的值，可以用for, list, enumerate等方法。
#     # 因为<线程3>的执行时间是5秒，晚于线程<线程4><线程5>完成。
#     # 所以下面的打印结果会打印到<线程3>时主线程就阻塞了，等待<线程3>的完成。
#     # 它会等待<线程3>完成后，再继续打印后面的<线程4><线程5>结果。

# for serial, value in enumerate(results, start=1):
#     print('线程 {}：{}'.format( serial, value))

# executor.shutdown()      # 关闭线程池。



# from concurrent import futures
# import time


# def func(n):
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '{}开始时间{}，传入值<{}>，结束时间{}'.format(threading.current_thread(),start, n, end)  ## threading.enumerate(), threading.current_thread(), threading.current_thread().name, threading.current_thread().getName()

# # 如果利用 with 上下文管理器，它会自动关闭线程池。
# with futures.ThreadPoolExecutor(max_workers=4) as executor:
#     # map会阻塞主线程，等待所有子线程结束，返回 result，如果子线程有报错，也会返回报错信息。
#     # 所以这是一个有了所有线程结果的results，下面的打印就不会有阻塞了。
#     results = executor.map(func,  [2,3,8,5,4])
        
# for serial, value in enumerate(results, start=1):
#     print('线程 {}：{}'.format( serial, value))


# from concurrent.futures import ThreadPoolExecutor
# import time

# def f_submit(x):
#     print(f'{time.strftime("%H:%M:%S")}submit：{x}\n')
#     time.sleep(1)
#     print(f'{time.strftime("%H:%M:%S")}{x}计算结果：{x**2}\n')
#     return x*x


# """ 用with开启线程池时，并自动关闭线程池。"""
# with ThreadPoolExecutor(max_workers=5) as e:
#     f1 = e.submit(f_submit, 1,)
#     f2 = e.submit(f_submit, 2,)
#     f3 = e.submit(f_submit, 3,)
#     f4 = e.submit(f_submit, 4,)
#     f5 = e.submit(f_submit, 5,)

#     """ result()获取子线程结果时，会阻塞主线程。
#         如果子线程有异常时，也会引发这个异常。
#         即，如果不用result()获取结果，那么即使子线程有异常，
#         该异常也不会被触发，只是那个有异常的子线程会停止运行。"""
#     print([f1.result(), f2.result(), f3.result(), f4.result(), f5.result()])
#     print('主线程\n')
#     #time.sleep(2)

# print('进程池外')    

# result()获取子线程结果时，会阻塞主线程。
# 如果子线程有异常时，也会引发这个异常（报错）。
# 即，如果不用result()获取结果，那么即使子线程有异常（报错），
# 该异常也不会被触发，只是那个有异常的子线程会停止运行。


# from concurrent import futures
# import time


# def func(n):
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '{}开始时间{}，传入值<{}>，结束时间{}'.format(threading.current_thread(),start, n, end)  ## threading.enumerate(), threading.current_thread(), threading.current_thread().name, threading.current_thread().getName()

# print('#============单个线程============')
# executor = futures.ThreadPoolExecutor(4)   # 最大线程数量为4个。
# sub = executor.submit(func, 3)    # 利用submit()把调用函数提交到线程池，返回一个Future实例。
# # print(sub)
# # print(type(sub))
# res = sub.result()    # 利用result()获取调用函数的运行结果。
# print(res)
# print()


# print('#============多个线程============')
# with  futures.ThreadPoolExecutor(4) as executor:
#     to_do = [ ]
#     for i in [2, 3, 8, 5, 4]:
#         sub = executor.submit(func, i)
#         to_do.append(sub)           # 存储各个Future到列表中，给后面的 as_completed()使用。

#     for i in futures.as_completed(to_do):    # 如果只是让线程执行，而不需要获取返回值，可以不用这段代码。
#             # 用 as_completed() 返回的顺序是按线程执行所用时间来排的，越早完成，越早取出来。
#             # 如果直接用 for i in to_do: 来读取结果，那么结果是按Future放入to_do中的顺序来取的。
#         res = i.result()
#         print(res)


# from concurrent import futures
# import time, os


# def func(n):
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '开始时间{}，传入值<{}>，结束时间{}'.format(start, n, end)


# if __name__ == '__main__':

#     print('#============单个进程============')
#     executor = futures.ProcessPoolExecutor(os.cpu_count())   # 默认值为CPU的核心数量os.cpu_count()。
#     sub = executor.submit(func, 3)    # 利用submit()把调用函数提交到进程池，返回一个Future实例。
#     # print(sub)
#     # print(type(sub))
#     res = sub.result()    # 利用result()获取调用函数的运行结果。
#     print(res)
#     print()


#     print('#============多个进程============')
#     with  futures.ProcessPoolExecutor(4) as executor:
#         to_do = [ ]
#         for i in [1, 2, 8, 4, 3]:
#             sub = executor.submit(func, i)
#             to_do.append(sub)    # 存储各个Future到列表中，给后面的 as_completed()使用。

#         for i in futures.as_completed(to_do):
#         # for i in to_do:
#         # 用 as_completed() 返回的顺序是按进程执行所用时间来排的，越早完成，越早取出来。
#         # 如果直接用 for i in to_do: 来读取结果，那么结果是按Future放入to_do中的顺序来取的。
#             res = i.result()
#             print(res)


#------------------------

# from concurrent.futures import ThreadPoolExecutor
# import urllib.request
# URLS = ['http://www.163.com', 'https://www.baidu.com/', 'https://github.com/']
# def load_url(url):
#     with urllib.request.urlopen(url, timeout=60) as conn:
#         print('%r page is %d bytes' % (url, len(conn.read())))

# executor = ThreadPoolExecutor(max_workers=3)

# for url in URLS:
#     future = executor.submit(load_url,url)
#     print(future.done())

# print('主线程')


# from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor #线程池，进程池
# import threading,time
 
# def test(arg):
#     print(arg,threading.current_thread().name)
#     time.sleep(1)
 
# if __name__ == "__main__":
#     thread_pool = ThreadPoolExecutor(5) #定义5个线程执行此任务
#     process_pool = ProcessPoolExecutor(5) #定义5个进程
#     for i in range(20):
#         thread_pool.submit(test,i)


# import time
# from concurrent.futures import ThreadPoolExecutor
# def func(name):
#     print(f"{name}开始")
#     time.sleep(2)
#     print(f"{name}结束")
# if __name__ == '__main__':
#     p = ThreadPoolExecutor(max_workers=3)  # 创建一个线程池，里面最多有3个线程同时工作
#     for i in range(1, 10):
#         p.submit(func, f"线程{i}")
#     p.shutdown()  # 主线程等待子线程结束  
#     print("主线程结束")


# import os, time, random, threading
# from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# def f(n):
#     time.sleep(random.randint(1, 3))
#     # print(n)
#     # print("进程(%s) %s的平方: %s" % (os.getpid(), n, n*n))
#     print("线程(%s) %s的平方: %s\n" % (threading.current_thread().getName(), n, n * n))
#     return n * n
# if __name__ == '__main__':
#     pool = ThreadPoolExecutor(max_workers=5)
#     # pool = ProcessPoolExecutor(max_workers=5)
#     ret_list = []
#     for i in range(10):
#         ret = pool.submit(f, i)  # 异步提交任务,f函数名称或者方法名称,i给f函数的参数
#         # print(ret.result())  #join
#         ret_list.append(ret)
#     # pool.shutdown()  #锁定线程池,不让新任务再提交进来了.轻易不用
#     for i in ret_list:
#         print(i.result())




# from  concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor
# from threading import currentThread
# import os,time,random
# def task(n):
#     print('%s:%s is running'%(currentThread().getName(),os.getpid()))  #看到的pid都是一样的，因为线程是共享了一个进程
#     time.sleep(random.randint(1,3))  #I/O密集型的，，一般用线程，用了进程耗时长
#     return n**2
# if __name__ == '__main__':
#     start = time.time()
#     p = ThreadPoolExecutor() #线程池 #如果不给定值，默认cup*5
#     l = []
#     for i in range(10):  #10个任务 # 线程池效率高了
#         obj  = p.submit(task,i)  #相当于apply_async异步方法
#         l.append(obj)
#     # p.shutdown()  #默认有个参数wait=True (相当于close和join)
#     print('='*30)
#     print([obj.result() for obj in l])
#     print(time.time() - start)  #3.001171827316284



# from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor
# import os, time, requests

# def get_page(url):
#     print('<%s> is getting [%s]'%(os.getpid(),url))
#     response = requests.get(url)
#     if response.status_code==200:  #200代表状态：下载成功了
#         return {'url':url,'text':response.text}
# def parse_page(res):
#     res = res.result()
#     print('<%s> is getting [%s]'%(os.getpid(),res['url']))
#     with open('db.txt','a') as f:
#         parse_res = 'url:%s size:%s\n'%(res['url'],len(res['text']))
#         f.write(parse_res)
# if __name__ == '__main__':
#     # p = ThreadPoolExecutor()
#     p = ProcessPoolExecutor()
#     l = [
#         'http://www.baidu.com',
#         'http://www.baidu.com',
#         'http://www.baidu.com',
#         'http://www.baidu.com',
#     ]
#     for url in l:
#         res = p.submit(get_page,url).add_done_callback(parse_page) #这里的回调函数拿到的是一个对象。得
#         #  先把返回的res得到一个结果。即在前面加上一个res.result() #谁好了谁去掉回调函数
#         # 回调函数也是一种编程思想。不仅开线程池用，开线程池也用
#     # p.shutdown()  #相当于进程池里的close和join
#     print('主',os.getpid())



# import time
# from concurrent import futures
# from concurrent.futures import ThreadPoolExecutor


# def display(args):
#     print(time.strftime('[%H:%M:%S]', time.localtime()), end=' ')
#     print(args)


# def task(n):
#     """只是休眠"""
#     display('begin sleep {}s.'.format(n))
#     time.sleep(n)
#     display('ended sleep {}s.'.format(n))


# def do_many_task_inorder():
#     """多线程
#     按任务发布顺序依次等待完成
#     """
#     tasks = [5, 4, 3, 2, 1]
#     with ThreadPoolExecutor(max_workers=3) as executor:
#         future_list = [executor.submit(task, arg) for arg in tasks]

#         display('非阻塞运行')

#         for future in future_list:
#             display(future)

#         display('统一结束(有序)')

#         for future in future_list:
#             display(future.result())


# def do_many_task_disorder():
#     """多线程执行
#     先完成先显示
#     """
#     tasks = [5, 4, 3, 2, 1]
#     with ThreadPoolExecutor(max_workers=3) as executor:
#         future_list = [executor.submit(task, arg) for arg in tasks]

#         display('非阻塞运行')

#         for future in future_list:
#             display(future)

#         display('统一结束(无序)')

#         done_iter = futures.as_completed(future_list)  # generator

#         for done in done_iter:
#             display(done)


# if __name__ == '__main__':
    # do_many_task_inorder()
    # do_many_task_disorder()


# concurrent.futures.ThreadPoolExecutor，在提交任务的时候，有两种方式，一种是submit（）函数，另外一种是map（）函数，二者的主要区别在于：
# 2.一、map能够保证输出的顺序, submit输出的顺序是乱的
# 2.二、若是你要提交的任务的函数是同样的，就能够简化成map。可是假如提交的任务函数是不同的，或者执行的过程之可能出现异常（使用map执行过程当中发现问题会直接抛出错误）就要用到submit（）
# 2.三、submit和map的参数是不一样的，submit每次都须要提交一个目标函数和对应的参数，map只须要提交一次目标函数，目标函数的参数放在一个迭代器（列表，字典）里就能够。



#-------------------------------------- future故障 --------------------------------------
# ThreadPoolExecutor 线程池执行器是 Executor 执行器的子类，通过线程池来执行异步调用。它管理一组工作线程，当工作线程有富余的时候，给它们传递任务。
# 当属于一个 Future 对象的可调用对象等待另一个 Future 的返回时，会发生死锁 deadlock。

# from concurrent.futures import ThreadPoolExecutor
# import time

# def wait_on_b():
#     time.sleep(5)
#     print(b.result())  # b will never complete because it is waiting on a.
#     return 5

# def wait_on_a():
#     time.sleep(5)
#     print(a.result())  # a will never complete because it is waiting on b.
#     return 6


# executor = ThreadPoolExecutor(max_workers=2)
# a = executor.submit(wait_on_b)
# # b = executor.submit(wait_on_a)
# print(a)

# # 再举一例：
# def wait_on_future():
#     f = executor.submit(pow, 5, 2)
#     # This will never complete because there is only one worker thread and
#     # it is executing this function.
#     print(f.result())

# executor = ThreadPoolExecutor(max_workers=1)
# executor.submit(wait_on_future)




# from concurrent.futures import ThreadPoolExecutor as Pool
# from concurrent.futures import as_completed
# from concurrent.futures import wait
# import requests
 
# URLS = ['http://qq.com', 'http://sina.com', 'http://www.baidu.com', ]
 
 
# def task(url, timeout=10):
#     return requests.get(url, timeout=timeout)
 
 
# with Pool(max_workers=3) as executor:
#     future_tasks = [executor.submit(task, url) for url in URLS]
 
#     for f in future_tasks:
#         if f.running():
#             print('%s is running' % str(f))
 
#     for f in as_completed(future_tasks):
#         try:
#             ret = f.done()
#             if ret:
#                 f_ret = f.result()
#                 print('%s, done, result: %s, %s' % (str(f), f_ret.url, len(f_ret.content)))
#         except Exception as e:
#             f.cancel()
#             print(str(e))

#     results = wait(future_tasks)
#     done = results[0]
#     for x in done:
#         print(x)


'''-------------------------------------------------- asyncio -----------------------------------------------------'''

# 一、asyncio

# import asyncio,time

# # 定义异步函数
# async def hello():
#     await asyncio.sleep(1)
#     print('Hello World:%s' % time.time())

# if __name__ =='__main__':
#     tasks = [hello() for i in range(5)]
#     loop = asyncio.get_event_loop()
#     loop.run_until_complete(asyncio.wait(tasks))

# async def 用来定义异步函数，await 表示当前协程任务等待睡眠时间，允许其他任务运行。
# 然后获得一个事件循环，主线程调用asyncio.get_event_loop()时会创建事件循环，你需要把异步的任务丢给这个循环的run_until_complete()方法，事件循环会安排协同程序的执行。


# 二、aiohttp

# import asyncio
# from aiohttp import ClientSession

# tasks = []
# url = "https://www.baidu.com/{}"
# async def hello(url):
#     async with ClientSession() as session:
#         async with session.get(url) as response:
#             response = await response.read()
#             print(response)

# if __name__ == '__main__':
#     loop = asyncio.get_event_loop()
#     loop.run_until_complete(hello(url))

# 多链接异步访问:
# 如果我们需要请求多个URL该怎么办呢，同步的做法访问多个URL只需要加个for循环就可以了。
# 但异步的实现方式并没那么容易，在之前的基础上需要将hello()包装在asyncio的Future对象中，然后将Future对象列表作为任务传递给事件循环。

# import time
# import asyncio
# from aiohttp import ClientSession

# tasks = []
# url = "https://www.baidu.com/{}"
# async def hello(url):
#     async with ClientSession() as session:
#         async with session.get(url) as response:
#             response = await response.read()
# #            print(response)
#             print('Hello World:%s' % time.time())

# def run():
#     for i in range(5):
#         task = asyncio.ensure_future(hello(url.format(i)))
#         tasks.append(task)

# if __name__ == '__main__':
#     loop = asyncio.get_event_loop()
#     run()
#     loop.run_until_complete(asyncio.wait(tasks))


# 收集http响应

# import time
# import asyncio
# from aiohttp import ClientSession

# tasks = []
# url = "https://www.baidu.com/{}"
# async def hello(url):
#     async with ClientSession() as session:
#         async with session.get(url) as response:
# #            print(response)
#             print('Hello World:%s' % time.time())
#             return await response.read()

# def run():
#     for i in range(5):
#         task = asyncio.ensure_future(hello(url.format(i)))
#         tasks.append(task)
#     result = loop.run_until_complete(asyncio.gather(*tasks))
#     print(result)

# if __name__ == '__main__':
#     loop = asyncio.get_event_loop()
#     run()


# 异常解决:
# 假如你的并发达到2000个，程序会报错：ValueError: too many file descriptors in select()。
# 报错的原因字面上看是 Python 调取的 select 对打开的文件有最大数量的限制，这个其实是操作系统的限制，linux打开文件的最大数默认是1024，windows默认是509，超过了这个值，程序就开始报错。
# 这里我们有三种方法解决这个问题：
# 1.限制并发数量。（一次不要塞那么多任务，或者限制最大并发数量）
# 2.使用回调的方式。
# 3.修改操作系统打开文件数的最大限制，在系统里有个配置文件可以修改默认值，具体步骤不再说明了。
# 不修改系统默认配置的话，个人推荐限制并发数的方法，设置并发数为500，处理速度更快。


# import time,asyncio,aiohttp


# url = 'https://www.baidu.com/'
# async def hello(url,semaphore):
#     async with semaphore:
#         async with aiohttp.ClientSession() as session:
#             async with session.get(url) as response:
#                 return await response.read()


# async def run():
#     semaphore = asyncio.Semaphore(500) # 限制并发量为500
#     to_get = [hello(url.format(),semaphore) for _ in range(1000)] #总共1000任务
#     await asyncio.wait(to_get)


# if __name__ == '__main__':
# #    now=lambda :time.time()
#     loop = asyncio.get_event_loop()
#     loop.run_until_complete(run())
#     loop.close()


#-------------------------------------------------------------------


# import asyncio
# import requests
# import os


# async def asyncget(url,headers):
#     return requests.get(url,headers=headers,verify=False)

# async def download(url,headers,fileDir,filename=None):

#     if filename == None:
#         filename = url[url.rfind("/")+1:]
#     if fileDir == None:
#         fileDir = "./files"

#     if not os.path.exists(fileDir):
#         print("亲，输入的目录不存在哦，下面将自动创建 "+ fileDir)
#         os.makedirs(fileDir)
    
#     filepath = os.path.join(fileDir,filename)

#     if os.path.exists(filepath):
#         print("亲，文件",filepath,"已经存在哦")
#         return
    
#     response =  await asyncget(url,headers=headers)

#     with open(filepath,"wb") as fp:
#         fp.write(response.content)
#     print("亲，文件",filepath,"写入完成哦")


# if __name__ == "__main__":
#     headers ={
#         "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36"
#     }
#     urls = [
#         "http://wx1.sinaimg.cn/mw600/00893JKXly1gikm1th96cj30u015b1a9.jpg",
#         "http://wx1.sinaimg.cn/mw600/00893JKXly1gikm1lkcxdj30sg18g0zh.jpg",
#         "http://wx2.sinaimg.cn/mw600/005QrDFkly1gfl4phpqvaj31w91w9x20.jpg"
#     ]

#     asyncio.run(
#         asyncio.wait(
#             [download(url=url,headers=headers,fileDir="imgs") for url in urls]
#         )
#     )



# import requests

# session = requests.Session()
# session.mount("https://", requests.adapters.HTTPAdapter(pool_maxsize=50))
# session.mount("http://", requests.adapters.HTTPAdapter(pool_maxsize=50))

# import asyncio

# async def perform_async_calls(self, session, urls):
#     loop = asyncio.get_event_loop()
#     futures = []
#     for url in urls:
#         futures.append(loop.run_in_executor(None, session.get, url)

#     results = []
#     for future in futures:
#         result = await future
#         results.append(result.json())

#     return results







