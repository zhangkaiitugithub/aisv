



'''-------------------------------------------- concurrent -----------------------------------------'''


# Python 中有多线程threading 和多进程multiprocessing 实现并发，
# 但是这两个东西开销很大，一是开启线程/进程的开销，二是主程序和子程序之间的通信需要 序列化和反序列化，
# 所以有些时候需要使用更加高级的用法，然而这些高级用法十分复杂，而且 threading 和 multiprocessing 用法还不一样。

# 于是诞生了 concurrent.future
# 1. 它可以解决大部分的复杂问题　　　　　　【但并不是全部，如果尝试后效果不好，还需要使用他们的高级用法】
# 2. 而且统一了线程和进程的用法
# concurrent.future 提供了 ThreadPoolExecutor 和 ProcessPoolExecutor 两个类，其实是对 线程池和进程池 的进一步抽象，而且具有以下特点：
# 3. 主程序可以获取子程序的状态和返回值
# 4. 子程序完成时，主程序能立刻知道


# 线程池：

 
# from concurrent.futures import ThreadPoolExecutor
# import time

# def spider(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# with ThreadPoolExecutor(max_workers=5) as t:  # 创建一个最大容纳量为5的线程池
#     task1 = t.submit(spider, 1)
#     task2 = t.submit(spider, 2)  # 通过submit提交执行的函数到线程池中
#     task3 = t.submit(spider, 3)  # 注意submit()不是阻塞的，而是立即返回的

#     # 通过done来判断线程是否完成
#     print(f"task1: {task1.done()}")  # 通过使用done()方法判断该任务是否结束，提交任务后立即判断任务状态，显示都是未完成，在显示2.5秒后
#     print(f"task2: {task2.done()}")
#     print(f"task3: {task3.done()}")

#     time.sleep(2.5)

#     print(f"task1: {task1.done()}")
#     print(f"task2: {task2.done()}")
#     print(f"task3: {task3.done()}")
#     # 通过result()来获取返回值
#     print(task1.result())


#-------------------------------------------------------------------

# # wait(fs, timeout=None, return_when=All_COMPLETED)
# # fs 表示需要执行的序列
# # timeout 等待的最大时间，如果超过这个时间
# # return_when 表示wait返回结果的结果，默认为ALLP_COMPLETED全部执行完成后再返回结果

# from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED, ALL_COMPLETED

# def spider2(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# with ThreadPoolExecutor(max_workers=5) as t:
#     all_task = [t.submit(spider2, page) for page in range(1, 5)]
#     # 返回条件 FIRST_COMPLETED 当第一个任务完成的时候就停止等待，继续主线程任务，所以紧接着打印了“结束”
#     wait(all_task, return_when=FIRST_COMPLETED)
#     print("结束")
#     # 设置延时（等待）时间2.5秒
#     print(wait(all_task, timeout=2.5))
#     # 所以最后只有task4还在运行


#-------------------------------------------------------------------

# as_completed
# 虽然使用return_when=FIRST_COMPLETED判断任务是否结束，但是不能在主线程中一直判断
# 最好的办法是当某个任务结束来，就给主线程返回结果，而不是一直判断每个任务是否结束
# as_completed就是当子线程中的任务执行完成后，直接用result()获取返回结果

# from concurrent.futures import ThreadPoolExecutor,as_completed
# import time

# def spider3(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# def main():
#     with ThreadPoolExecutor(max_workers=5) as t:
#         obj_list = []
#         for page in range(1, 5):
#             obj = t.submit(spider3, page)
#             obj_list.append(obj)
#         for future in as_completed(obj_list):
#             data = future.result()
#             print(f"main:{data}")

# main()  ## 使用过后发现，这个多线程比上面的测试耗时都多一些

# as_completed()方法是一个生成器，在没有任务完成的时候就会一直阻塞，除非设置了timeout
# 当某个任务完成的时候，会yield这个任务，就能执行for循环下面的语句，然后继续阻塞程序，直到所有任务结束
# 同时，先完成的任务会先返回给主线程

# ------------
# wait()与as_completed():
# wait方法可以让主线程阻塞，直到满足设定的要求。有三种条件ALL_COMPLETED, FIRST_COMPLETED，FIRST_EXCEPTION
# as_completed同样也与主进程阻塞有关(即需要所有future对象都运行结束才结束阻塞）

# from concurrent.futures import ThreadPoolExecutor,wait,as_completed,FIRST_COMPLETED,ALL_COMPLETED
# import time

# def test(g):
#     time.sleep(2)
#     print(f'{g}_crawling has done\n')
#     return f'result: {g}'

# if __name__ == '__main__':
#     record = []                                      # 创建列表放置后续产生的future对象
#     t = time.time()
#     with ThreadPoolExecutor(max_workers=10) as executor:
#     # 线程池的使用方式，由于是with方式，因此不需要自己关闭。同时线程池会自动join()
#     # 当然也可以使用executor = ThreadPoolExecutor(max_workers=4),
#     # 但需要在使用结束后使用shutdown方法，才能阻塞主线程。
#         for l in list(range(10)):                    # 创建一个参数列表用于后续传入函数
#             future = executor.submit(test,l)
#             # 传入要执行的函数及参数，返回的是一个future对象，注意这里不要调用，此处调用会使线程阻塞。
#             record.append(future)                    # 将对象添加到record列表中
#         wait(record, return_when=FIRST_COMPLETED)    # 第一次完成后即不再阻塞
#         # wait(record, return_when=ALL_COMPLETED)      # 全部完成后即不再阻塞
#         print('第一次运行结束')                       # 测试第一次完成后是否不再阻塞
#         for future in as_completed(record):          # 其实就相当于shutdown操作
#             print(future.result())                   # 并不按顺序提取，谁先完成就先打印，此处看不出来
#     print('程序结束')                                 # 测试主线程是否阻塞
#     print(time.time()-t)

#-------------------------------------------------------------------

# map
# map(fn, *iterables, timeout=None)
# fn 需要线程执行的函数
# iterables 接收一个可迭代对象
# 和wait()的timeout一样，用于延时，但是map是返回线程执行的结果，如果timeout小于线程执行时间会抛出TimeoutError

# from concurrent.futures import ThreadPoolExecutor,as_completed
# import time

# def spider4(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# def main2():
#     executor = ThreadPoolExecutor(max_workers=4)
#     i = 1
#     # 列表中的每一个元素都执行来spider4()函数，并分配各线程池   task1:2
#     for result in executor.map(spider4, [2, 3, 1, 4]):
#         print(f"task{i}:{result}")
#         i += 1

# main2()

# 使用map方法，无需提前使用submit方法，与Python高阶函数map含义相同，都是将序列中的每个元素都执行同一个行数
# 与as_completed()方法的结果不同，输出顺序和列表的顺序相同，
# 就算1秒的任务执行完成，也会先打印前面提交的任务返回的结果

# map可以保证输出的顺序, submit输出的顺序是乱的
# 如果你要提交的任务的函数是一样的，就可以简化成map。但是假如提交的任务函数是不一样的，
# 或者执行的过程之可能出现异常（使用map执行过程中发现问题会直接抛出错误）就要用到submit（）
# submit和map的参数是不同的，submit每次都需要提交一个目标函数和对应的参数，
# map只需要提交一次目标函数，目标函数的参数放在一个迭代器（列表，字典）里就可以。




#-------------------------------------------------------------------

# from concurrent import futures
# import time, threading


# def func(n):                                 # 首先定义一个供线程调用的函数。  
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '{}开始时间{}，传入值<{}>，结束时间{}'.format(threading.current_thread(),start, n, end)  ## threading.enumerate(), threading.current_thread(), threading.current_thread().name, threading.current_thread().getName()


# executor = futures.ThreadPoolExecutor(max_workers=4)
#     # max_workers 为线程池中，最大工作线程的个数。

# results = executor.map(func,  [2,3,8,5,4])
#     # 利用 executor.map() 调用函数，第一个参数为函数名，后面为函数参数。
#     # executor.map 返回的是一个生成器，results内的值是按函数的“调用顺序“来排列的。
#     # 完成一个线程，就把结果放入 results 中。
#     # 如果要取里面的值，可以用for, list, enumerate等方法。
#     # 因为<线程3>的执行时间是5秒，晚于线程<线程4><线程5>完成。
#     # 所以下面的打印结果会打印到<线程3>时主线程就阻塞了，等待<线程3>的完成。
#     # 它会等待<线程3>完成后，再继续打印后面的<线程4><线程5>结果。

# for serial, value in enumerate(results, start=1):
#     print('线程 {}：{}'.format( serial, value))

# executor.shutdown()      # 关闭线程池。



# from concurrent import futures
# import time


# def func(n):
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '{}开始时间{}，传入值<{}>，结束时间{}'.format(threading.current_thread(),start, n, end)  ## threading.enumerate(), threading.current_thread(), threading.current_thread().name, threading.current_thread().getName()

# # 如果利用 with 上下文管理器，它会自动关闭线程池。
# with futures.ThreadPoolExecutor(max_workers=4) as executor:
#     # map会阻塞主线程，等待所有子线程结束，返回 result，如果子线程有报错，也会返回报错信息。
#     # 所以这是一个有了所有线程结果的results，下面的打印就不会有阻塞了。
#     results = executor.map(func,  [2,3,8,5,4])
        
# for serial, value in enumerate(results, start=1):
#     print('线程 {}：{}'.format( serial, value))


# from concurrent.futures import ThreadPoolExecutor
# import time

# def f_submit(x):
#     print(f'{time.strftime("%H:%M:%S")}submit：{x}\n')
#     time.sleep(1)
#     print(f'{time.strftime("%H:%M:%S")}{x}计算结果：{x**2}\n')
#     return x*x


# """ 用with开启线程池时，并自动关闭线程池。"""
# with ThreadPoolExecutor(max_workers=5) as e:
#     f1 = e.submit(f_submit, 1,)
#     f2 = e.submit(f_submit, 2,)
#     f3 = e.submit(f_submit, 3,)
#     f4 = e.submit(f_submit, 4,)
#     f5 = e.submit(f_submit, 5,)

#     """ result()获取子线程结果时，会阻塞主线程。
#         如果子线程有异常时，也会引发这个异常。
#         即，如果不用result()获取结果，那么即使子线程有异常，
#         该异常也不会被触发，只是那个有异常的子线程会停止运行。"""
#     print([f1.result(), f2.result(), f3.result(), f4.result(), f5.result()])
#     print('主线程\n')
#     #time.sleep(2)

# print('进程池外')    

# result()获取子线程结果时，会阻塞主线程。
# 如果子线程有异常时，也会引发这个异常（报错）。
# 即，如果不用result()获取结果，那么即使子线程有异常（报错），
# 该异常也不会被触发，只是那个有异常的子线程会停止运行。


# from concurrent import futures
# import time


# def func(n):
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '{}开始时间{}，传入值<{}>，结束时间{}'.format(threading.current_thread(),start, n, end)  ## threading.enumerate(), threading.current_thread(), threading.current_thread().name, threading.current_thread().getName()

# print('#============单个线程============')
# executor = futures.ThreadPoolExecutor(4)   # 最大线程数量为4个。
# sub = executor.submit(func, 3)    # 利用submit()把调用函数提交到线程池，返回一个Future实例。
# # print(sub)
# # print(type(sub))
# res = sub.result()    # 利用result()获取调用函数的运行结果。
# print(res)
# print()


# print('#============多个线程============')
# with  futures.ThreadPoolExecutor(4) as executor:
#     to_do = [ ]
#     for i in [2, 3, 8, 5, 4]:
#         sub = executor.submit(func, i)
#         to_do.append(sub)           # 存储各个Future到列表中，给后面的 as_completed()使用。

#     for i in futures.as_completed(to_do):    # 如果只是让线程执行，而不需要获取返回值，可以不用这段代码。
#             # 用 as_completed() 返回的顺序是按线程执行所用时间来排的，越早完成，越早取出来。
#             # 如果直接用 for i in to_do: 来读取结果，那么结果是按Future放入to_do中的顺序来取的。
#         res = i.result()
#         print(res)


# from concurrent import futures
# import time, os


# def func(n):
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '开始时间{}，传入值<{}>，结束时间{}'.format(start, n, end)


# if __name__ == '__main__':

#     print('#============单个进程============')
#     executor = futures.ProcessPoolExecutor(os.cpu_count())   # 默认值为CPU的核心数量os.cpu_count()。
#     sub = executor.submit(func, 3)    # 利用submit()把调用函数提交到进程池，返回一个Future实例。
#     # print(sub)
#     # print(type(sub))
#     res = sub.result()    # 利用result()获取调用函数的运行结果。
#     print(res)
#     print()


#     print('#============多个进程============')
#     with  futures.ProcessPoolExecutor(4) as executor:
#         to_do = [ ]
#         for i in [1, 2, 8, 4, 3]:
#             sub = executor.submit(func, i)
#             to_do.append(sub)    # 存储各个Future到列表中，给后面的 as_completed()使用。

#         for i in futures.as_completed(to_do):
#         # for i in to_do:
#         # 用 as_completed() 返回的顺序是按进程执行所用时间来排的，越早完成，越早取出来。
#         # 如果直接用 for i in to_do: 来读取结果，那么结果是按Future放入to_do中的顺序来取的。
#             res = i.result()
#             print(res)


#-------------------------------------------------------------------

# from concurrent.futures import ThreadPoolExecutor
# import urllib.request
# URLS = ['http://www.163.com', 'https://www.baidu.com/', 'https://github.com/']
# def load_url(url):
#     with urllib.request.urlopen(url, timeout=60) as conn:
#         print('%r page is %d bytes' % (url, len(conn.read())))

# executor = ThreadPoolExecutor(max_workers=3)

# for url in URLS:
#     future = executor.submit(load_url,url)
#     print(future.done())

# print('主线程')


# from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor #线程池，进程池
# import threading,time
 
# def test(arg):
#     print(arg,threading.current_thread().name)
#     time.sleep(1)
 
# if __name__ == "__main__":
#     thread_pool = ThreadPoolExecutor(5) #定义5个线程执行此任务
#     process_pool = ProcessPoolExecutor(5) #定义5个进程
#     for i in range(20):
#         thread_pool.submit(test,i)


# import time
# from concurrent.futures import ThreadPoolExecutor
# def func(name):
#     print(f"{name}开始")
#     time.sleep(2)
#     print(f"{name}结束")
# if __name__ == '__main__':
#     p = ThreadPoolExecutor(max_workers=3)  # 创建一个线程池，里面最多有3个线程同时工作
#     for i in range(1, 10):
#         p.submit(func, f"线程{i}")
#     p.shutdown()  # 主线程等待子线程结束  
#     print("主线程结束")


# import os, time, random, threading
# from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# def f(n):
#     time.sleep(random.randint(1, 3))
#     # print(n)
#     # print("进程(%s) %s的平方: %s" % (os.getpid(), n, n*n))
#     print("线程(%s) %s的平方: %s\n" % (threading.current_thread().getName(), n, n * n))
#     return n * n
# if __name__ == '__main__':
#     pool = ThreadPoolExecutor(max_workers=5)
#     # pool = ProcessPoolExecutor(max_workers=5)
#     ret_list = []
#     for i in range(10):
#         ret = pool.submit(f, i)  # 异步提交任务,f函数名称或者方法名称,i给f函数的参数
#         # print(ret.result())  #join
#         ret_list.append(ret)
#     # pool.shutdown()  #锁定线程池,不让新任务再提交进来了.轻易不用
#     for i in ret_list:
#         print(i.result())




# from  concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor
# from threading import currentThread
# import os,time,random
# def task(n):
#     print('%s:%s is running'%(currentThread().getName(),os.getpid()))  #看到的pid都是一样的，因为线程是共享了一个进程
#     time.sleep(random.randint(1,3))  #I/O密集型的，，一般用线程，用了进程耗时长
#     return n**2
# if __name__ == '__main__':
#     start = time.time()
#     p = ThreadPoolExecutor() #线程池 #如果不给定值，默认cup*5
#     l = []
#     for i in range(10):  #10个任务 # 线程池效率高了
#         obj  = p.submit(task,i)  #相当于apply_async异步方法
#         l.append(obj)
#     # p.shutdown()  #默认有个参数wait=True (相当于close和join)
#     print('='*30)
#     print([obj.result() for obj in l])
#     print(time.time() - start)  #3.001171827316284



# from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor
# import os, time, requests

# def get_page(url):
#     print('<%s> is getting [%s]'%(os.getpid(),url))
#     response = requests.get(url)
#     if response.status_code==200:  #200代表状态：下载成功了
#         return {'url':url,'text':response.text}
# def parse_page(res):
#     res = res.result()
#     print('<%s> is getting [%s]'%(os.getpid(),res['url']))
#     with open('db.txt','a') as f:
#         parse_res = 'url:%s size:%s\n'%(res['url'],len(res['text']))
#         f.write(parse_res)
# if __name__ == '__main__':
#     # p = ThreadPoolExecutor()
#     p = ProcessPoolExecutor()
#     l = [
#         'http://www.baidu.com',
#         'http://www.baidu.com',
#         'http://www.baidu.com',
#         'http://www.baidu.com',
#     ]
#     for url in l:
#         res = p.submit(get_page,url).add_done_callback(parse_page) #这里的回调函数拿到的是一个对象。得
#         #  先把返回的res得到一个结果。即在前面加上一个res.result() #谁好了谁去掉回调函数
#         # 回调函数也是一种编程思想。不仅开线程池用，开线程池也用
#     # p.shutdown()  #相当于进程池里的close和join
#     print('主',os.getpid())



# import time
# from concurrent import futures
# from concurrent.futures import ThreadPoolExecutor


# def display(args):
#     print(time.strftime('[%H:%M:%S]', time.localtime()), end=' ')
#     print(args)


# def task(n):
#     """只是休眠"""
#     display('begin sleep {}s.'.format(n))
#     time.sleep(n)
#     display('ended sleep {}s.'.format(n))


# def do_many_task_inorder():
#     """多线程
#     按任务发布顺序依次等待完成
#     """
#     tasks = [5, 4, 3, 2, 1]
#     with ThreadPoolExecutor(max_workers=3) as executor:
#         future_list = [executor.submit(task, arg) for arg in tasks]

#         display('非阻塞运行')

#         for future in future_list:
#             display(future)

#         display('统一结束(有序)')

#         for future in future_list:
#             display(future.result())


# def do_many_task_disorder():
#     """多线程执行
#     先完成先显示
#     """
#     tasks = [5, 4, 3, 2, 1]
#     with ThreadPoolExecutor(max_workers=3) as executor:
#         future_list = [executor.submit(task, arg) for arg in tasks]

#         display('非阻塞运行')

#         for future in future_list:
#             display(future)

#         display('统一结束(无序)')

#         done_iter = futures.as_completed(future_list)  # generator

#         for done in done_iter:
#             display(done)


# if __name__ == '__main__':
    # do_many_task_inorder()
    # do_many_task_disorder()


# concurrent.futures.ThreadPoolExecutor，在提交任务的时候，有两种方式，一种是submit（）函数，另外一种是map（）函数，二者的主要区别在于：
# 2.一、map能够保证输出的顺序, submit输出的顺序是乱的
# 2.二、若是你要提交的任务的函数是同样的，就能够简化成map。可是假如提交的任务函数是不同的，或者执行的过程之可能出现异常（使用map执行过程当中发现问题会直接抛出错误）就要用到submit（）
# 2.三、submit和map的参数是不一样的，submit每次都须要提交一个目标函数和对应的参数，map只须要提交一次目标函数，目标函数的参数放在一个迭代器（列表，字典）里就能够。



#-------------------------------------- future故障 --------------------------------------
# ThreadPoolExecutor 线程池执行器是 Executor 执行器的子类，通过线程池来执行异步调用。它管理一组工作线程，当工作线程有富余的时候，给它们传递任务。
# 当属于一个 Future 对象的可调用对象等待另一个 Future 的返回时，会发生死锁 deadlock。

# from concurrent.futures import ThreadPoolExecutor
# import time

# def wait_on_b():
#     time.sleep(5)
#     print(b.result())  # b will never complete because it is waiting on a.
#     return 5

# def wait_on_a():
#     time.sleep(5)
#     print(a.result())  # a will never complete because it is waiting on b.
#     return 6


# executor = ThreadPoolExecutor(max_workers=2)
# a = executor.submit(wait_on_b)
# # b = executor.submit(wait_on_a)
# print(a)

# # 再举一例：
# def wait_on_future():
#     f = executor.submit(pow, 5, 2)
#     # This will never complete because there is only one worker thread and
#     # it is executing this function.
#     print(f.result())

# executor = ThreadPoolExecutor(max_workers=1)
# executor.submit(wait_on_future)




# from concurrent.futures import ThreadPoolExecutor as Pool
# from concurrent.futures import as_completed
# from concurrent.futures import wait
# import requests
 
# URLS = ['http://qq.com', 'http://sina.com', 'http://www.baidu.com', ]
 
 
# def task(url, timeout=10):
#     return requests.get(url, timeout=timeout)
 
 
# with Pool(max_workers=3) as executor:
#     future_tasks = [executor.submit(task, url) for url in URLS]
 
#     for f in future_tasks:
#         if f.running():
#             print('%s is running' % str(f))
 
#     for f in as_completed(future_tasks):
#         try:
#             ret = f.done()
#             if ret:
#                 f_ret = f.result()
#                 print('%s, done, result: %s, %s' % (str(f), f_ret.url, len(f_ret.content)))
#         except Exception as e:
#             f.cancel()
#             print(str(e))

#     results = wait(future_tasks)
#     done = results[0]
#     for x in done:
#         print(x)


'''-------------------------------------------------- asyncio -----------------------------------------------------'''

# 一、asyncio

# import asyncio,time

# # 定义异步函数
# async def hello():
#     await asyncio.sleep(1)
#     print('Hello World:%s' % time.time())

# if __name__ =='__main__':
#     tasks = [hello() for i in range(5)]
#     loop = asyncio.get_event_loop()
#     loop.run_until_complete(asyncio.wait(tasks))

# async def 用来定义异步函数，await 表示当前协程任务等待睡眠时间，允许其他任务运行。
# 然后获得一个事件循环，主线程调用asyncio.get_event_loop()时会创建事件循环，你需要把异步的任务丢给这个循环的run_until_complete()方法，事件循环会安排协同程序的执行。


# 二、aiohttp

# import asyncio
# from aiohttp import ClientSession

# tasks = []
# url = "https://www.baidu.com/{}"
# async def hello(url):
#     async with ClientSession() as session:
#         async with session.get(url) as response:
#             response = await response.read()
#             print(response)

# if __name__ == '__main__':
#     loop = asyncio.get_event_loop()
#     loop.run_until_complete(hello(url))

# 多链接异步访问:
# 如果我们需要请求多个URL该怎么办呢，同步的做法访问多个URL只需要加个for循环就可以了。
# 但异步的实现方式并没那么容易，在之前的基础上需要将hello()包装在asyncio的Future对象中，然后将Future对象列表作为任务传递给事件循环。

# import time
# import asyncio
# from aiohttp import ClientSession

# tasks = []
# url = "https://www.baidu.com/{}"
# async def hello(url):
#     async with ClientSession() as session:
#         async with session.get(url) as response:
#             response = await response.read()
# #            print(response)
#             print('Hello World:%s' % time.time())

# def run():
#     for i in range(5):
#         task = asyncio.ensure_future(hello(url.format(i)))
#         tasks.append(task)

# if __name__ == '__main__':
#     loop = asyncio.get_event_loop()
#     run()
#     loop.run_until_complete(asyncio.wait(tasks))


# 收集http响应

# import time
# import asyncio
# from aiohttp import ClientSession

# tasks = []
# url = "https://www.baidu.com/{}"
# async def hello(url):
#     async with ClientSession() as session:
#         async with session.get(url) as response:
# #            print(response)
#             print('Hello World:%s' % time.time())
#             return await response.read()

# def run():
#     for i in range(5):
#         task = asyncio.ensure_future(hello(url.format(i)))
#         tasks.append(task)
#     result = loop.run_until_complete(asyncio.gather(*tasks))
#     print(result)

# if __name__ == '__main__':
#     loop = asyncio.get_event_loop()
#     run()


# 异常解决:
# 假如你的并发达到2000个，程序会报错：ValueError: too many file descriptors in select()。
# 报错的原因字面上看是 Python 调取的 select 对打开的文件有最大数量的限制，这个其实是操作系统的限制，linux打开文件的最大数默认是1024，windows默认是509，超过了这个值，程序就开始报错。
# 这里我们有三种方法解决这个问题：
# 1.限制并发数量。（一次不要塞那么多任务，或者限制最大并发数量）
# 2.使用回调的方式。
# 3.修改操作系统打开文件数的最大限制，在系统里有个配置文件可以修改默认值，具体步骤不再说明了。
# 不修改系统默认配置的话，个人推荐限制并发数的方法，设置并发数为500，处理速度更快。


# import time,asyncio,aiohttp


# url = 'https://www.baidu.com/'
# async def hello(url,semaphore):
#     async with semaphore:
#         async with aiohttp.ClientSession() as session:
#             async with session.get(url) as response:
#                 return await response.read()


# async def run():
#     semaphore = asyncio.Semaphore(500) # 限制并发量为500
#     to_get = [hello(url.format(),semaphore) for _ in range(1000)] #总共1000任务
#     await asyncio.wait(to_get)


# if __name__ == '__main__':
# #    now=lambda :time.time()
#     loop = asyncio.get_event_loop()
#     loop.run_until_complete(run())
#     loop.close()


#-------------------------------------------------------------------


# import asyncio
# import requests
# import os


# async def asyncget(url,headers):
#     return requests.get(url,headers=headers,verify=False)

# async def download(url,headers,fileDir,filename=None):

#     if filename == None:
#         filename = url[url.rfind("/")+1:]
#     if fileDir == None:
#         fileDir = "./files"

#     if not os.path.exists(fileDir):
#         print("亲，输入的目录不存在哦，下面将自动创建 "+ fileDir)
#         os.makedirs(fileDir)
    
#     filepath = os.path.join(fileDir,filename)

#     if os.path.exists(filepath):
#         print("亲，文件",filepath,"已经存在哦")
#         return
    
#     response =  await asyncget(url,headers=headers)

#     with open(filepath,"wb") as fp:
#         fp.write(response.content)
#     print("亲，文件",filepath,"写入完成哦")


# if __name__ == "__main__":
#     headers ={
#         "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36"
#     }
#     urls = [
#         "http://wx1.sinaimg.cn/mw600/00893JKXly1gikm1th96cj30u015b1a9.jpg",
#         "http://wx1.sinaimg.cn/mw600/00893JKXly1gikm1lkcxdj30sg18g0zh.jpg",
#         "http://wx2.sinaimg.cn/mw600/005QrDFkly1gfl4phpqvaj31w91w9x20.jpg"
#     ]

#     asyncio.run(
#         asyncio.wait(
#             [download(url=url,headers=headers,fileDir="imgs") for url in urls]
#         )
#     )



# import requests

# session = requests.Session()
# session.mount("https://", requests.adapters.HTTPAdapter(pool_maxsize=50))
# session.mount("http://", requests.adapters.HTTPAdapter(pool_maxsize=50))

# import asyncio

# async def perform_async_calls(self, session, urls):
#     loop = asyncio.get_event_loop()
#     futures = []
#     for url in urls:
#         futures.append(loop.run_in_executor(None, session.get, url)

#     results = []
#     for future in futures:
#         result = await future
#         results.append(result.json())

#     return results





# import asyncio

# async def test(): 
#     print('hello 异步')

# c = test() # 调用异步函数,得到协程对象-->c 
# print(c)
# asyncio.run(c)

# # asyncio.run(test())


# 说明:
# 一、使用async def 将方法变成协程，用await修饰有阻塞的操作.
# 二、await后可跟三种类型: 协程, 任务 和 Future.

# # 1、并发运行两个coroutine，写法一： 用Task
import asyncio
import time

async def say_after(delay, what):
    await asyncio.sleep(delay)
    print(what)

async def main():
    task1 = asyncio.create_task(say_after(10, 'hello'))
    task2 = asyncio.create_task(say_after(2, 'world'))

    print(f"started at {time.strftime('%X')}")
    r1 = await task1
    r2 = await task2
    print(f"finished at {time.strftime('%X')}")
    print("result: ", r1, r2)

asyncio.run(main())


# # 2、并发运行两个coroutine，写法二： 用 gather
# import asyncio
# import time

# async def say_after(delay, what):
#     await asyncio.sleep(delay)
#     print(what)

# async def main():
#     print(f"started at {time.strftime('%X')}")
#     result = await asyncio.gather(say_after(10, 'hello'), say_after(2, 'world'))
#     print(f"finished at {time.strftime('%X')}")
#     print("result: ", result)

# asyncio.run(main())



# 启动一个协程，任务有返回值
# import asyncio

# async def coroutine():
#     print('协程运行...')
#     return 'ok'

# # 定义一个事件循环监听
# event_loop = asyncio.get_event_loop()

# try:
#     print('协程开始...')
#     coroutine_obj = coroutine()
#     print('进入事件循环监听...')
#     return_value = event_loop.run_until_complete(coroutine())  # run_until_complete翻译成中文：一直运行到完成为止
#     print('coroutine()返回值：', return_value)
# finally:
#     print('关闭事件循环监听..')
#     event_loop.close()


# 启动一个协程，任务调用其它任务运行，需要注意：await 的使用
# import asyncio

# async def coroutine():
#     print('coroutine内部运行')

#     print('等待task_1运行结果')
#     task1 = await task_1() #await作用：控制运行流程，按顺序执行，即等待该函数运行完成，再继续往后执行
#     print('等待task_2运行结果')
#     task2 = await task_2(task1)

#     return (task1, task2)

# async def task_1():
#     print('task_1内部运行')
#     return 'task_1 ok'
# async def task_2(arg):
#     print('task_2内部运行')
#     return 'task_2 arg:{}'.format(arg)

# event_loop = asyncio.get_event_loop()  # 定义一个事件循环监听

# try:
#     coroutine_obj = coroutine()
#     return_value = event_loop.run_until_complete(coroutine())  # run_until_complete翻译成中文：一直运行到完成为止
#     print('coroutine()返回值：', return_value)
# finally:
#     event_loop.close()


# 4、生成器而不是协程
# @asyncio.coroutine 替换为 async, yield from 替换为 await
# import asyncio

# async def coroutine():
#     print('coroutine内部运行')

#     print('等待task_1运行结果')
#     task1 = await task_1()  # await作用： 控制运行流程，按顺序执行，即等待该函数运行完成，再继续往后执行
#     print('等待task_2运行结果')
#     task2 = await task_2(task1)

#     return (task1, task2)

# async def task_1():
#     print('task_1内部运行')
#     return 'task_1 ok'

# async def task_2(arg):
#     print('task_2内部运行')
#     return 'task_2 arg:{}'.format(arg)

# event_loop = asyncio.get_event_loop()

# try:
#     coroutine_obj = coroutine()
#     return_value = event_loop.run_until_complete(coroutine())  # run_until_complete翻译成中文：一直运行到完成为止
#     print('coroutine()返回值：', return_value)
# finally:
#     event_loop.close()



# 5、协程回调函数调用，此示例：讯速回调
# import asyncio
# import functools

# def callback(arg, *, kwarg='default'):
#     print('回调函数arg={},kwarg={}'.format(arg, kwarg))

# async def main(loop):
#     print('注册回调函数')
#     loop.call_soon(callback, 1)  # 执行回调函数，传入参数1
#     wrapped = functools.partial(callback, kwarg='not default')  # 利用偏函数，给kwarg传默认值
#     loop.call_soon(wrapped, 2)  # 执行回调函数，传入参数2
#     await asyncio.sleep(4)

# event_loop = asyncio.get_event_loop()
# try:
#     print('进入事件循环监听')
#     event_loop.run_until_complete(main(event_loop))  # 将事件循环对象传入main函数中
# finally:
#     print('关闭事件循环监听')
#     event_loop.close()


# 6、协程回调函数调用，此示例：延时回调
# import asyncio

# def callback(arg):
#     print('回调函数arg={}'.format(arg))

# async def mai(loop):
#     print('注册回调函数')
#     loop.call_later(1,callback,'延时1秒回调参数1')
#     loop.call_later(1,callback,'延时1秒回调参数2')
#     loop.call_soon(callback,'讯速的回调参数')
#     await asyncio.sleep(3)

# event_loop = asyncio.get_event_loop()
# try:
#     print('进入事件循环监听')
#     event_loop.run_until_complete(mai(event_loop))  # 将事件循环对象传入main函数中
# finally:
#     print('关闭事件循环监听')
#     event_loop.close()


# 7、协程回调函数调用，此示例：指定时间回调
# import asyncio
# import time

# def callback(arg, loop):
#     print('回调函数arg={} 回调的时间time={}'.format(arg, loop.time()))

# async def main(loop):
#     now = loop.time()
#     print('时钟时间:{}'.format(time.time()))
#     print('时事件循环时间:{}'.format(loop.time()))
#     print('注册回调函数')
#     loop.call_at(now + 1, callback, '参数1', loop)
#     loop.call_at(now + 2, callback, '参数2', loop)
#     loop.call_soon(callback, '讯速的回调参数', loop)
#     await asyncio.sleep(4)

# event_loop = asyncio.get_event_loop()
# try:
#     print('进入事件循环监听')
#     event_loop.run_until_complete(main(event_loop))  # 将事件循环对象传入main函数中
# finally:
#     print('关闭事件循环监听')
#     event_loop.close()


 # 8、基于Future实现异步返回任务执行结果
# import asyncio

# def mark_done(future, result):
#     """标记完成的函数"""
#     print('设置 Future 返回结果 {}'.format(result))
#     future.set_result(result)

# event_loop = asyncio.get_event_loop()
# try:
#     all_done = asyncio.Future()
#     print('调度标记完成的函数')
#     event_loop.call_soon(mark_done, all_done, '这个是调度传入的数据')
#     result = event_loop.run_until_complete(all_done)
#     print('运行返回的结果:{}'.format(result))
# finally:
#     print('关闭事件循环监听')
#     event_loop.close()

# print('Future 返回的结果: {}'.format(all_done.result()))
# """
# 结论：
#     返回结果可以从两个地方获取：
#     1、result = event_loop.run_until_complete(all_done)
#     2、Future.result()
# """


# 9、基于Future+await类现异步返回任务执行结果
# import asyncio

# def mark_done(future, result):
#     """标记完成的函数"""
#     print('设置 Future 返回结果 {}'.format(result))
#     future.set_result(result)

# async def main(loop):
#     all_done = asyncio.Future()
#     print('调度标记完成的函数')
#     loop.call_soon(mark_done, all_done, '这个是调度传入的数据')
#     result = await all_done  # await作用：等all_done返回结果，再往下运行
#     print('mark_done()执行完成，返回值 : {}'.format(result))

# event_loop = asyncio.get_event_loop()
# try:
#     event_loop.run_until_complete(main(event_loop))
# finally:
#     print('关闭事件循环监听')
#     event_loop.close()


# 10、基于Future的回调
# import asyncio, functools
# def callback(future, n):
#     print('{}: future 完成: {}'.format(n, future.result()))

# async def register_callbacks(future_obj):
#     print('将回调函数注册到Future中')
#     future_obj.add_done_callback(functools.partial(callback, n=1))  # 这里需要注意的是add_done_callback函数，还为把当前实例对象作为参数，传给函数，所以回调函数多一个callback(future, n)
#     future_obj.add_done_callback(functools.partial(callback, n=2))

# async def main(future_obj):
#     await register_callbacks(future_obj)  # 注册future的回调函数
#     print('设置Future返回结果')
#     future_obj.set_result('the result')

# event_loop = asyncio.get_event_loop()
# try:
#     future_obj = asyncio.Future()  # 创建一个future实例
#     event_loop.run_until_complete(main(future_obj))  # 增future实例传给main函数处理
# finally:
#     event_loop.close()


 # 11、asyncio创建任务运行
# import asyncio

# async def task_func():
#     print('task_func 执行完成')
#     return 'task_func返回值ok'

# async def main(loop):
#     print('创建任务')
#     task = loop.create_task(task_func())
#     print('等待task的结果 {}'.format(task))
#     return_value = await task #直到遇到await，上面的task开始运行
#     print('已完成任务{}'.format(task)) #经过上面的运行，task里面已经有result执行结果
#     print('return value: {}'.format(return_value))

# event_loop = asyncio.get_event_loop()
# try:
#     event_loop.run_until_complete(main(event_loop))
# finally:
#     event_loop.close()


# 12、asyncio取消任务运行
# import asyncio

# async def task_func():
#     print('task_func 执行完成')
#     return 'task_func返回值ok'

# async def main(loop):
#     print('创建任务')
#     task = loop.create_task(task_func())
#     print('取消任务')
#     task.cancel()
#     print('取消任务结果 {}'.format(task))
#     try:
#         await task #直到遇到await，上面的task开始运行
#     except asyncio.CancelledError:
#         print('从已取消的任务中捕获错误')
#     else:
#         print('任务执行结果 {}'.format(task))

# event_loop = asyncio.get_event_loop()
# try:
#     event_loop.run_until_complete(main(event_loop))
# finally:
#     event_loop.close()


# 13、利用回调取消任务执行
# import asyncio

# async def task_func():
#     print('task_func睡眠')
#     try:
#         await asyncio.sleep(1)
#     except asyncio.CancelledError:
#         print('task_func 任务取消')
#         raise
#     return 'task_func返回值ok'

# def task_canceller(task_obj):
#     print('task_canceller运行')
#     task_obj.cancel()
#     print('取消task_obj任务')

# async def main(loop):
#     print('创建任务')
#     task = loop.create_task(task_func())
#     loop.call_soon(task_canceller, task)
#     try:
#         await task  # 直到遇到await，上面的task开始运行
#     except asyncio.CancelledError:
#         print('从已取消的任务中捕获错误')
#     else:
#         print('任务执行结果 {}'.format(task))

# event_loop = asyncio.get_event_loop()
# try:
#     event_loop.run_until_complete(main(event_loop))
# finally:
#     event_loop.close()


# 14、asyncio.ensure_future(),增加函数，直到await才运行
# import asyncio

# async def wrapped():
#     print('wrapped 运行')
#     return 'wrapped result'

# async def inner(task):
#     print('inner: 开始运行')
#     print('inner: task {!r}'.format(task))
#     result = await task
#     print('inner: task 返回值 {!r}'.format(result))

# async def start_task():
#     print('开始创建task')
#     task = asyncio.ensure_future(wrapped())
#     print('等待inner运行')
#     await inner(task)
#     print('starter: inner returned')

# event_loop = asyncio.get_event_loop()
# try:
#     print('进程事件循环')
#     result = event_loop.run_until_complete(start_task())
# finally:
#     event_loop.close()


# 15、asyncio.wait()批量等待多个协程直到运行完成，包装多个返回显示结果
# import asyncio

# async def phase(i):
#     print('phase形参传入值{}'.format(i))
#     await asyncio.sleep(0.1 * i)
#     print('完成phase的次数{}'.format(i))
#     return 'phase {} result'.format(i)

# async def main(num_phase):
#     print('main函数开始')
#     phases = [
#         phase(i) for i in range(num_phase)
#     ]
#     print('等待phases里面的多个函数执行完成')
#     completed, pending = await asyncio.wait(phases)  # completed : 运行完成存在这里 ，pending ： 没有运行完成存在这里
#     for_completed_results = [t.result() for t in completed]
#     print('for_completed_results : {}'.format(for_completed_results))

# event_loop = asyncio.get_event_loop()
# try:
#     print('进程事件循环')
#     result = event_loop.run_until_complete(main(3))
# finally:
#     event_loop.close()


# 16、asyncio.wait()批量等待多个协程设置超时时间并且取消未完成的任务，包装多个返回显示结果
# import asyncio

# async def phase(i):
#     print('phase形参传入值{}'.format(i))
#     try:
#         await asyncio.sleep(0.1 * i)
#     except asyncio.CancelledError:
#         print('phase {} 取消'.format(i))
#     else:
#         print('完成phase的次数{}'.format(i))
#         return 'phase {} result'.format(i)

# async def main(num_phase):
#     print('main函数开始')
#     phases = [
#         phase(i) for i in range(num_phase)
#     ]
#     print('等待phases里面的多个函数执行完成')
#     completed, pending = await asyncio.wait(phases, timeout=0.1)  # completed : 运行完成存在这里 ，pending ： 没有运行完成存在这里

#     print('completed长度：{}，pending长度 :{}'.format(len(completed), len(pending)))

#     if pending:
#         print('取消未完成的任务')
#         for t in pending:
#             t.cancel()
#     print('main函数执行完成')

# event_loop = asyncio.get_event_loop()
# try:
#     print('进程事件循环')
#     result = event_loop.run_until_complete(main(3))
# finally:
#     event_loop.close()


# 17、asyncio.gather()多个协程运行，函数返回值接收
# import asyncio

# async def phase1():
#     print('phase1运行中')
#     await asyncio.sleep(4)
#     print('phase1运行完成')
#     return 'phase1 result'
# async def phase2():
#     print('phase2运行中')
#     await asyncio.sleep(1)
#     print('phase2运行完成')
#     return 'phase2 result'

# async def main():
#     print('main函数开始')
#     results = await asyncio.gather(phase1(),phase2())
#     print('results : {}'.format(results))

# event_loop = asyncio.get_event_loop()
# try:
#     print('进程事件循环')
#     result = event_loop.run_until_complete(main())
# finally:
#     event_loop.close()


# 18、asyncio.as_completed()多个协程运行，函数返回值不是有序的接收
# import asyncio

# async def phase(i):
#     print('phase {} 运行中'.format(i))
#     await asyncio.sleep(1 - (0.1 * i))
#     print('phase {} 运行完成'.format(i))
#     return 'phase {} result'.format(i)

# async def main(num_phases):
#     print('main函数开始')
#     phases = [
#         phase(i) for i in range(num_phases)
#     ]
#     print('等待phases运行完成')
#     results = []
#     for next_to_complete in asyncio.as_completed(phases):
#         task_result = await next_to_complete
#         print('接到到task_result : {}'.format(task_result))
#         results.append(task_result)
#     print('results : {}'.format(results))
#     return results

# event_loop = asyncio.get_event_loop()
# try:
#     print('进程事件循环')
#     event_loop.run_until_complete(main(3))
# finally:
#     event_loop.close()


# 19、asyncio.Lock() 协程锁的打开与关闭
# import asyncio, functools

# def unlock(lock):
#     print('回调释放锁')
#     lock.release()

# async def coro1(lock):
#     """with方式获得锁"""
#     async with lock:
#         print('coro1 打开锁运算')
#     print('coro1 锁已释放')

# async def coro2(lock):
#     """传统方式获取锁"""
#     await lock.acquire()
#     try:
#         print('coro2 打开锁运算')
#     finally:
#         print('coro2 锁已释放')
#         lock.release()

# async def main(loop):
#     lock = asyncio.Lock()
#     print('启动协程之前获取锁')
#     await lock.acquire()
#     print('获得锁 {}'.format(lock.locked()))

#     # 运行完成，回调解锁
#     loop.call_later(2, functools.partial(unlock, lock))

#     print('等待协程运行')
#     await asyncio.wait([coro1(lock), coro2(lock)])


# event_loop = asyncio.get_event_loop()
# try:
#     event_loop.run_until_complete(main(event_loop))
# finally:
#     event_loop.close()


# 20、asyncio.Event() 事件的查看与设置
# import asyncio, functools

# def set_event(event):
#     print('回调设置event')
#     event.set()

# async def coro1(event):
#     print('coro1 等待事件')
#     await event.wait()
#     print('coro1 触发运行')

# async def coro2(event):
#     print('coro2 等待事件')
#     await event.wait()
#     print('coro2 触发运行')

# async def main(loop):
#     event = asyncio.Event()
#     print('event开始之前状态:{}'.format(event.is_set()))
#     loop.call_later(2, functools.partial(set_event, event))  # 延时0.1秒后，回调set_event函数
#     await asyncio.wait([coro1(event), coro2(event)])
#     print('event开始之后状态:{}'.format(event.is_set()))

# event_loop = asyncio.get_event_loop()
# try:
#     event_loop.run_until_complete(main(event_loop))
# finally:
#     event_loop.close()


# 21、asyncio.Condition(),对事件状态单独通知执行
# import asyncio

# async def consumer(condition_obj, i):
#     async with condition_obj:
#         print('消费者 {} 等待中'.format(i))
#         await condition_obj.wait()
#         print('消费者 {} 触发'.format(i))
#     print('消费者 {} 消费结束'.format(i))

# async def manipulate_condition(condition_obj):
#     print('开始操作condition')
#     await asyncio.sleep(1)
#     for i in range(3):
#         async with condition_obj:
#             print('通知消费者 {}'.format(i))
#             condition_obj.notify(i)
#         await asyncio.sleep(2)

#     async with condition_obj:
#         print('通知其它所有的消费者')
#         condition_obj.notify_all()
#     print('操作condition结束')

# async def main(loop):
#     condition_obj = asyncio.Condition()  # 创建一个操作状态的对象
#     consumers = [consumer(condition_obj, i) for i in range(5)]  # 运5个消费者函数
#     loop.create_task(manipulate_condition(condition_obj))  # 创建一个操作状态的任务
#     await asyncio.wait(consumers)  # 等待consumers所有的函数执行完成

# event_loop = asyncio.get_event_loop()
# try:
#     event_loop.run_until_complete(main(event_loop))
# finally:
#     event_loop.close()


# 22、协程队列Queue,生产者与消费者的示例
# import asyncio

# async def consumer(n, q):
#     print('消费者 {} 开始'.format(n))
#     while True:
#         print('消费费 {} 等待消费'.format(n))
#         item = await q.get()
#         print('消费者 {} 消费了 {}'.format(n, item))
#         if item is None:
#             q.task_done()
#             break
#         else:
#             await asyncio.sleep(0.01 * item)
#             q.task_done()
#     print('消费者 {} 结束'.format(n))

# async def producer(q, num_worker):
#     print('生产者 开始')

#     for i in range(num_worker * 3):
#         await q.put(i)
#         print('生产者 增加数据 {} 到队列中'.format(i))

#     print('生产者 增加停止信号到队列中')
#     for i in range(num_worker):
#         await q.put(None)
#     print('生产者 等待队列清空')
#     await q.join()
#     print('生产者 结束')

# async def main(loop, num_consumers):
#     q = asyncio.Queue(maxsize=num_consumers)  # 创建一个队列，最大的长度是num_consumers
#     consumers = [loop.create_task(consumer(i, q)) for i in range(num_consumers)]
#     producer_task = loop.create_task(producer(q, num_consumers))
#     await asyncio.wait(consumers + [producer_task])

# event_loop = asyncio.get_event_loop()
# try:
#     event_loop.run_until_complete(main(event_loop, 2))
# finally:
#     event_loop.close()


# 29、协程与线程结合（ThreadPoolExecutor）
# import sys, time, logging, asyncio,  concurrent.futures

# def blocks(n):
#     log = logging.getLogger('blocks({})'.format(n))
#     log.info('运行')
#     time.sleep(0.1)
#     log.info('done')
#     return n ** 2

# async def run_blocking_tasks(executor):
#     """运行阻塞的任务"""
#     log = logging.getLogger('run_blocking_tasks')
#     log.info('开始运行')
#     log.info('创建执行任务')
#     loop = asyncio.get_event_loop()
#     blocking_tasks = [loop.run_in_executor(executor, blocks, i) for i in range(6)]
#     log.info('等待执行任务')
#     completed, pending = await asyncio.wait(blocking_tasks)
#     results = [t.result() for t in completed]
#     log.info('运行结果: {!r}'.format(results))
#     log.info('exitrun_blocking_tasks 退出')

# if __name__ == '__main__':
#     logging.basicConfig(level=logging.INFO, format='%(threadName)10s %(name)18s: %(message)s', stream=sys.stderr)
#     executor = concurrent.futures.ThreadPoolExecutor(max_workers=3)  # 创建一个线程池执行器，最大开启3个工作线程
#     event_loop = asyncio.get_event_loop()  # 创建一个事件循环
#     try:
#         event_loop.run_until_complete(run_blocking_tasks(executor))
#     finally:
#         event_loop.close()


# 30、协程与进程结合（ProcessPoolExecutor）
# import sys, time, logging, asyncio, concurrent.futures

# def blocks(n):
#     log = logging.getLogger('blocks({})'.format(n))
#     log.info('运行')
#     time.sleep(0.1)
#     log.info('done')
#     return n ** 2

# async def run_blocking_tasks(executor):
#     """运行阻塞的任务"""
#     log = logging.getLogger('run_blocking_tasks')
#     log.info('开始运行')
#     log.info('创建执行任务')
#     loop = asyncio.get_event_loop()
#     blocking_tasks = [loop.run_in_executor(executor, blocks, i) for i in range(6)]
#     log.info('等待执行任务')
#     completed, pending = await asyncio.wait(blocking_tasks)
#     results = [t.result() for t in completed]
#     log.info('运行结果: {!r}'.format(results))
#     log.info('exitrun_blocking_tasks 退出')

# if __name__ == '__main__':
#     logging.basicConfig(level=logging.INFO, format='PID %(process)5s %(name)18s: %(message)s', stream=sys.stderr)
#     executor = concurrent.futures.ProcessPoolExecutor(max_workers=3)  # 创建一个线程池执行器，最大开启3个工作线程
#     event_loop = asyncio.get_event_loop()  # 创建一个事件循环
#     try:
#         event_loop.run_until_complete(run_blocking_tasks(executor))
#     finally:
#         event_loop.close()


# 31、asyncio调试模式的开启
# import sys, time, logging, argparse, asyncio, warnings

# parser = argparse.ArgumentParser('debugging asyncio')  #接收命令行
# parser.add_argument('-v', dest='verbose', default=False, action='store_true',)
# args = parser.parse_args()

# logging.basicConfig(level=logging.DEBUG, format='%(levelname)7s: %(message)s', stream=sys.stderr,)  # 设置日志级别
# LOG = logging.getLogger('')

# async def inner():
#     LOG.info('inner 函数开始')
#     time.sleep(0.1)
#     LOG.info('inner 运行完成')

# async def outer(loop):
#     LOG.info('outer 函数开始')
#     await asyncio.ensure_future(loop.create_task(inner()))  # ensure_future，直到await才运行
#     LOG.info('outer 运行完成')

# event_loop = asyncio.get_event_loop()
# if args.verbose:
#     LOG.info('开启DEBUG模式')
#     event_loop.set_debug(True)
#     event_loop.slow_callback_duration = 0.001  # 使“慢”任务的阈值非常小以便于说明。默认值为0.1，即100毫秒。
#     warnings.simplefilter('always', ResourceWarning)  # 在警告过滤器列表中插入一个简单的条目（在前面）。

# LOG.info('entering event loop')
# event_loop.run_until_complete(outer(event_loop))


# 32、利用生成器的方式，创建协程socket监听

# import sys, logging, asyncio

# async def echo(reader, writer):
#     address = writer.get_extra_info('peername')
#     log = logging.getLogger('echo_{}_{}'.format(*address))
#     log.debug('connection accepted')
#     while True:
#         data = await reader.read(128)
#         if data:
#             log.debug('received {!r}'.format(data))
#             writer.write(data)
#             await writer.drain()
#             log.debug('sent {!r}'.format(data))
#         else:
#             log.debug('closing')
#             writer.close()
#             return

# #开启Debug模式
# logging.basicConfig(
#     level=logging.DEBUG,
#     format='%(name)s: %(message)s',
#     stream=sys.stderr,
# )
# #设置日志的title
# log = logging.getLogger('main')

# #设置开启服务的IP+端口
# server_address = ('localhost', 8888)

# #获取事件循环
# event_loop = asyncio.get_event_loop()

# # 创建服务器，让循环在之前完成协同工作。并且启动实际事件循环
# coroutine = asyncio.start_server(echo, *server_address,loop=event_loop)
# server = event_loop.run_until_complete(coroutine)
# log.debug('starting up on {} port {}'.format(*server_address))

# try:
#     #开启一直循环处理任务
#     event_loop.run_forever()
# finally:
#     #结束后清理的工作
#     log.debug('closing server')
#     server.close()
#     event_loop.run_until_complete(server.wait_closed())
#     log.debug('closing event loop')
#     event_loop.close()





