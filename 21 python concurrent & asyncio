



'''-------------------------------------------- concurrent -----------------------------------------'''


# Python 中有多线程threading 和多进程multiprocessing 实现并发，
# 但是这两个东西开销很大，一是开启线程/进程的开销，二是主程序和子程序之间的通信需要 序列化和反序列化，
# 所以有些时候需要使用更加高级的用法，然而这些高级用法十分复杂，而且 threading 和 multiprocessing 用法还不一样。

# 于是诞生了 concurrent.future
# 1. 它可以解决大部分的复杂问题　　　　　　【但并不是全部，如果尝试后效果不好，还需要使用他们的高级用法】
# 2. 而且统一了线程和进程的用法
# concurrent.future 提供了 ThreadPoolExecutor 和 ProcessPoolExecutor 两个类，其实是对 线程池和进程池 的进一步抽象，而且具有以下特点：
# 3. 主程序可以获取子程序的状态和返回值
# 4. 子程序完成时，主程序能立刻知道


# 线程池：

 
# from concurrent.futures import ThreadPoolExecutor
# import time

# def spider(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# with ThreadPoolExecutor(max_workers=5) as t:  # 创建一个最大容纳量为5的线程池
#     task1 = t.submit(spider, 1)
#     task2 = t.submit(spider, 2)  # 通过submit提交执行的函数到线程池中
#     task3 = t.submit(spider, 3)  # 注意submit()不是阻塞的，而是立即返回的

#     # 通过done来判断线程是否完成
#     print(f"task1: {task1.done()}")  # 通过使用done()方法判断该任务是否结束，提交任务后立即判断任务状态，显示都是未完成，在显示2.5秒后
#     print(f"task2: {task2.done()}")
#     print(f"task3: {task3.done()}")

#     time.sleep(2.5)

#     print(f"task1: {task1.done()}")
#     print(f"task2: {task2.done()}")
#     print(f"task3: {task3.done()}")
#     # 通过result()来获取返回值
#     print(task1.result())


#-------------------------------------------------------------------

# # wait(fs, timeout=None, return_when=All_COMPLETED)
# # fs 表示需要执行的序列
# # timeout 等待的最大时间，如果超过这个时间
# # return_when 表示wait返回结果的结果，默认为ALLP_COMPLETED全部执行完成后再返回结果

# from concurrent.futures import ThreadPoolExecutor, wait, FIRST_COMPLETED, ALL_COMPLETED

# def spider2(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# with ThreadPoolExecutor(max_workers=5) as t:
#     all_task = [t.submit(spider2, page) for page in range(1, 5)]
#     # 返回条件 FIRST_COMPLETED 当第一个任务完成的时候就停止等待，继续主线程任务，所以紧接着打印了“结束”
#     wait(all_task, return_when=FIRST_COMPLETED)
#     print("结束")
#     # 设置延时（等待）时间2.5秒
#     print(wait(all_task, timeout=2.5))
#     # 所以最后只有task4还在运行


#-------------------------------------------------------------------

# as_completed
# 虽然使用return_when=FIRST_COMPLETED判断任务是否结束，但是不能在主线程中一直判断
# 最好的办法是当某个任务结束来，就给主线程返回结果，而不是一直判断每个任务是否结束
# as_completed就是当子线程中的任务执行完成后，直接用result()获取返回结果

# from concurrent.futures import ThreadPoolExecutor,as_completed
# import time

# def spider3(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# def main():
#     with ThreadPoolExecutor(max_workers=5) as t:
#         obj_list = []
#         for page in range(1, 5):
#             obj = t.submit(spider3, page)
#             obj_list.append(obj)
#         for future in as_completed(obj_list):
#             data = future.result()
#             print(f"main:{data}")

# main()  ## 使用过后发现，这个多线程比上面的测试耗时都多一些

# as_completed()方法是一个生成器，在没有任务完成的时候就会一直阻塞，除非设置了timeout
# 当某个任务完成的时候，会yield这个任务，就能执行for循环下面的语句，然后继续阻塞程序，直到所有任务结束
# 同时，先完成的任务会先返回给主线程

# ------------
# wait()与as_completed():
# wait方法可以让主线程阻塞，直到满足设定的要求。有三种条件ALL_COMPLETED, FIRST_COMPLETED，FIRST_EXCEPTION
# as_completed同样也与主进程阻塞有关(即需要所有future对象都运行结束才结束阻塞）

# from concurrent.futures import ThreadPoolExecutor,wait,as_completed,FIRST_COMPLETED,ALL_COMPLETED
# import time

# def test(g):
#     time.sleep(2)
#     print(f'{g}_crawling has done\n')
#     return f'result: {g}'

# if __name__ == '__main__':
#     record = []                                      # 创建列表放置后续产生的future对象
#     t = time.time()
#     with ThreadPoolExecutor(max_workers=10) as executor:
#     # 线程池的使用方式，由于是with方式，因此不需要自己关闭。同时线程池会自动join()
#     # 当然也可以使用executor = ThreadPoolExecutor(max_workers=4),
#     # 但需要在使用结束后使用shutdown方法，才能阻塞主线程。
#         for l in list(range(10)):                    # 创建一个参数列表用于后续传入函数
#             future = executor.submit(test,l)
#             # 传入要执行的函数及参数，返回的是一个future对象，注意这里不要调用，此处调用会使线程阻塞。
#             record.append(future)                    # 将对象添加到record列表中
#         wait(record, return_when=FIRST_COMPLETED)    # 第一次完成后即不再阻塞
#         # wait(record, return_when=ALL_COMPLETED)      # 全部完成后即不再阻塞
#         print('第一次运行结束')                       # 测试第一次完成后是否不再阻塞
#         for future in as_completed(record):          # 其实就相当于shutdown操作
#             print(future.result())                   # 并不按顺序提取，谁先完成就先打印，此处看不出来
#     print('程序结束')                                 # 测试主线程是否阻塞
#     print(time.time()-t)

#-------------------------------------------------------------------

# map
# map(fn, *iterables, timeout=None)
# fn 需要线程执行的函数
# iterables 接收一个可迭代对象
# 和wait()的timeout一样，用于延时，但是map是返回线程执行的结果，如果timeout小于线程执行时间会抛出TimeoutError

# from concurrent.futures import ThreadPoolExecutor,as_completed
# import time

# def spider4(page):
#     time.sleep(page)
#     print(f"crawl task {page} finished")
#     return page

# def main2():
#     executor = ThreadPoolExecutor(max_workers=4)
#     i = 1
#     # 列表中的每一个元素都执行来spider4()函数，并分配各线程池   task1:2
#     for result in executor.map(spider4, [2, 3, 1, 4]):
#         print(f"task{i}:{result}")
#         i += 1

# main2()

# 使用map方法，无需提前使用submit方法，与Python高阶函数map含义相同，都是将序列中的每个元素都执行同一个行数
# 与as_completed()方法的结果不同，输出顺序和列表的顺序相同，
# 就算1秒的任务执行完成，也会先打印前面提交的任务返回的结果

# map可以保证输出的顺序, submit输出的顺序是乱的
# 如果你要提交的任务的函数是一样的，就可以简化成map。但是假如提交的任务函数是不一样的，
# 或者执行的过程之可能出现异常（使用map执行过程中发现问题会直接抛出错误）就要用到submit（）
# submit和map的参数是不同的，submit每次都需要提交一个目标函数和对应的参数，
# map只需要提交一次目标函数，目标函数的参数放在一个迭代器（列表，字典）里就可以。




#-------------------------------------------------------------------

# from concurrent import futures
# import time, threading


# def func(n):                                 # 首先定义一个供线程调用的函数。  
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '{}开始时间{}，传入值<{}>，结束时间{}'.format(threading.current_thread(),start, n, end)  ## threading.enumerate(), threading.current_thread(), threading.current_thread().name, threading.current_thread().getName()


# executor = futures.ThreadPoolExecutor(max_workers=4)
#     # max_workers 为线程池中，最大工作线程的个数。

# results = executor.map(func,  [2,3,8,5,4])
#     # 利用 executor.map() 调用函数，第一个参数为函数名，后面为函数参数。
#     # executor.map 返回的是一个生成器，results内的值是按函数的“调用顺序“来排列的。
#     # 完成一个线程，就把结果放入 results 中。
#     # 如果要取里面的值，可以用for, list, enumerate等方法。
#     # 因为<线程3>的执行时间是5秒，晚于线程<线程4><线程5>完成。
#     # 所以下面的打印结果会打印到<线程3>时主线程就阻塞了，等待<线程3>的完成。
#     # 它会等待<线程3>完成后，再继续打印后面的<线程4><线程5>结果。

# for serial, value in enumerate(results, start=1):
#     print('线程 {}：{}'.format( serial, value))

# executor.shutdown()      # 关闭线程池。



# from concurrent import futures
# import time


# def func(n):
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '{}开始时间{}，传入值<{}>，结束时间{}'.format(threading.current_thread(),start, n, end)  ## threading.enumerate(), threading.current_thread(), threading.current_thread().name, threading.current_thread().getName()

# # 如果利用 with 上下文管理器，它会自动关闭线程池。
# with futures.ThreadPoolExecutor(max_workers=4) as executor:
#     # map会阻塞主线程，等待所有子线程结束，返回 result，如果子线程有报错，也会返回报错信息。
#     # 所以这是一个有了所有线程结果的results，下面的打印就不会有阻塞了。
#     results = executor.map(func,  [2,3,8,5,4])
        
# for serial, value in enumerate(results, start=1):
#     print('线程 {}：{}'.format( serial, value))


# from concurrent.futures import ThreadPoolExecutor
# import time

# def f_submit(x):
#     print(f'{time.strftime("%H:%M:%S")}submit：{x}\n')
#     time.sleep(1)
#     print(f'{time.strftime("%H:%M:%S")}{x}计算结果：{x**2}\n')
#     return x*x


# """ 用with开启线程池时，并自动关闭线程池。"""
# with ThreadPoolExecutor(max_workers=5) as e:
#     f1 = e.submit(f_submit, 1,)
#     f2 = e.submit(f_submit, 2,)
#     f3 = e.submit(f_submit, 3,)
#     f4 = e.submit(f_submit, 4,)
#     f5 = e.submit(f_submit, 5,)

#     """ result()获取子线程结果时，会阻塞主线程。
#         如果子线程有异常时，也会引发这个异常。
#         即，如果不用result()获取结果，那么即使子线程有异常，
#         该异常也不会被触发，只是那个有异常的子线程会停止运行。"""
#     print([f1.result(), f2.result(), f3.result(), f4.result(), f5.result()])
#     print('主线程\n')
#     #time.sleep(2)

# print('进程池外')    

# result()获取子线程结果时，会阻塞主线程。
# 如果子线程有异常时，也会引发这个异常（报错）。
# 即，如果不用result()获取结果，那么即使子线程有异常（报错），
# 该异常也不会被触发，只是那个有异常的子线程会停止运行。


# from concurrent import futures
# import time


# def func(n):
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '{}开始时间{}，传入值<{}>，结束时间{}'.format(threading.current_thread(),start, n, end)  ## threading.enumerate(), threading.current_thread(), threading.current_thread().name, threading.current_thread().getName()

# print('#============单个线程============')
# executor = futures.ThreadPoolExecutor(4)   # 最大线程数量为4个。
# sub = executor.submit(func, 3)    # 利用submit()把调用函数提交到线程池，返回一个Future实例。
# # print(sub)
# # print(type(sub))
# res = sub.result()    # 利用result()获取调用函数的运行结果。
# print(res)
# print()


# print('#============多个线程============')
# with  futures.ThreadPoolExecutor(4) as executor:
#     to_do = [ ]
#     for i in [2, 3, 8, 5, 4]:
#         sub = executor.submit(func, i)
#         to_do.append(sub)           # 存储各个Future到列表中，给后面的 as_completed()使用。

#     for i in futures.as_completed(to_do):    # 如果只是让线程执行，而不需要获取返回值，可以不用这段代码。
#             # 用 as_completed() 返回的顺序是按线程执行所用时间来排的，越早完成，越早取出来。
#             # 如果直接用 for i in to_do: 来读取结果，那么结果是按Future放入to_do中的顺序来取的。
#         res = i.result()
#         print(res)


# from concurrent import futures
# import time, os


# def func(n):
#     start = time.strftime('[%H:%M:%S]')
#     time.sleep(n)
#     end = time.strftime('[%H:%M:%S]')
#     return '开始时间{}，传入值<{}>，结束时间{}'.format(start, n, end)


# if __name__ == '__main__':

#     print('#============单个进程============')
#     executor = futures.ProcessPoolExecutor(os.cpu_count())   # 默认值为CPU的核心数量os.cpu_count()。
#     sub = executor.submit(func, 3)    # 利用submit()把调用函数提交到进程池，返回一个Future实例。
#     # print(sub)
#     # print(type(sub))
#     res = sub.result()    # 利用result()获取调用函数的运行结果。
#     print(res)
#     print()


#     print('#============多个进程============')
#     with  futures.ProcessPoolExecutor(4) as executor:
#         to_do = [ ]
#         for i in [1, 2, 8, 4, 3]:
#             sub = executor.submit(func, i)
#             to_do.append(sub)    # 存储各个Future到列表中，给后面的 as_completed()使用。

#         for i in futures.as_completed(to_do):
#         # for i in to_do:
#         # 用 as_completed() 返回的顺序是按进程执行所用时间来排的，越早完成，越早取出来。
#         # 如果直接用 for i in to_do: 来读取结果，那么结果是按Future放入to_do中的顺序来取的。
#             res = i.result()
#             print(res)


#-------------------------------------------------------------------

# from concurrent.futures import ThreadPoolExecutor
# import urllib.request
# URLS = ['http://www.163.com', 'https://www.baidu.com/', 'https://github.com/']
# def load_url(url):
#     with urllib.request.urlopen(url, timeout=60) as conn:
#         print('%r page is %d bytes' % (url, len(conn.read())))

# executor = ThreadPoolExecutor(max_workers=3)

# for url in URLS:
#     future = executor.submit(load_url,url)
#     print(future.done())

# print('主线程')


# from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor #线程池，进程池
# import threading,time
 
# def test(arg):
#     print(arg,threading.current_thread().name)
#     time.sleep(1)
 
# if __name__ == "__main__":
#     thread_pool = ThreadPoolExecutor(5) #定义5个线程执行此任务
#     process_pool = ProcessPoolExecutor(5) #定义5个进程
#     for i in range(20):
#         thread_pool.submit(test,i)


# import time
# from concurrent.futures import ThreadPoolExecutor
# def func(name):
#     print(f"{name}开始")
#     time.sleep(2)
#     print(f"{name}结束")
# if __name__ == '__main__':
#     p = ThreadPoolExecutor(max_workers=3)  # 创建一个线程池，里面最多有3个线程同时工作
#     for i in range(1, 10):
#         p.submit(func, f"线程{i}")
#     p.shutdown()  # 主线程等待子线程结束  
#     print("主线程结束")


# import os, time, random, threading
# from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor

# def f(n):
#     time.sleep(random.randint(1, 3))
#     # print(n)
#     # print("进程(%s) %s的平方: %s" % (os.getpid(), n, n*n))
#     print("线程(%s) %s的平方: %s\n" % (threading.current_thread().getName(), n, n * n))
#     return n * n
# if __name__ == '__main__':
#     pool = ThreadPoolExecutor(max_workers=5)
#     # pool = ProcessPoolExecutor(max_workers=5)
#     ret_list = []
#     for i in range(10):
#         ret = pool.submit(f, i)  # 异步提交任务,f函数名称或者方法名称,i给f函数的参数
#         # print(ret.result())  #join
#         ret_list.append(ret)
#     # pool.shutdown()  #锁定线程池,不让新任务再提交进来了.轻易不用
#     for i in ret_list:
#         print(i.result())




# from  concurrent.futures import ProcessPoolExecutor,ThreadPoolExecutor
# from threading import currentThread
# import os,time,random
# def task(n):
#     print('%s:%s is running'%(currentThread().getName(),os.getpid()))  #看到的pid都是一样的，因为线程是共享了一个进程
#     time.sleep(random.randint(1,3))  #I/O密集型的，，一般用线程，用了进程耗时长
#     return n**2
# if __name__ == '__main__':
#     start = time.time()
#     p = ThreadPoolExecutor() #线程池 #如果不给定值，默认cup*5
#     l = []
#     for i in range(10):  #10个任务 # 线程池效率高了
#         obj  = p.submit(task,i)  #相当于apply_async异步方法
#         l.append(obj)
#     # p.shutdown()  #默认有个参数wait=True (相当于close和join)
#     print('='*30)
#     print([obj.result() for obj in l])
#     print(time.time() - start)  #3.001171827316284



# from concurrent.futures import ThreadPoolExecutor,ProcessPoolExecutor
# import os, time, requests

# def get_page(url):
#     print('<%s> is getting [%s]'%(os.getpid(),url))
#     response = requests.get(url)
#     if response.status_code==200:  #200代表状态：下载成功了
#         return {'url':url,'text':response.text}
# def parse_page(res):
#     res = res.result()
#     print('<%s> is getting [%s]'%(os.getpid(),res['url']))
#     with open('db.txt','a') as f:
#         parse_res = 'url:%s size:%s\n'%(res['url'],len(res['text']))
#         f.write(parse_res)
# if __name__ == '__main__':
#     # p = ThreadPoolExecutor()
#     p = ProcessPoolExecutor()
#     l = [
#         'http://www.baidu.com',
#         'http://www.baidu.com',
#         'http://www.baidu.com',
#         'http://www.baidu.com',
#     ]
#     for url in l:
#         res = p.submit(get_page,url).add_done_callback(parse_page) #这里的回调函数拿到的是一个对象。得
#         #  先把返回的res得到一个结果。即在前面加上一个res.result() #谁好了谁去掉回调函数
#         # 回调函数也是一种编程思想。不仅开线程池用，开线程池也用
#     # p.shutdown()  #相当于进程池里的close和join
#     print('主',os.getpid())



# import time
# from concurrent import futures
# from concurrent.futures import ThreadPoolExecutor


# def display(args):
#     print(time.strftime('[%H:%M:%S]', time.localtime()), end=' ')
#     print(args)


# def task(n):
#     """只是休眠"""
#     display('begin sleep {}s.'.format(n))
#     time.sleep(n)
#     display('ended sleep {}s.'.format(n))


# def do_many_task_inorder():
#     """多线程
#     按任务发布顺序依次等待完成
#     """
#     tasks = [5, 4, 3, 2, 1]
#     with ThreadPoolExecutor(max_workers=3) as executor:
#         future_list = [executor.submit(task, arg) for arg in tasks]

#         display('非阻塞运行')

#         for future in future_list:
#             display(future)

#         display('统一结束(有序)')

#         for future in future_list:
#             display(future.result())


# def do_many_task_disorder():
#     """多线程执行
#     先完成先显示
#     """
#     tasks = [5, 4, 3, 2, 1]
#     with ThreadPoolExecutor(max_workers=3) as executor:
#         future_list = [executor.submit(task, arg) for arg in tasks]

#         display('非阻塞运行')

#         for future in future_list:
#             display(future)

#         display('统一结束(无序)')

#         done_iter = futures.as_completed(future_list)  # generator

#         for done in done_iter:
#             display(done)


# if __name__ == '__main__':
    # do_many_task_inorder()
    # do_many_task_disorder()


# concurrent.futures.ThreadPoolExecutor，在提交任务的时候，有两种方式，一种是submit（）函数，另外一种是map（）函数，二者的主要区别在于：
# 2.一、map能够保证输出的顺序, submit输出的顺序是乱的
# 2.二、若是你要提交的任务的函数是同样的，就能够简化成map。可是假如提交的任务函数是不同的，或者执行的过程之可能出现异常（使用map执行过程当中发现问题会直接抛出错误）就要用到submit（）
# 2.三、submit和map的参数是不一样的，submit每次都须要提交一个目标函数和对应的参数，map只须要提交一次目标函数，目标函数的参数放在一个迭代器（列表，字典）里就能够。



#-------------------------------------- future故障 --------------------------------------
# ThreadPoolExecutor 线程池执行器是 Executor 执行器的子类，通过线程池来执行异步调用。它管理一组工作线程，当工作线程有富余的时候，给它们传递任务。
# 当属于一个 Future 对象的可调用对象等待另一个 Future 的返回时，会发生死锁 deadlock。

# from concurrent.futures import ThreadPoolExecutor
# import time

# def wait_on_b():
#     time.sleep(5)
#     print(b.result())  # b will never complete because it is waiting on a.
#     return 5

# def wait_on_a():
#     time.sleep(5)
#     print(a.result())  # a will never complete because it is waiting on b.
#     return 6


# executor = ThreadPoolExecutor(max_workers=2)
# a = executor.submit(wait_on_b)
# # b = executor.submit(wait_on_a)
# print(a)

# # 再举一例：
# def wait_on_future():
#     f = executor.submit(pow, 5, 2)
#     # This will never complete because there is only one worker thread and
#     # it is executing this function.
#     print(f.result())

# executor = ThreadPoolExecutor(max_workers=1)
# executor.submit(wait_on_future)




# from concurrent.futures import ThreadPoolExecutor as Pool
# from concurrent.futures import as_completed
# from concurrent.futures import wait
# import requests
 
# URLS = ['http://qq.com', 'http://sina.com', 'http://www.baidu.com', ]
 
 
# def task(url, timeout=10):
#     return requests.get(url, timeout=timeout)
 
 
# with Pool(max_workers=3) as executor:
#     future_tasks = [executor.submit(task, url) for url in URLS]
 
#     for f in future_tasks:
#         if f.running():
#             print('%s is running' % str(f))
 
#     for f in as_completed(future_tasks):
#         try:
#             ret = f.done()
#             if ret:
#                 f_ret = f.result()
#                 print('%s, done, result: %s, %s' % (str(f), f_ret.url, len(f_ret.content)))
#         except Exception as e:
#             f.cancel()
#             print(str(e))

#     results = wait(future_tasks)
#     done = results[0]
#     for x in done:
#         print(x)


'''-------------------------------------------------- asyncio -----------------------------------------------------'''

# 一、asyncio

# import asyncio,time

# # 定义异步函数
# async def hello():
#     await asyncio.sleep(1)
#     print('Hello World:%s' % time.time())

# if __name__ =='__main__':
#     tasks = [hello() for i in range(5)]
#     loop = asyncio.get_event_loop()
#     loop.run_until_complete(asyncio.wait(tasks))

# async def 用来定义异步函数，await 表示当前协程任务等待睡眠时间，允许其他任务运行。
# 然后获得一个事件循环，主线程调用asyncio.get_event_loop()时会创建事件循环，你需要把异步的任务丢给这个循环的run_until_complete()方法，事件循环会安排协同程序的执行。


# 二、aiohttp

# import asyncio
# from aiohttp import ClientSession

# tasks = []
# url = "https://www.baidu.com/{}"
# async def hello(url):
#     async with ClientSession() as session:
#         async with session.get(url) as response:
#             response = await response.read()
#             print(response)

# if __name__ == '__main__':
#     loop = asyncio.get_event_loop()
#     loop.run_until_complete(hello(url))

# 多链接异步访问:
# 如果我们需要请求多个URL该怎么办呢，同步的做法访问多个URL只需要加个for循环就可以了。
# 但异步的实现方式并没那么容易，在之前的基础上需要将hello()包装在asyncio的Future对象中，然后将Future对象列表作为任务传递给事件循环。

# import time
# import asyncio
# from aiohttp import ClientSession

# tasks = []
# url = "https://www.baidu.com/{}"
# async def hello(url):
#     async with ClientSession() as session:
#         async with session.get(url) as response:
#             response = await response.read()
# #            print(response)
#             print('Hello World:%s' % time.time())

# def run():
#     for i in range(5):
#         task = asyncio.ensure_future(hello(url.format(i)))
#         tasks.append(task)

# if __name__ == '__main__':
#     loop = asyncio.get_event_loop()
#     run()
#     loop.run_until_complete(asyncio.wait(tasks))


# 收集http响应

# import time
# import asyncio
# from aiohttp import ClientSession

# tasks = []
# url = "https://www.baidu.com/{}"
# async def hello(url):
#     async with ClientSession() as session:
#         async with session.get(url) as response:
# #            print(response)
#             print('Hello World:%s' % time.time())
#             return await response.read()

# def run():
#     for i in range(5):
#         task = asyncio.ensure_future(hello(url.format(i)))
#         tasks.append(task)
#     result = loop.run_until_complete(asyncio.gather(*tasks))
#     print(result)

# if __name__ == '__main__':
#     loop = asyncio.get_event_loop()
#     run()


# 异常解决:
# 假如你的并发达到2000个，程序会报错：ValueError: too many file descriptors in select()。
# 报错的原因字面上看是 Python 调取的 select 对打开的文件有最大数量的限制，这个其实是操作系统的限制，linux打开文件的最大数默认是1024，windows默认是509，超过了这个值，程序就开始报错。
# 这里我们有三种方法解决这个问题：
# 1.限制并发数量。（一次不要塞那么多任务，或者限制最大并发数量）
# 2.使用回调的方式。
# 3.修改操作系统打开文件数的最大限制，在系统里有个配置文件可以修改默认值，具体步骤不再说明了。
# 不修改系统默认配置的话，个人推荐限制并发数的方法，设置并发数为500，处理速度更快。


# import time,asyncio,aiohttp


# url = 'https://www.baidu.com/'
# async def hello(url,semaphore):
#     async with semaphore:
#         async with aiohttp.ClientSession() as session:
#             async with session.get(url) as response:
#                 return await response.read()


# async def run():
#     semaphore = asyncio.Semaphore(500) # 限制并发量为500
#     to_get = [hello(url.format(),semaphore) for _ in range(1000)] #总共1000任务
#     await asyncio.wait(to_get)


# if __name__ == '__main__':
# #    now=lambda :time.time()
#     loop = asyncio.get_event_loop()
#     loop.run_until_complete(run())
#     loop.close()


#-------------------------------------------------------------------


# import asyncio
# import requests
# import os


# async def asyncget(url,headers):
#     return requests.get(url,headers=headers,verify=False)

# async def download(url,headers,fileDir,filename=None):

#     if filename == None:
#         filename = url[url.rfind("/")+1:]
#     if fileDir == None:
#         fileDir = "./files"

#     if not os.path.exists(fileDir):
#         print("亲，输入的目录不存在哦，下面将自动创建 "+ fileDir)
#         os.makedirs(fileDir)
    
#     filepath = os.path.join(fileDir,filename)

#     if os.path.exists(filepath):
#         print("亲，文件",filepath,"已经存在哦")
#         return
    
#     response =  await asyncget(url,headers=headers)

#     with open(filepath,"wb") as fp:
#         fp.write(response.content)
#     print("亲，文件",filepath,"写入完成哦")


# if __name__ == "__main__":
#     headers ={
#         "User-Agent": "Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36"
#     }
#     urls = [
#         "http://wx1.sinaimg.cn/mw600/00893JKXly1gikm1th96cj30u015b1a9.jpg",
#         "http://wx1.sinaimg.cn/mw600/00893JKXly1gikm1lkcxdj30sg18g0zh.jpg",
#         "http://wx2.sinaimg.cn/mw600/005QrDFkly1gfl4phpqvaj31w91w9x20.jpg"
#     ]

#     asyncio.run(
#         asyncio.wait(
#             [download(url=url,headers=headers,fileDir="imgs") for url in urls]
#         )
#     )



# import requests

# session = requests.Session()
# session.mount("https://", requests.adapters.HTTPAdapter(pool_maxsize=50))
# session.mount("http://", requests.adapters.HTTPAdapter(pool_maxsize=50))

# import asyncio

# async def perform_async_calls(self, session, urls):
#     loop = asyncio.get_event_loop()
#     futures = []
#     for url in urls:
#         futures.append(loop.run_in_executor(None, session.get, url)

#     results = []
#     for future in futures:
#         result = await future
#         results.append(result.json())

#     return results







